{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Neural Network that takes as input both the aptamer features and the peptide features to predict affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features for both aptamers and peptides + construct training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-589fa753b273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numba import jit, cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "d = 200\n",
    "samples = 28000\n",
    "split = 8000\n",
    "k_apt = 4\n",
    "k_pep = 2\n",
    "random.seed(42)\n",
    "device = torch.device('cuda')\n",
    "#torch.cuda.get_device_properties(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to classify binding affinity of a sample. \n",
    "'''\n",
    "def classify_affinity(affinity):\n",
    "    if float(affinity) <= 9:\n",
    "        return 0\n",
    "    elif float(affinity) <= 50:\n",
    "        return 1\n",
    "    elif float(affinity) <= 400:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def classify_rc_affinity(rc):\n",
    "    if int(rc) > 50:\n",
    "        return 0\n",
    "    elif int(rc) > 10:\n",
    "        return 1\n",
    "    elif int(rc) > 5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"../data/mhcflurry_dataset.json\"\n",
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "'''\n",
    "Constructs a dataset that has 10,000 pairs for every class of binding affinity. \n",
    "'''\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    \n",
    "    # Full dataset. The index of the list corresponds to the binding affinity class\n",
    "    full_dataset = [[], [], [], []]\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for p, b in peptides:\n",
    "            #affinity_class = classify_affinity(b)\n",
    "            affinity_class = classify_rc_affinity(b)\n",
    "            full_dataset[affinity_class].append((aptamer, p))\n",
    "    \n",
    "    subsampled_dataset = [[], [], [], []]\n",
    "    \n",
    "    for i in range(len(full_dataset)):\n",
    "        full_class = np.asarray(full_dataset[i])\n",
    "        # Sample the hardcoded number of samples pairs randomly\n",
    "        subsampled_dataset[i] = np.copy(full_class[np.random.choice(full_class.shape[0], samples, replace=False), :])\n",
    "    \n",
    "    subsampled_dataset = np.asarray(subsampled_dataset)    \n",
    "    return subsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_dataset = construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extracts features from the subsampled dataset\n",
    "'''\n",
    "@jit(target='cuda')\n",
    "def extract_features(dataset, d, k_apt, k_pep):\n",
    "    # Number of features\n",
    "    aptamer_features = [[], [], [], []]\n",
    "    peptide_features = [[], [], [], []]\n",
    "\n",
    "    for i in range(dataset.shape[0]):\n",
    "        flattened = dataset[i].flatten('F')\n",
    "        all_aptamers = flattened[:samples]\n",
    "        all_peptides = flattened[samples:]\n",
    "\n",
    "        split = int(0.8*len(all_aptamers))\n",
    "        all_aptamers = all_aptamers[:split]\n",
    "        all_peptides = all_peptides[:split]\n",
    "\n",
    "        # Generate the aptamer features randomly\n",
    "        for j in range(d):\n",
    "            # Find a random aptamer\n",
    "            apt = random.choice(all_aptamers)\n",
    "\n",
    "            # Find a random subsection of k elements from this sequence and the quartile\n",
    "            start = random.randint(0, len(apt)-k_apt)\n",
    "            quartile_pctg = (start + 1)/float(len(apt))\n",
    "            if quartile_pctg <= 0.25:\n",
    "                quartile = 1\n",
    "            elif quartile_pctg > 0.25 and quartile_pctg <= 0.5:\n",
    "                quartile = 2\n",
    "            elif quartile_pctg > 0.5 and quartile_pctg <= 0.75:\n",
    "                quartile = 3\n",
    "            else:\n",
    "                quartile = 4\n",
    "            \n",
    "            aptamer_features[i].append((apt[start:start+k_apt], quartile))\n",
    "\n",
    "        # Generate the peptide features randomly\n",
    "        for j in range(d):\n",
    "            # Find a random aptamer\n",
    "            pep = random.choice(all_peptides)\n",
    "\n",
    "            # Find a random subsection of k elements from this sequence\n",
    "            start = random.randint(0, len(pep)-k_pep)\n",
    "            quartile_pctg = (start + 1)/float(len(pep))\n",
    "            if quartile_pctg <= 0.25:\n",
    "                quartile = 1\n",
    "            elif quartile_pctg > 0.25 and quartile_pctg <= 0.5:\n",
    "                quartile = 2\n",
    "            elif quartile_pctg > 0.5 and quartile_pctg <= 0.75:\n",
    "                quartile = 3\n",
    "            else:\n",
    "                quartile = 4\n",
    "            \n",
    "            peptide_features[i].append((pep[start:start+k_pep], quartile))\n",
    "\n",
    "\n",
    "    return aptamer_features, peptide_features, split\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\nInternal error at <numba.typeinfer.CallConstraint object at 0x7f867dad4fd0>.\n\u001b[1m\u001b[1m\u001b[1m\u001b[0m\n\u001b[0m\u001b[1m[1] During: resolving callee type: BoundFunction(array.flatten for array([unichr x 40], 2d, C))\u001b[0m\n\u001b[0m\u001b[1m[2] During: typing of call at <ipython-input-7-69d2e31294ea> (11)\n\u001b[0m\nEnable logging at debug level for details.\n\u001b[1m\nFile \"<ipython-input-7-69d2e31294ea>\", line 11:\u001b[0m\n\u001b[1mdef extract_features(dataset, d, k_apt, k_pep):\n    <source elided>\n    for i in range(dataset.shape[0]):\n\u001b[1m        flattened = dataset[i].flatten('F')\n\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e352bccd62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maptamer_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeptide_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsampled_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_apt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_pep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/dispatcher.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisable_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/compiler.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mSpecialize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minvoke\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         '''\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedmem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0mcfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/compiler.pyc\u001b[0m in \u001b[0;36mspecialize\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    808\u001b[0m         argtypes = tuple(\n\u001b[1;32m    809\u001b[0m             [self.typingctx.resolve_argument_type(a) for a in args])\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/compiler.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             kernel = compile_kernel(self.py_func, argtypes,\n\u001b[0;32m--> 826\u001b[0;31m                                     **self.targetoptions)\n\u001b[0m\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler_lock.pyc\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/compiler.pyc\u001b[0m in \u001b[0;36mcompile_kernel\u001b[0;34m(pyfunc, args, link, debug, inline, fastmath, extensions, max_registers)\u001b[0m\n\u001b[1;32m     60\u001b[0m def compile_kernel(pyfunc, args, link, debug=False, inline=False,\n\u001b[1;32m     61\u001b[0m                    fastmath=False, extensions=[], max_registers=None):\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mcres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfndesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllvm_func_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     lib, kernel = cres.target_context.prepare_cuda_kernel(cres.library, fname,\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler_lock.pyc\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/cuda/compiler.pyc\u001b[0m in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, inline)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                   \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                   \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                   locals={})\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mlibrary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    526\u001b[0m     pipeline = pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    527\u001b[0m                               args, return_type, flags, locals)\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_ir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[1;32m    384\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_ir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_final_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCompilerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All available pipelines exhausted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\nInternal error at <numba.typeinfer.CallConstraint object at 0x7f867dad4fd0>.\n\u001b[1m\u001b[1m\u001b[1m\u001b[0m\n\u001b[0m\u001b[1m[1] During: resolving callee type: BoundFunction(array.flatten for array([unichr x 40], 2d, C))\u001b[0m\n\u001b[0m\u001b[1m[2] During: typing of call at <ipython-input-7-69d2e31294ea> (11)\n\u001b[0m\nEnable logging at debug level for details.\n\u001b[1m\nFile \"<ipython-input-7-69d2e31294ea>\", line 11:\u001b[0m\n\u001b[1mdef extract_features(dataset, d, k_apt, k_pep):\n    <source elided>\n    for i in range(dataset.shape[0]):\n\u001b[1m        flattened = dataset[i].flatten('F')\n\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "aptamer_features, peptide_features, split = extract_features(subsampled_dataset, d, k_apt, k_pep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates training and testing sets. Training is the first 8000 samples, test is the last 2000 samples. \n",
    "'''\n",
    "def construct_train_test_sets(aptamer_features, peptide_features, split):\n",
    "    train_pairs = [[], [], [], []]\n",
    "    test_pairs = [[], [], [], []]\n",
    "    \n",
    "    for c in range(len(subsampled_dataset)):\n",
    "        train_pairs[c] = subsampled_dataset[c][:split]\n",
    "        test_pairs[c] = subsampled_dataset[c][split:]\n",
    "    \n",
    "    train_pairs = np.asarray(train_pairs)\n",
    "    test_pairs = np.asarray(test_pairs)\n",
    "    \n",
    "    train_aptamers = [[], [], [], []]\n",
    "    test_aptamers = [[], [], [], []]\n",
    "    \n",
    "    train_peptides = [[], [], [], []]\n",
    "    test_peptides = [[], [], [], []]\n",
    "    \n",
    "    # Make a 0/1 matrix for the training aptamers/peptides\n",
    "    for i in range(len(train_aptamers)):\n",
    "        pairs = train_pairs[i]\n",
    "        apt_features = aptamer_features[i]\n",
    "        pep_features = peptide_features[i]\n",
    "        \n",
    "        for j in range(len(pairs)):\n",
    "            a, p = pairs[j]\n",
    "            matrix_aptamer_train = []\n",
    "            matrix_peptide_train = []\n",
    "            \n",
    "            for k in range(len(apt_features)):\n",
    "                feat, quartile = apt_features[k]\n",
    "                starts = [m.start() for m in re.finditer(feat, a)]\n",
    "                if len(starts) == 0:\n",
    "                    matrix_aptamer_train.append(0)\n",
    "                    continue\n",
    "                exists = False\n",
    "                for s in starts:\n",
    "                    pctg = (s + 1) / len(a)\n",
    "                    if pctg <= 0.25 and quartile == 1:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.25 and pctg <= 0.5) and quartile == 2:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.5 and pctg <= 0.75) and quartile == 3:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif pctg > 0.75 and quartile == 4:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    else:\n",
    "                        exists = False\n",
    "                if exists:\n",
    "                    matrix_aptamer_train.append(1)\n",
    "                if not exists:\n",
    "                    matrix_aptamer_train.append(0)\n",
    "            \n",
    "            train_aptamers[i].append(matrix_aptamer_train)\n",
    "            \n",
    "            for k in range(len(pep_features)):\n",
    "                feat, quartile = pep_features[k]\n",
    "                starts = [m.start() for m in re.finditer(feat, p)]\n",
    "                if len(starts) == 0:\n",
    "                    matrix_peptide_train.append(0)\n",
    "                    continue\n",
    "                exists = False\n",
    "                for s in starts:\n",
    "                    pctg = (s + 1) / len(p)\n",
    "                    if pctg <= 0.25 and quartile == 1:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.25 and pctg <= 0.5) and quartile == 2:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.5 and pctg <= 0.75) and quartile == 3:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif pctg > 0.75 and quartile == 4:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    else:\n",
    "                        exists = False\n",
    "                \n",
    "                if exists:\n",
    "                    matrix_peptide_train.append(1)\n",
    "                if not exists:\n",
    "                    matrix_peptide_train.append(0)\n",
    "                    \n",
    "            train_peptides[i].append(matrix_peptide_train)\n",
    "    \n",
    "    train_aptamers = np.asarray(train_aptamers)\n",
    "    train_peptides = np.asarray(train_peptides)\n",
    "    \n",
    "    # Make a 0/1 matrix for the testing aptamers/peptides\n",
    "\n",
    "    for i in range(len(test_aptamers)):\n",
    "        pairs = test_pairs[i]\n",
    "        apt_features = aptamer_features[i]\n",
    "        pep_features = peptide_features[i]\n",
    "        \n",
    "        for j in range(len(pairs)):\n",
    "            a, p = pairs[j]\n",
    "            matrix_aptamer_test = []\n",
    "            matrix_peptide_test = []\n",
    "            \n",
    "            for k in range(len(apt_features)):\n",
    "                feat, quartile = apt_features[k]\n",
    "                starts = [m.start() for m in re.finditer(feat, a)]\n",
    "                if len(starts) == 0:\n",
    "                    matrix_aptamer_test.append(0)\n",
    "                    continue\n",
    "                exists = False\n",
    "                for s in starts:\n",
    "                    # Each s is an index of the beginning of this features\n",
    "                    # If one of them appears in the correct quartile, then this is 1\n",
    "                    pctg = (s + 1) / len(a)\n",
    "                    if pctg <= 0.25 and quartile == 1:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.25 and pctg <= 0.5) and quartile == 2:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.5 and pctg <= 0.75) and quartile == 3:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif pctg > 0.75 and quartile == 4:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    else:\n",
    "                        exists = False\n",
    "                if exists:\n",
    "                    matrix_aptamer_test.append(1)\n",
    "                if not exists:\n",
    "                    matrix_aptamer_test.append(0)\n",
    "                        \n",
    "            test_aptamers[i].append(matrix_aptamer_test)\n",
    "            \n",
    "            for k in range(len(pep_features)):\n",
    "                feat, quartile = pep_features[k]\n",
    "                starts = [m.start() for m in re.finditer(feat, p)]\n",
    "                if len(starts) == 0:\n",
    "                    matrix_peptide_test.append(0)\n",
    "                    continue\n",
    "                exists = False\n",
    "                for s in starts:\n",
    "                    pctg = (s + 1) / len(p)\n",
    "                    if pctg <= 0.25 and quartile == 1:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.25 and pctg <= 0.5) and quartile == 2:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif (pctg > 0.5 and pctg <= 0.75) and quartile == 3:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    elif pctg > 0.75 and quartile == 4:\n",
    "                        exists = True\n",
    "                        break\n",
    "                    else:\n",
    "                        exists = False\n",
    "                if exists:\n",
    "                    matrix_peptide_test.append(1)\n",
    "                else:\n",
    "                    matrix_peptide_test.append(0)\n",
    "\n",
    "            test_peptides[i].append(matrix_peptide_test)\n",
    "                \n",
    "    test_aptamers = np.asarray(test_aptamers)\n",
    "    test_peptides = np.asarray(test_peptides)\n",
    "    \n",
    "    \n",
    "    return train_aptamers, train_peptides, test_aptamers, test_peptides\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aptamers, train_peptides, test_aptamers, test_peptides = construct_train_test_sets(aptamer_features, peptide_features, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(train_aptamers.shape))\n",
    "print(str(train_peptides.shape))\n",
    "print(str(test_aptamers.shape))\n",
    "print(str(test_peptides.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AptamerPeptideDataset(Dataset):\n",
    "    '''\n",
    "    @param: peptides = n*m\n",
    "    @param: aptamers = n*m\n",
    "    @param: affinities = n*1\n",
    "    '''\n",
    "    def __init__(self, peptides, aptamers, affinities):\n",
    "        self.peptides = peptides\n",
    "        self.aptamers = aptamers\n",
    "        affinities = np.reshape(affinities, (affinities.shape[0], 1))\n",
    "        self.affinities = affinities\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.peptides.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        pep = self.peptides[idx]\n",
    "        apt = self.aptamers[idx]        \n",
    "        aff_class = self.affinities[idx]\n",
    "        \n",
    "        sample = {'peptide': pep, 'aptamer': apt, 'affinity': aff_class}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the dataset to fit the dataset class\n",
    "def reshape_dataset(train_aptamers, test_aptamers, train_peptides, test_peptides):\n",
    "    all_train_aptamers = []\n",
    "    all_test_aptamers = []\n",
    "    for i in range(len(train_aptamers)):\n",
    "        all_train_aptamers.extend(train_aptamers[i])\n",
    "        all_test_aptamers.extend(test_aptamers[i])\n",
    "\n",
    "    # n * m\n",
    "    all_train_aptamers = np.array(all_train_aptamers)\n",
    "    all_test_aptamers = np.array(all_test_aptamers)\n",
    "\n",
    "    all_train_peptides = []\n",
    "    all_test_peptides = []\n",
    "    for i in range(len(train_peptides)):\n",
    "        all_train_peptides.extend(train_peptides[i])\n",
    "        all_test_peptides.extend(test_peptides[i])\n",
    "\n",
    "    # n * m\n",
    "    all_train_peptides = np.array(all_train_peptides)\n",
    "    all_test_peptides = np.array(all_test_peptides)\n",
    "\n",
    "\n",
    "    # n * 1\n",
    "    train_affinity_classes = np.repeat(np.array([[0, 1], [2, 3]]), split)\n",
    "    test_affinity_classes = np.repeat(np.array([[0, 1], [2, 3]]), samples-split)\n",
    "\n",
    "    return all_train_peptides, all_train_aptamers, all_test_peptides, all_test_aptamers, train_affinity_classes, test_affinity_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pep, train_apt, test_pep, test_apt, train_aff, test_aff = reshape_dataset(train_aptamers, test_aptamers, train_peptides, test_peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train pep shape: \", train_pep.shape)\n",
    "print(\"Train apt shape: \", train_apt.shape)\n",
    "print(\"Test apt shape: \", test_apt.shape)\n",
    "print(\"Test_pep shape: \", test_pep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AptamerPeptideDataset(train_pep, train_apt, train_aff)\n",
    "test_dataset = AptamerPeptideDataset(test_pep, test_apt, test_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a small neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class SmallNN(nn.Module):\n",
    "    def __init__(self, d_value):\n",
    "        super(SmallNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_value, 1024)\n",
    "        self.prelu1 = nn.PReLU(num_parameters=1)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.prelu2 = nn.PReLU(num_parameters=1)\n",
    "        self.lin1 = nn.Linear(512, 250)\n",
    "        self.prelu3 = nn.PReLU(num_parameters=1)\n",
    "        self.lin2 = nn.Linear(250, 100)\n",
    "        self.fc3 = nn.Linear(250, 4)\n",
    "        self.sequential = nn.Sequential(self.fc1, self.prelu1, self.fc2, self.prelu2, self.lin1, self.prelu3, self.fc3)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "       \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.type(torch.FloatTensor)\n",
    "        pep = pep.type(torch.FloatTensor)\n",
    "        apt = self.sequential(apt)\n",
    "        pep = self.sequential(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc4(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        loss = nn.MSELoss()\n",
    "        label = label.type(torch.FloatTensor)\n",
    "        label = np.reshape(label, (1, 4))\n",
    "        return loss(prediction, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmallNN(d_value=d)\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        pep = data['peptide']\n",
    "        apt = data['aptamer']\n",
    "        label = data['affinity'].item()\n",
    "        one_hot_label = [0] * 4\n",
    "        one_hot_label[label] = 1\n",
    "        one_hot_label = torch.tensor(one_hot_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(pep, apt)\n",
    "        loss = model.loss(output, one_hot_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'small_nn_aptamer.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "        pep = data['peptide']\n",
    "        apt = data['aptamer']\n",
    "        label = data['affinity'].item()\n",
    "        \n",
    "        output = model(pep, apt)\n",
    "        pred = torch.argmax(output).item()\n",
    "        \n",
    "        total += 1\n",
    "        correct += (pred == label)\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimental_loop(d, k_apt, k_pep):\n",
    "    # Construct the dataset\n",
    "    print(\"Constructing the dataset\")\n",
    "    aptamer_features, peptide_features, split = extract_features(subsampled_dataset, d=d, k_apt=k_apt, k_pep=k_pep)\n",
    "    train_aptamers, train_peptides, test_aptamers, test_peptides = construct_train_test_sets(aptamer_features, peptide_features, split)\n",
    "    train_pep, train_apt, test_pep, test_apt, train_aff, test_aff = reshape_dataset(train_aptamers, test_aptamers, train_peptides, test_peptides)\n",
    "    train_dataset = AptamerPeptideDataset(train_pep, train_apt, train_aff)\n",
    "    test_dataset = AptamerPeptideDataset(test_pep, test_apt, test_aff)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "    testloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "    # Construct the model\n",
    "    print(\"Constructing the model\")\n",
    "    model = SmallNN(d_value=d)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    # Training loop\n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader):\n",
    "            pep = data['peptide']\n",
    "            apt = data['aptamer']\n",
    "            label = data['affinity'].item()\n",
    "            one_hot_label = [0] * 4\n",
    "            one_hot_label[label] = 1\n",
    "            one_hot_label = torch.tensor(one_hot_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(pep, apt)\n",
    "            loss = model.loss(output, one_hot_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # Testing loop\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            pep = data['peptide']\n",
    "            apt = data['aptamer']\n",
    "            label = data['affinity'].item()\n",
    "\n",
    "            output = model(pep, apt)\n",
    "            pred = torch.argmax(output).item()\n",
    "\n",
    "            total += 1\n",
    "            correct += (pred == label)\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimental_testing_loop(d, k_apt, k_pep):\n",
    "    # Construct the dataset\n",
    "    aptamer_features, peptide_features, split = extract_features(subsampled_dataset, d=d, k_apt=k_apt, k_pep=k_pep)\n",
    "    train_aptamers, train_peptides, test_aptamers, test_peptides = construct_train_test_sets(aptamer_features, peptide_features, split)\n",
    "    train_pep, train_apt, test_pep, test_apt, train_aff, test_aff = reshape_dataset(train_aptamers, test_aptamers, train_peptides, test_peptides)\n",
    "    train_dataset = AptamerPeptideDataset(train_pep, train_apt, train_aff)\n",
    "    test_dataset = AptamerPeptideDataset(test_pep, test_apt, test_aff)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "    testloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "    # Construct the model\n",
    "    model = SmallNN(d_value=d)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    # Training loop\n",
    "    for epoch in range(3):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        model.train()\n",
    "        for i, data in enumerate(trainloader):\n",
    "            pep = data['peptide']\n",
    "            apt = data['aptamer']\n",
    "            label = data['affinity'].item()\n",
    "            one_hot_label = [0] * 4\n",
    "            one_hot_label[label] = 1\n",
    "            one_hot_label = torch.tensor(one_hot_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(pep, apt)\n",
    "            loss = model.loss(output, one_hot_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Testing loop\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(testloader):\n",
    "                pep = data['peptide']\n",
    "                apt = data['aptamer']\n",
    "                label = data['affinity'].item()\n",
    "\n",
    "                output = model(pep, apt)\n",
    "                pred = torch.argmax(output).item()\n",
    "\n",
    "                total += 1\n",
    "                correct += (pred == label)\n",
    "        print('Accuracy of the network after ' + str(epoch) + ' epoch on the test samples: %d %%' % (100* correct/total))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = [1800, 2200, 2600, 3000, 3400]\n",
    "for d in d_values:\n",
    "    correct, total = experimental_loop(d=d, k_apt=4, k_pep=4)\n",
    "    print('D-value', d)\n",
    "    print('Accuracy of the network on the test samples: %d %%' % (100* correct/total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_apt_values = [4, 6, 8, 10]\n",
    "k_pep_values = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for a in k_apt_values:\n",
    "    for p in k_pep_values:\n",
    "        correct, total = experimental_loop(d=1800, k_apt=a, k_pep=p)\n",
    "        print(\"D-value \" + str(d) + \" K_apt \" + str(a) + \" K_pep \" + str(p))\n",
    "        print('Accuracy of the network on the test samples: %d %%' % (100* correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_testing_loop(1800, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = experimental_loop(1800, 2, 5)\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100* correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
