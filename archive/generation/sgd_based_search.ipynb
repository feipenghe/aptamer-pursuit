{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model:  LinearNet  at epoch:  13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (fc_apt_1): Linear(in_features=160, out_features=200, bias=True)\n",
       "  (fc_apt_2): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (fc_apt_3): Linear(in_features=250, out_features=300, bias=True)\n",
       "  (fc_pep_1): Linear(in_features=160, out_features=200, bias=True)\n",
       "  (fc_pep_2): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc_apt): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=250, bias=True)\n",
       "    (2): Linear(in_features=250, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_pep): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=250, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=550, out_features=600, bias=True)\n",
       "  (fc2): Linear(in_features=600, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = LinearNet()\n",
    "model_name = model.name\n",
    "model_id = \"07062020\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "optimizer = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "init_epoch = checkpoint['epoch'] +1\n",
    "print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(curr_aptamer[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTTGTTTATAGGGAGCTTGGAATCATAGCAACGCGGAAA\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "aptamer_0 = \"\"\n",
    "for i in range(40):\n",
    "    aptamer_0 += random.choice(na_list)\n",
    "print(str(aptamer_0))\n",
    "aptamer_0 = one_hot(aptamer_0, seq_type='aptamer')\n",
    "print(str(aptamer_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGATGGGAC\n",
      "ATTTGGATGGCCGGCCGGGGGGGGGGAGCCCCGACGGGAC\n",
      "ATTTGGAAGGCCGGCCGCGGGGGGGGAGCCCCGACGGGAC\n",
      "ATTTGGAAGGCCGGCAGCGGGGGGGGAGCCCCGACGGGAC\n",
      "ATTTGGAAGGCCGGCAGCGGGGGGGGAGCCCCGACGGGAC\n",
      "ATTTGGAAGGCCGGCAGCGGGGGGGGAGCCCCGACGGGAC\n",
      "ATTTGGAAGGCCGGCAGCGGGGGGGGAGCCTCGGCGGGAC\n",
      "ATTTGGAAGGCCGGCAGTGGCGGGGGAGCCTCGGCGGGAC\n",
      "ATTTGGAAGACCGGCAGTGGCGGAGGAGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCAGAGGAGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCAGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCAGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ATTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ACTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ACTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGGGAC\n",
      "ACTTGTAAGACAGGCAGTGGCCGAGGCGCCTCGGCGAGAC\n",
      "ACCTGTATGACAGGCAGTGGCCGAGGCGCCTCGGCGAGCC\n",
      "ACCTCTATGACAGGCAGTGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGTGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGTGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGTGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGTGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGACAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGGCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTCAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "ACCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGAAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCGCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCGAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCGTCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATCAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "AGCTTAATGAGAGGCAGCGGCCGCAGCCCCTCATAAAGCC\n",
      "0.2765457332134247\n"
     ]
    }
   ],
   "source": [
    "curr_aptamer = aptamer_0\n",
    "for k in range(400):\n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    train_score = update(a, p, None, single_alphabet=False)\n",
    "    train_score.backward()\n",
    "    new_aptamer = np.zeros((40, 4, 1))\n",
    "    alpha_k = 4/(k + 2)\n",
    "    #print(str(np.sum(a.grad.cpu().numpy())))\n",
    "    for i in range(40):\n",
    "        # Gradient wrt aptamer\n",
    "        ind = np.argmax(a.grad[:, :, i].cpu().numpy())\n",
    "        for j in range(4):\n",
    "            # new_aptamer.shape = 40, 4, 1\n",
    "            # curr_aptamer.shape = 1, 4, 40\n",
    "            new_aptamer[i, j, 0] = (1 - alpha_k)*a[0, j, i] + alpha_k*(j == ind)\n",
    "    \n",
    "    curr_aptamer = new_aptamer\n",
    "    # Round the aptamer and find the resulting string\n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    aptamer_string = stringify(rounded_aptamer)\n",
    "    print(str(aptamer_string))\n",
    "    \n",
    "a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "final_score = update(a, p, None, single_alphabet=False)\n",
    "print(str(final_score.cpu().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1870]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Test the actual aptamer\n",
    "actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "final_score = update(a, p, None, single_alphabet=False)\n",
    "print(str(final_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
