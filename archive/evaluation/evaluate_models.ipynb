{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import pairwise2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nupyck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export NUPACKHOME=\"/ssd1/home/aishrm2/aptamer-pursuit/src/models/evaluation/nupyck/lib/nupack\"\n",
    "import nupyck\n",
    "import importlib\n",
    "importlib.reload(nupyck)\n",
    "!pip install biopython\n",
    "from Bio import SeqIO\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nupyck.mfe([\"GAUCCGC\", \"GCGAAUC\"], [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize free energies\n",
    "binding_energies = []\n",
    "free_energies = []\n",
    "f = open(\"binding_energies.txt\", \"r\")\n",
    "for x in f:\n",
    "    binding_energies.append(float(x))\n",
    "\n",
    "f = open('free_energies.txt', 'r')\n",
    "for x in f:\n",
    "    free_energies.append(float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(binding_energies, ax=ax, label='Binding Aptamers')\n",
    "sns.distplot(free_energies, ax=ax, label='Non binding aptamers')\n",
    "plt.legend()\n",
    "plt.title(\"Free Energy Distributions\")\n",
    "plt.xlabel(\"Free Energy Score (MFE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aptamers binding\n",
    "aptamers_binding = SeqIO.parse(\"aptamers_binding.fasta\", \"fasta\")\n",
    "apts = []\n",
    "for record in aptamers_binding:\n",
    "    apts.append(str(record.seq))\n",
    "binding_energies = []\n",
    "for i, apt in enumerate(tqdm.tqdm(apts)):\n",
    "    try:\n",
    "        energy = nupyck.mfe([apt], [1])[0]['energy']\n",
    "        binding_energies.append(energy)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(str(len(binding_energies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('binding_energies.txt', 'w') as f:\n",
    "    for e in binding_energies:\n",
    "        f.write('%s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamers_free = SeqIO.parse(\"aptamers_free.fasta\", \"fasta\")\n",
    "apts = []\n",
    "for record in aptamers_free:\n",
    "    apts.append(str(record.seq))\n",
    "free_energies = []\n",
    "for i, apt in enumerate(tqdm.tqdm(apts)):\n",
    "    try:\n",
    "        energy = nupyck.mfe([apt], [1])[0]['energy']\n",
    "        free_energies.append(energy)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(str(len(free_energies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('free_energies.txt', 'w') as f:\n",
    "    for e in free_energies:\n",
    "        f.write('%s\\n' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(n) #use for eval\n",
    "#train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModelSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1dModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"LinearConv1dModel\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 1)\n",
    "        self.conv_type = '1d'\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dModelSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModelSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"Conv1dModelSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "        self.conv_type = '1d'\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrueLinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is too complex for our input sequence size\n",
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 300, 1)\n",
    "        self.cnn_apt_3 = nn.Conv1d(300, 150, 1)\n",
    "        self.cnn_apt_4 = nn.Conv1d(150, 75, 1)\n",
    "        self.cnn_apt_5 = nn.Conv1d(25, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 250, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(250, 500, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 250, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(250, 100, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(100, 10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.maxpool, self.relu, self.cnn_apt_3, self.maxpool, self.relu, self.cnn_apt_4, self.maxpool, self.relu, self.cnn_apt_5, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.maxpool, self.relu, self.cnn_pep_3, self.maxpool, self.relu, self.cnn_pep_4, self.maxpool, self.relu, self.cnn_pep_5, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(180, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrueLinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"TrueLinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.conv_type = '1d'\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimizedVCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinimizedVCNet, self).__init__()\n",
    "        self.name = \"MinimizedVCNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 500, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(500, 100, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 250, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(250, 100, 3, padding=2)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(800, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        # conv --> relu --> pool after every one\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaptumNet, self).__init__()\n",
    "        self.name = \"CaptumNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 500, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(500, 100, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 250, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(250, 100, 3, padding=2)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(800, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        apt = input[0]\n",
    "        pep = input[1]\n",
    "        \n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[0], apt.shape[1]))).to(device) #(1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[0], pep.shape[1]))).to(device) #(1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "# If one_input, concatenates apt and pep together\n",
    "def update(x, y, one_input=False):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    if one_input:\n",
    "        out = model([x, y])\n",
    "    else:\n",
    "        out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train', one_input=False):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep, one_input=one_input)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\", one_input=False):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.to(device)\n",
    "        y = pep.to(device)\n",
    "        if one_input:\n",
    "            out = model([x, y])\n",
    "        else:\n",
    "            out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_train_loss(train_loss, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.ylabel(\"Train loss\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Train loss at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/train_loss.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_loss(test_loss, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Test loss\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Test loss at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/test_loss.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Recall at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/recall.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_ecdf_test(test_score, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Test CDF at epoch %d' %epoch+ \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/test_cdf.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Train CDF at epoch %d' %epoch+ \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores, model_name, model_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=ax)\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=ax)\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=ax)\n",
    "    ax.set_title(\"Distribution of Scores\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/histogram.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "checkpointed_model = '../../models/model_checkpoints/MinimizedVCNet/05122020.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = MinimizedVCNet()\n",
    "optim = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236524\n",
      "118262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('AATATAATGACGCCAGTACTTCATGGATGTGCGGTAAGCT', 'MSRPAWDS')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(len(S_prime_test)))\n",
    "print(str(len(S_test)))\n",
    "S_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation set is S_prime_test and S_test\n",
    "validation_set = []\n",
    "for (apt, pep) in S_test[:4000]:\n",
    "    validation_set.append((apt, pep, 1))\n",
    "\n",
    "for (apt, pep) in S_new[:4000]:\n",
    "    validation_set.append((apt, pep, 0))\n",
    "\n",
    "np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CAGGGGTGCCCCGCTACGGCAAATTCACACACTGTCAGTA', 'MERSDQAD', 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "hydrophobicity_binding = []\n",
    "hydrophobicity_free = []\n",
    "\n",
    "arginine_binding = []\n",
    "arginine_free = []\n",
    "\n",
    "gc_binding = []\n",
    "gc_free = []\n",
    "\n",
    "aptamers_binding = []\n",
    "aptamers_free = []\n",
    "\n",
    "peptides_binding = []\n",
    "peptides_free = []\n",
    "\n",
    "# Only look at similarity for these lists\n",
    "dataset_aptamers_binding = []\n",
    "dataset_peptides_binding = []\n",
    "dataset_aptamers_free = []\n",
    "dataset_peptides_free = []\n",
    "\n",
    "generated_aptamers_binding = []\n",
    "generated_peptides_binding = []\n",
    "generated_aptamers_free = []\n",
    "generated_peptides_free = []\n",
    "\n",
    "for (apt, pep, label) in validation_set:\n",
    "    x, y = convert(apt, pep)\n",
    "    score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "    hp = 0\n",
    "    for aa in pep:\n",
    "        hp += hydrophobicity[aa]\n",
    "    \n",
    "    if score < 0.3:\n",
    "        hydrophobicity_free.append(hp)\n",
    "        arginine_free.append(pep.count('R'))\n",
    "        gc_free.append(gc_count)\n",
    "        aptamers_free.append(apt)\n",
    "        peptides_free.append(pep)\n",
    "        \n",
    "    elif score > 0.6:\n",
    "        hydrophobicity_binding.append(hp)\n",
    "        arginine_binding.append(pep.count('R'))\n",
    "        gc_binding.append(gc_count)\n",
    "        aptamers_binding.append(apt)\n",
    "        peptides_binding.append(pep)\n",
    "        \n",
    "    \n",
    "    # The sample is generated\n",
    "    if label == 0 and score > 0.6:\n",
    "        generated_aptamers_binding.append(apt)\n",
    "        generated_peptides_binding.append(pep)\n",
    "        \n",
    "    elif label == 0 and score < 0.3:\n",
    "        generated_aptamers_free.append(apt)\n",
    "        generated_peptides_free.append(pep)\n",
    "    \n",
    "    # The sample is from our dataset\n",
    "    elif label == 1 and score > 0.6:\n",
    "        dataset_aptamers_binding.append(apt)\n",
    "        dataset_peptides_binding.append(pep)\n",
    "        \n",
    "    elif label == 1 and score < 0.3:\n",
    "        dataset_aptamers_free.append(apt)\n",
    "        dataset_peptides_free.append(pep)\n",
    "        \n",
    "    \n",
    "    gc_count = apt.count('C') + apt.count('G')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the aptamers binding and aptamers free to a fasta file to be used with mfold\n",
    "with open(\"aptamers_binding.fasta\", 'w') as f:\n",
    "    count = 1\n",
    "    for apt in aptamers_binding:\n",
    "        f.write('> Sequence: ' + str(count) + '\\n')\n",
    "        f.write(apt +'\\n')\n",
    "        count += 1\n",
    "with open(\"aptamers_free.fasta\", 'w') as f:\n",
    "    count = 1\n",
    "    for apt in aptamers_free:\n",
    "        f.write('> Sequence: ' + str(count) + '\\n')\n",
    "        f.write(apt +'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Hydrophobicity of binding peptides:  256.99847094801225\n",
      "Average Hydrophobicity of non-binding peptides:  238.16441717791412\n",
      "Average Number of Arginines in binding peptides:  0.8757645259938838\n",
      "Average Number of Arginines in non-binding peptides:  0.6662576687116565\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrophobicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hydrophobicity_binding, bins=10, alpha=0.5, label='Hydrophobicity of Binding Peptides')\n",
    "plt.hist(hydrophobicity_free, bins=10 , alpha=0.5, label='Hydrophobicity of Non-Binding Peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Hydrophobicity Score\")\n",
    "plt.title('Hydrophobicity of Outputs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arginine content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Arginine content\n",
    "plt.hist(arginine_binding, bins=8, alpha=0.5, label='Number of arginines in binding peptides')\n",
    "plt.hist(arginine_free, bins=8 , alpha=0.5, label='Number of arginines in non-binding peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Number of Arginines\")\n",
    "plt.title('Arginine Count in Peptides')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gc_binding, bins=8, alpha=0.5, label='GC Count in Binding Aptamers')\n",
    "plt.hist(gc_free, bins=8 , alpha=0.5, label='GC Count in Non-Binding aptamers')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"GC Count\")\n",
    "plt.title('GC Count in Aptamers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: binding aptamers that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_binding_similarity = []\n",
    "generated_binding_similarity = []\n",
    "\n",
    "# For aptamers in our dataset\n",
    "for i, apt in enumerate(tqdm.tqdm(dataset_aptamers_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -3, -1, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_binding_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, apt in enumerate(tqdm.tqdm(generated_aptamers_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -3, -1, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_binding_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_binding_similarity = np.asarray(dataset_binding_similarity)\n",
    "generated_binding_similarity = np.asarray(generated_binding_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_binding_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_binding_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Aptamers in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for aptamers that we predict will bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: binding peptides that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_binding_similarity = []\n",
    "generated_binding_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, pep in enumerate(tqdm.tqdm(dataset_peptides_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_binding_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, pep in enumerate(tqdm.tqdm(generated_peptides_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_binding_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_binding_similarity = np.asarray(dataset_binding_similarity)\n",
    "generated_binding_similarity = np.asarray(generated_binding_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_binding_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_binding_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Peptides in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for peptides that we predict will bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: non binding peptides that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_free_similarity = []\n",
    "generated_free_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, pep in enumerate(tqdm.tqdm(dataset_peptides_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_free_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, pep in enumerate(tqdm.tqdm(generated_peptides_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_free_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_free_similarity = np.asarray(dataset_free_similarity)\n",
    "generated_free_similarity = np.asarray(generated_free_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_free_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_free_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Peptides in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for peptides that we predict will not bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: non binding aptamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_free_similarity = []\n",
    "generated_free_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, apt in enumerate(tqdm.tqdm(dataset_aptamers_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_free_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, apt in enumerate(tqdm.tqdm(generated_aptamers_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_free_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_free_similarity = np.asarray(dataset_free_similarity)\n",
    "generated_free_similarity = np.asarray(generated_free_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_free_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_free_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Aptamers in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for aptamers that we predict will not bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointed_model = '../../models/model_checkpoints/MinimizedVCNet/05082020.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = MinimizedVCNet()\n",
    "optim = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apt, pep, score = validation_set[0]\n",
    "baseline_apt, baseline_pep = convert(apt, pep)\n",
    "baseline = [baseline_apt, baseline_pep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.cat([input_apt, input_pep])\n",
    "\n",
    "baseline.shape\n",
    "\n",
    "\n",
    "# ig = IntegratedGradients(model)\n",
    "# attributions, delta = ig.attribute(input_apt, input_pep, baseline_apt, baseline_pep, target=0, return_convergence_delta=True)\n",
    "# # # print('IG Attributions:', attributions)\n",
    "# # print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to set it up their way with a model that only takes one input\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(3, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
    "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
    "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
    "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin2(self.relu(self.lin1(input)))\n",
    "model = ToyModel()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 3)\n",
    "baseline = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLift(model)\n",
    "attributions, delta = dl.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('DeepLift Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = NeuronConductance(model, model.lin1)\n",
    "attributions = nc.attribute(input, neuron_index=2, target=0)\n",
    "print('Neuron Attributions:', attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
