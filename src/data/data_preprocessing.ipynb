{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10000 # number of samples used to calculate loss\n",
    "N_GRAM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids for aptamer\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids for peptide\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of {token: index} for all possible ngrams with size <= N_GRAM\n",
    "def get_vocab():\n",
    "    def generator(letters):\n",
    "        letters = \"\".join(letters)\n",
    "        for n in range(1, N_GRAM+1):\n",
    "            for item in itertools.product(letters, repeat=n):\n",
    "                yield \"\".join(item)\n",
    "    a = [i for i in generator(na_list)]\n",
    "    p = [i for i in generator(aa_list)]\n",
    "    vocab_apt = {a[i]: i for i in range(len(a))}\n",
    "    vocab_pep = {p[i]: i for i in range(len(p))}\n",
    "    return vocab_apt, vocab_pep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_apt, vocab_pep = get_vocab()\n",
    "VOCAB_SIZE_APT = len(vocab_apt) #84\n",
    "VOCAB_SIZE_PEP = len(vocab_pep) #8420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates the broken-down tokens of the given sequence with N_GRAM\n",
    "def ngrams_iterator(seq):\n",
    "    for char in seq:\n",
    "        yield char\n",
    "    for n in range(2, N_GRAM + 1):\n",
    "        for char in zip(*[seq[i:] for i in range(n)]):\n",
    "            yield ''.join(char)\n",
    "\n",
    "            \n",
    "# Encodes aptamer/peptide to a binary vector, 1 being the correspoinding ngram is present\n",
    "def binary_encoding(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        vocab_size = VOCAB_SIZE_PEP\n",
    "        vocab = vocab_pep\n",
    "    else:\n",
    "        vocab_size = VOCAB_SIZE_APT\n",
    "        vocab = vocab_apt\n",
    "    x = torch.zeros(vocab_size)\n",
    "    for i in ngrams_iterator(sequence):\n",
    "        x[vocab[i]] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to binary matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1]))).to(device) #(1, 1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1]))).to(device) #(1, 1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            pep_pmf = get_y_pmf(peptide)\n",
    "            ds.append((aptamer, peptide, pep_pmf))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    y_pmf = get_y_pmf(y)\n",
    "    return y, y_pmf\n",
    "\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        x = get_x()\n",
    "        y, pmf = get_y()\n",
    "        pair = (x, y, pmf)\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        x = get_x()\n",
    "        y, pmf = get_y()\n",
    "        S_new.append((x, y, pmf))\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep, _) in dset[:k]:\n",
    "        pairs.append((apt, pep))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'):\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for item in dset[:k]:\n",
    "        pairs.append((item[0], item[1]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_samples = loss_samples(k, 'train')\n",
    "test_loss_samples = loss_samples(k, 'test')\n",
    "prime_train_loss_samples = prime_loss_samples(k, 'train')\n",
    "prime_test_loss_samples = prime_loss_samples(k, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5, device='cuda:0')\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5).cuda()\n",
    "y = torch.tensor([1,2,3])\n",
    "file = open('test.pkl', 'wb')\n",
    "pickle.dump(x, file)\n",
    "pickle.dump(y, file)\n",
    "file.close()\n",
    "f = open('test.pkl', 'rb')\n",
    "for i in range(2):\n",
    "    a = pickle.load(f)\n",
    "    print(a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_tr_n = open('s_tr_n.pkl', 'wb')\n",
    "Sp_tr_n = open('sp_tr_n.pkl', 'wb')\n",
    "S_te_n = open('s_te_n.pkl', 'wb')\n",
    "Sp_te_n = open('sp_te_n.pkl', 'wb')\n",
    "\n",
    "tr_loss_n = open('tr_loss_n.pkl', 'wb')\n",
    "te_loss_n = open('te_loss_n.pkl', 'wb')\n",
    "ptr_loss_n = open('ptr_loss_n.pkl', 'wb')\n",
    "pte_loss_n = open('pte_loss_n.pkl', 'wb')\n",
    "\n",
    "S_tr = open('s_tr.pkl', 'wb')\n",
    "Sp_tr = open('sp_tr.pkl', 'wb')\n",
    "S_te = open('s_te.pkl', 'wb')\n",
    "Sp_te = open('sp_te.pkl', 'wb')\n",
    "\n",
    "tr_loss = open('tr_loss.pkl', 'wb')\n",
    "te_loss = open('te_loss.pkl', 'wb')\n",
    "ptr_loss = open('ptr_loss.pkl', 'wb')\n",
    "pte_loss = open('pte_loss.pkl', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aptamer, peptide, _, (apt_prime, pep_prime, pep_pmf), indicator in train_ds:\n",
    "    a_n = binary_encoding(aptamer, seq_type='apt')\n",
    "    a_p_n = binary_encoding(apt_prime, seq_type='apt')\n",
    "    p_n = binary_encoding(peptide)\n",
    "    p_p_n = binary_encoding(pep_prime)\n",
    "\n",
    "    a = one_hot(aptamer, seq_type='apt')\n",
    "    a_p = one_hot(apt_prime, seq_type='apt')\n",
    "    p = one_hot(peptide)\n",
    "    p_p = one_hot(pep_prime)\n",
    "\n",
    "    a, p = convert(a, p)\n",
    "    a_p, p_p = convert(a_p, p_p)\n",
    "\n",
    "    pickle.dump((a_n, p_n, a_p_n, p_p_n, pep_pmf, indicator), S_tr_n)\n",
    "    pickle.dump((a, p, a_p, p_p, pep_pmf, indicator), S_tr)\n",
    "S_tr_n.close()\n",
    "S_tr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for aptamer, peptide, _ in S_test:\n",
    "    a_n = binary_encoding(aptamer, seq_type='apt')\n",
    "    p_n = binary_encoding(peptide)\n",
    "\n",
    "    a = one_hot(aptamer, seq_type='apt')\n",
    "    p = one_hot(peptide)\n",
    "\n",
    "    a, p = convert(a, p)\n",
    "\n",
    "    pickle.dump((a_n, p_n,), S_te_n)\n",
    "    pickle.dump((a, p), S_te)\n",
    "S_te_n.close()\n",
    "S_te.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_n = open('s_new.pkl', 'wb')\n",
    "S_new_n = open('s_new_n.pkl', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2a2f0495ea02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_new_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mS_new_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "for aptamer, peptide, _ in S_new:\n",
    "    a_n = binary_encoding(aptamer, seq_type='apt')\n",
    "    p_n = binary_encoding(peptide)\n",
    "\n",
    "    a = one_hot(aptamer, seq_type='apt')\n",
    "    p = one_hot(peptide)\n",
    "\n",
    "    a, p = convert(a, p)\n",
    "\n",
    "    pickle.dump((a_n, p_n,), S_new_n)\n",
    "    pickle.dump((a, p), S_n)\n",
    "S_new_n.close()\n",
    "S_n.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_train_loss_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_test_loss_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
