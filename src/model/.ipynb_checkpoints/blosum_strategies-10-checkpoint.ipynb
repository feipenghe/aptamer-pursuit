{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blosum strategies\n",
    "* This file compares using the original blosum rows as features vs. doing eigenvalue decomposition and then constructing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*12 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "uniform_pvals = [0.05]*20\n",
    "\n",
    "encoding_style = 'regular'\n",
    "lambda_val = 1\n",
    "alpha=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New distribution (not NNK exactly)\n",
    "aa_list_2 = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\",  \"I\",  \"K\",  \"L\",  \"M\",  \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\",  \"W\", \"Y\"]\n",
    "pvals_2 = [0.07668660126327106, 0.035596693992742914, 0.02474465797607849, 0.04041795457599785, 0.02319916677865878, 0.1149711060341352, 0.02187206020696143, 0.021972853111140975, 0.030170675984410696, 0.0904280338664158, 0.030069883080231154, 0.017672355866147026, 0.03937642789947588, 0.03156497782556108, 0.1183812659588765, 0.07880325225104153, 0.043290552345114905, 0.08557317564843435, 0.053369842763069476, 0.02183846257223492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'blosum62.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-720d243671be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Original BLOSUM62 matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moriginal_blosum62\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blosum62.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msplit_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'blosum62.txt'"
     ]
    }
   ],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biological features\n",
    "# kd hydrophobicity scale\n",
    "hydrophobicity = {'G': -0.4, 'A': 1.8, 'L':3.8, 'M': 1.9, 'F':2.8, 'W':-0.9, 'K':-3.9, 'Q':-3.5, 'E':-3.5, 'S':-0.8, 'P':-1.6, 'V':4.2, 'I':4.5, 'C':2.5, 'Y':-1.3, 'H':-3.2, 'R':-4.5, 'N':-3.5, 'D':-3.5, 'T':-0.7}\n",
    "# Ranked\n",
    "polarity = {'A': 0.45, 'R': 0.75, 'N': 0.8, 'D': 0.95, 'C':0.35, 'Q': 0.85, 'E': 0.90, 'G': 0.55, 'H': 0.5, 'I': 0.05, 'L': 0.15, 'K': 1, 'M': 0.25, 'F': 0.1, 'P': 0.65, 'S': 0.7, 'T': 0.6, 'W': 0.3, 'Y': 0.4, 'V': 0.2}\n",
    "# Van der waal's volume\n",
    "volume = {'A': 67, 'R': 148, 'N': 96, 'D': 91, 'C':86, 'Q': 114, 'E': 109, 'G': 48, 'H': 118, 'I': 124, 'L': 124, 'K': 135, 'M': 124, 'F': 135, 'P': 90, 'S': 73, 'T': 93, 'W':163, 'Y':141, 'V': 105}\n",
    "\n",
    "# TODO: branched, hydrophilic, hydrophobic, shape\n",
    "charge = {'A': 2, 'R': 3, 'N': 2, 'D': 1, 'C': 2, 'E':1, 'Q':2, 'G':2, 'H': 3, 'I':2, 'L': 2, 'K': 3, 'M':2, 'F':2, 'P':2, 'S':2, 'T': 2, 'W':2, 'Y': 2, 'V':2 }\n",
    "\n",
    "# Normalize all of the biological features:\n",
    "# Charge\n",
    "for k,v in charge.items():\n",
    "    if v == 1:\n",
    "        charge[k] = 0\n",
    "    elif v == 2:\n",
    "        charge[k] = 0.5\n",
    "    else:\n",
    "        charge[k] = 1.0\n",
    "\n",
    "# Volume\n",
    "for k,v in volume.items():\n",
    "    volume[k] = v/163.0\n",
    "\n",
    "#Polarity\n",
    "min_v = -4.5\n",
    "max_v = 4.5\n",
    "for k,v in hydrophobicity.items():\n",
    "    hydrophobicity[k] = (v - min_v)/(max_v - min_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, filepath, distribution='NNK', negfilepath=None):\n",
    "        def construct_binary_dataset(filepath, distribution='NNK', negfilepath=None):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            pos_ds = []\n",
    "            neg_ds = []\n",
    "            gen_ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    pos_ds.append((aptamer, peptide, 1))\n",
    "                    gen_ds.append((get_x(), get_y(distribution), 0))\n",
    "            with open(negfilepath, 'r') as f:\n",
    "                neg_data = json.load(f)\n",
    "            for aptamer in neg_data:\n",
    "                peptides = neg_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    neg_ds.append((aptamer, peptide, 0))\n",
    "            \n",
    "            pos_ds = list(set(pos_ds)) #removed duplicates, random order\n",
    "            neg_ds = list(set(neg_ds))\n",
    "            gen_ds = list(set(gen_ds))\n",
    "            return pos_ds, neg_ds, gen_ds\n",
    "\n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y(distribution):\n",
    "            if distribution == 'NNK':\n",
    "                y_idx = np.random.choice(20, 7, p=pvals)\n",
    "                lst = aa_list\n",
    "            elif distribution == 'uniform':\n",
    "                y_idx = np.random.choice(20, 7, p=uniform_pvals)\n",
    "                lst = aa_list\n",
    "            elif distribution == 'new_nnk':\n",
    "                y_idx = np.random.choice(20, 7, p=pvals_2)\n",
    "                lst = aa_list_2\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += lst[i]\n",
    "            return y\n",
    "\n",
    "        self.pos_ds, self.neg_ds, self.gen_ds =construct_binary_dataset(filepath, distribution, negfilepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.pos_ds), len(self.neg_ds))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.pos_ds[idx], self.neg_ds[idx], self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        def construct_binary_dataset(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    ds.append((aptamer, peptide, 1))\n",
    "            ds = list(set(ds)) #removed duplicates, random order\n",
    "            return ds\n",
    "\n",
    "        self.binary_ds=construct_binary_dataset(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.binary_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AUCDataset(Dataset):\n",
    "    def __init__(self, filepath, negfilepath=None):\n",
    "        def construct_dataset(filepath, negfilepath=None):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            bio_ds = []\n",
    "            neg_ds = []\n",
    "            gen_ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    bio_ds.append((aptamer, peptide, 1))\n",
    "                    gen_ds.append((get_x(), get_y('NNK'), 0))\n",
    "            with open(negfilepath, 'r') as f:\n",
    "                neg_data = json.load(f)\n",
    "            for aptamer in neg_data:\n",
    "                peptides = neg_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    neg_ds.append((aptamer, peptide, 0))\n",
    "            bio_ds = list(set(bio_ds)) #removed duplicates, random order\n",
    "            gen_ds = list(set(gen_ds)) #removed duplicates, random order\n",
    "            neg_ds = list(set(neg_ds))\n",
    " \n",
    "            return bio_ds, neg_ds, gen_ds\n",
    "\n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "        \n",
    "         # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y(distribution='NNK'):\n",
    "            if distribution == 'NNK':\n",
    "                y_idx = np.random.choice(20, 7, p=pvals)\n",
    "                lst = aa_list\n",
    "            elif distribution == 'uniform':\n",
    "                y_idx = np.random.choice(20, 7, p=uniform_pvals)\n",
    "                lst = aa_list\n",
    "            elif distribution == 'new_nnk':\n",
    "                y_idx = np.random.choice(20, 7, p=pvals_2)\n",
    "                lst = aa_list_2\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += lst[i]\n",
    "            return y\n",
    "\n",
    "        self.bio_ds, self.neg_ds, self.gen_ds = construct_dataset(filepath, negfilepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.bio_ds), len(self.neg_ds))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.bio_ds[idx], self.neg_ds[idx], self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        def construct_generated_dataset(k):\n",
    "            S_new = []\n",
    "            for _, i in enumerate(tqdm.tqdm(range(k))):\n",
    "                pair = (get_x(), get_y())\n",
    "                S_new.append(pair)\n",
    "            np.random.shuffle(S_new)\n",
    "            return S_new\n",
    "        \n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "        self.gen_ds = construct_generated_dataset(n)\n",
    "    def __len__(self):\n",
    "        return len(self.gen_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/experimental_replicate_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-085fcf1a2d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbinary_ds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinaryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/experimental_replicate_1.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/neg_datasets/neg1_all_pairs_noArgi_noHis.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8049efc29642>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, distribution, negfilepath)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_ds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mconstruct_binary_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8049efc29642>\u001b[0m in \u001b[0;36mconstruct_binary_dataset\u001b[0;34m(filepath, distribution, negfilepath)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_binary_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                 \u001b[0maptamer_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mpos_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/experimental_replicate_1.json'"
     ]
    }
   ],
   "source": [
    "binary_ds=BinaryDataset(filepath=\"../data/experimental_replicate_1.json\", distribution='NNK', negfilepath='../data/neg_datasets/neg1_all_pairs_noArgi_noHis.json')\n",
    "n = len(binary_ds.pos_ds)\n",
    "m = int(0.8*n)\n",
    "train_pos = binary_ds.pos_ds[:m]\n",
    "val_pos = binary_ds.pos_ds[m:]\n",
    "n = len(binary_ds.neg_ds)\n",
    "m = int(0.8*n)\n",
    "train_neg = binary_ds.neg_ds[:m]\n",
    "val_neg = binary_ds.neg_ds[m:]\n",
    "n = len(binary_ds.gen_ds)\n",
    "m = int(0.8*n)\n",
    "train_gen = binary_ds.gen_ds[:m]\n",
    "val_gen = binary_ds.gen_ds[m:]\n",
    "#auc_ds = AUCDataset(filepath=\"../data/experimental_replicate_1.json\", distribution='NNK', negfilepath='../data/neg_datasets/neg1_all_pairs_noArgi_noHis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_ds_1 = AUCDataset(filepath='../data/experimental_replicate_2.json', negfilepath='../data/neg_datasets/neg2_all_pairs_noArgi_noHis.json')\n",
    "auc_ds_2 = AUCDataset(filepath='../data/experimental_replicate_3.json', negfilepath='../data/neg_datasets/neg3_all_pairs_noArgi_noHis.json')\n",
    "auc_ds_3 = AUCDataset(filepath='../data/experimental_replicate_4.json', negfilepath='../data/neg_datasets/neg4_all_pairs_noArgi_noHis.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#half_ds = HalfDataset(filepath=\"../data/higher_quality_data.json\")\n",
    "#eval_ds = EvalDataset(filepath=\"../data/experimental_replicate_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumConvNet, self).__init__()\n",
    "        self.name = \"BlosumConvNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 100, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(100, 200, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(200, 300, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 100, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 200, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(200, 300, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(300, 350, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1400, 700 )\n",
    "        self.fc2 = nn.Linear(700, 250)\n",
    "        self.fc3 = nn.Linear(250, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearBaseline, self).__init__()\n",
    "        self.name = \"LinearBaseline\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_1 = nn.Linear(320, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc_1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ConvBaseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBaseline, self).__init__()\n",
    "        self.name = \"ConvBaseline\"\n",
    "        self.single_alphabet = True\n",
    "        \n",
    "        self.cnn_1 = nn.Conv1d(24, 100, 3)\n",
    "        self.fc_1 = nn.Linear(4600, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "    \n",
    "    def forward(self, pair):\n",
    "        x = self.cnn_1(pair)\n",
    "        x = x.view(-1, 1).T\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(self.maxpool(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LinearTwoHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearTwoHead, self).__init__()\n",
    "        self.name = \"LinearTwoHead\"\n",
    "        self.single_alphabet=False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 150)\n",
    "        self.fc_apt_3 = nn.Linear(150, 100)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 250)\n",
    "        self.fc_pep_2 = nn.Linear(250, 100)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "\n",
    "        \n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.relu, self.fc_apt_2, self.relu,  self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.relu,  self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(200, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ConvTwoHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvTwoHead, self).__init__()\n",
    "        self.name = \"ConvTwoHead\"\n",
    "        self.single_alphabet=False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 35, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(35, 60, 3, padding=2) \n",
    "\n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 75, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(885, 400)\n",
    "        self.fc2 = nn.Linear(400, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "    \n",
    "## For aptamer sequence, translate\n",
    "## For peptide sequence, translate and add additional biological properties\n",
    "def extract_features(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "            encoding = np.zeros((4, len(sequence)))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                \n",
    "                idx = letters.index(char)\n",
    "                char_hydro = hydrophobicity[char]\n",
    "                char_polar = polarity[char]\n",
    "                char_vol = volume[char]\n",
    "                char_charge = charge[char]\n",
    "                \n",
    "                # Put in the biological features of the amino acids\n",
    "                encoding[0][i] = char_polar\n",
    "                encoding[1][i] = char_vol\n",
    "                encoding[2][i] = char_charge\n",
    "                encoding[3][i] = char_hydro\n",
    "        else:\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to directly translated sequence\n",
    "def translate(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        \n",
    "        encoding = np.zeros(len(apt) + len(pep))\n",
    "        \n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            idx = na_list.index(char)\n",
    "            encoding[i] = idx\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            idx = aa_list.index(char)\n",
    "            encoding[i+len(apt)] = idx\n",
    "        return encoding     \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        \n",
    "        encoding = np.zeros(len(sequence))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            idx = letters.index(char)\n",
    "            encoding[i] = idx\n",
    "        return encoding\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = one_hot([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[1], pair.shape[0]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        apt = one_hot(apt, seq_type='aptamer') \n",
    "        pep = one_hot(pep, seq_type='peptide') \n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 1, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 1, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=True\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out\n",
    "\n",
    "## Plotting functions\n",
    "def plot_loss(iters, train_losses, val_losses, model_name, model_id):\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/loss.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(iters, train_acc, val_acc, model_name, model_id):\n",
    "    plt.title(\"Training Accuracy Curve\")\n",
    "    plt.plot(train_acc, label=\"Train\")\n",
    "    plt.plot(val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/accuracy.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    \n",
    "    sns.distplot(train_gen_scores , color=\"skyblue\", label='Generated Train Samples', ax=ax)\n",
    "    sns.distplot(val_gen_scores, color='dodgerblue', label='Generated Validation Samples')\n",
    "    sns.distplot(train_scores , color=\"lightcoral\", label='Dataset Train Samples', ax=ax)\n",
    "    sns.distplot(val_scores, color='red', label='Dataset Validation Samples', ax=ax)\n",
    "    \n",
    "    ax.set_title(\"Categorizing the output scores of the model\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/histogram.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, iters, epoch, gamma, model_name, model_id):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Test CDF at epoch %d' %epoch + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/test_cdf.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, iters, epoch, gamma, model_name, model_id):\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" % iters)\n",
    "    plt.title('Train CDF at epoch %d' %epoch+ \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def auc_cdf(train, new, model_name, model_id, val=False): \n",
    "    #train is the sorted list of outputs from the model with training pairs as inputs\n",
    "    #new is the list of outputs from the model with generated pairs as inputs\n",
    "    a = train + new\n",
    "    n = len(a)\n",
    "    m = len(train)\n",
    "    train = np.asarray(train)\n",
    "    new = np.asarray(new)\n",
    "    y = np.arange(0, m+2)/m\n",
    "    gamma = [0]\n",
    "    for x in train:\n",
    "        gamma.append(sum(a<=x)/n)\n",
    "    gamma.append(1)\n",
    "    plt.plot(gamma, y)\n",
    "    if val:\n",
    "        plt.title(\"Validation CDF\")\n",
    "    else:\n",
    "        plt.title(\"Train CDF\")\n",
    "    plt.xlim([0,1])\n",
    "    if val:\n",
    "        plt.savefig('plots/binary/%s/%s/val_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig('plots/binary/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return np.trapz(y, gamma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test the regular translate method\n",
    "oh = blosum62_encoding('LL', seq_type='peptide')\n",
    "oh.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test the translate method with single alphabet\n",
    "apt, pep, label = convert(\"GGGG\", \"LL\", label=1.0, single_alphabet=False)\n",
    "pep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if device == torch.cuda:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def classifier(model, \n",
    "               pos, \n",
    "               neg,\n",
    "               gen,\n",
    "               val_pos,\n",
    "               val_neg,\n",
    "               val_gen,\n",
    "               lr,\n",
    "               model_id,\n",
    "               num_epochs=50,\n",
    "               batch_size=16,\n",
    "               single_alphabet=False,\n",
    "               run_from_checkpoint=None, \n",
    "               save_checkpoints=None,\n",
    "               cdf=False):\n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        init_epoch = checkpoint['epoch'] +1\n",
    "        print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "        init_epoch = 0\n",
    "    \n",
    "    train_losses, val_losses, train_losses_avg, val_losses_avg, train_acc, val_acc = [], [], [], [], [], []\n",
    "    \n",
    "    iters, train_correct, val_correct = 0, 0, 0\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.9) #Decays lr by gamma factor every step_size epochs. \n",
    "    \n",
    "    # Keep track of the scores across four classes\n",
    "    train_scores, train_gen_scores, val_scores, val_gen_scores = [], [], [], []\n",
    "    \n",
    "    # TP, TN, FP, FN\n",
    "    train_stats = [0, 0, 0, 0]\n",
    "    val_stats = [0, 0, 0, 0]\n",
    "    \n",
    "    # Number of times that val_score < train_score\n",
    "    auc_tracker = 0\n",
    "    auc_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        full_epoch = epoch + init_epoch\n",
    "        print(\"Starting epoch: %d\" % full_epoch, \" with learning rate: \", scheduler.get_lr())\n",
    "        #for i, (apt, pep, label) in enumerate(tqdm.tqdm(pos)):\n",
    "        for i in tqdm.tqdm(range(100000)):\n",
    "            # Sample random number between 0-1\n",
    "            num = random.uniform(0, 1)\n",
    "            # Pick pos sample with prob 1/2 (this should be threshold)\n",
    "            if num < 0.5:\n",
    "                apt,pep,label = random.choice(pos)\n",
    "            elif num >= 0.5 and num < 0.5+(alpha/2):\n",
    "                apt,pep,label = random.choice(neg)\n",
    "            else:\n",
    "            #elif num >= 0.5 and num < 0.5+(beta/2):\n",
    "                apt,pep,label = random.choice(gen)\n",
    "            \n",
    "            # Pick \n",
    "            model_name = model.name\n",
    "            model.train()\n",
    "            if single_alphabet:\n",
    "                p, l = convert(apt, pep, label, single_alphabet=True)\n",
    "                train_score = update(None, None, p, single_alphabet=True)\n",
    "            else:\n",
    "                a, p, l = convert(apt, pep, label, single_alphabet=False)\n",
    "                train_score = update(a, p, None, single_alphabet=False)\n",
    "                \n",
    "            lam = 1\n",
    "            # The original label is non binding\n",
    "            if label == 0.0:\n",
    "                train_gen_scores.append(train_score.item())\n",
    "                # Score is non binding\n",
    "                if train_score.item() < 0.5:\n",
    "                    train_stats[1] += 1\n",
    "                    train_correct += 1\n",
    "                # Score is binding\n",
    "                else:\n",
    "                    lam = lambda_val\n",
    "                    train_stats[2] += 1\n",
    "            # The original label is binding\n",
    "            elif label == 1.0:\n",
    "                train_scores.append(train_score.item())\n",
    "                # The score is binding\n",
    "                if train_score.item() >= 0.5:\n",
    "                    train_correct += 1\n",
    "                    train_stats[0] += 1\n",
    "                # The score is nonbinding\n",
    "                else:\n",
    "                    train_stats[3] += 1\n",
    "                    lam = lambda_val\n",
    "                \n",
    "            iters += 1\n",
    "            train_loss = criterion(train_score, l)\n",
    "            total_train_loss += lam * train_loss\n",
    "\n",
    "            \n",
    "            if iters % batch_size == 0:\n",
    "                ave_train_loss = total_train_loss/batch_size\n",
    "                train_losses.append(ave_train_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                ave_train_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if iters % 5000 == 0:\n",
    "                train_acc.append(100*train_correct/iters)\n",
    "                train_losses_avg.append(np.average(train_losses[-5000:]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "            \n",
    "            num = random.uniform(0, 1)\n",
    "            # Pick pos sample with prob 1/2\n",
    "            if num < 0.5:\n",
    "                apt,pep,label = random.choice(val_pos)\n",
    "            elif num >= 0.5 and num < 0.5+(alpha/2):\n",
    "                apt,pep,label = random.choice(val_neg)\n",
    "            else:\n",
    "                apt,pep,label = random.choice(val_gen)\n",
    "            if single_alphabet:\n",
    "                p_val, l_val = convert(apt, pep, label, single_alphabet=True)\n",
    "                val_score = model(p_val)\n",
    "            else:\n",
    "                a_val, p_val, l_val = convert(apt,pep,label)\n",
    "                val_score = model(a_val, p_val)\n",
    "            \n",
    "            \n",
    "#             # Calculate AUC\n",
    "#             auc_count += 1\n",
    "#             idx = int(random.random()*len(auc_ds))\n",
    "#             bio_sample, gen_sample = auc_ds[idx]\n",
    "#             if single_alphabet:\n",
    "#                 p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2], single_alphabet=True)\n",
    "#                 bio_score = model(p)\n",
    "#                 p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2], single_alphabet=True)\n",
    "#                 gen_score = model(p)\n",
    "#             else:\n",
    "#                 a, p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2])\n",
    "#                 bio_score = model(a, p)\n",
    "#                 a, p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2])\n",
    "#                 gen_score = model(a, p)\n",
    "#             if bio_score > gen_score:\n",
    "#                 auc_tracker += 1\n",
    "            \n",
    "            lam = 1\n",
    "            if l_val.item() == 0.0:\n",
    "                val_gen_scores.append(val_score.item())\n",
    "                if val_score.item() < 0.5:\n",
    "                    val_stats[1] += 1\n",
    "                    val_correct += 1\n",
    "                else:\n",
    "                    lam = lambda_val\n",
    "                    val_stats[2] += 1\n",
    "            if l_val.item() == 1.0:\n",
    "                val_scores.append(val_score.item())\n",
    "                if val_score.item() >= 0.5:\n",
    "                    val_stats[0] += 1\n",
    "                    val_correct += 1\n",
    "                else:\n",
    "                    val_stats[3] += 1\n",
    "                    lam = lambda_val\n",
    "\n",
    "            val_loss = criterion(val_score, l_val) \n",
    "            total_val_loss += lam * val_loss\n",
    "\n",
    "            if iters % batch_size == 0:\n",
    "                ave_val_loss = total_val_loss/batch_size\n",
    "                val_losses.append(ave_val_loss.item())\n",
    "                total_val_loss = 0\n",
    "            if iters % 5000 == 0:\n",
    "                val_acc.append(100*val_correct/iters)\n",
    "                val_losses_avg.append(np.average(val_losses[-5000:]))\n",
    "                \n",
    "            if iters % 50000 == 0:\n",
    "                plot_loss(iters, train_losses_avg, val_losses_avg, model_name, model_id)\n",
    "                plot_accuracy(iters, train_acc, val_acc, model_name, model_id)\n",
    "                plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id)\n",
    "                #val_auc = auc_cdf(sorted(val_scores[-1000:]), sorted(val_gen_scores[-10000:]), model_name, model_id, val=True)\n",
    "                #train_auc = auc_cdf(sorted(train_scores[-1000:]), sorted(train_gen_scores[-10000:]), model_name, model_id)\n",
    "                \n",
    "                print(\"Training Accuracy at epoch %d: {}\".format(train_acc[-1]) %full_epoch)\n",
    "                print(\"Validation Accuracy epoch %d: {}\".format(val_acc[-1]) %full_epoch)\n",
    "                #print(\"Training CDF at epoch %d: {}\".format(train_auc) % full_epoch)\n",
    "                #print(\"Validation CDF epoch %d: {}\".format(val_auc) % full_epoch)\n",
    "                #print(\"AUC epoch %d: {}\".format(auc_tracker/float(auc_count)) % full_epoch)\n",
    "                #print(\"Train: Sensitivity: \" + str(train_stats[0]/(train_stats[0] + train_stats[3])) + \" Specificity: \" + str(train_stats[1]/(train_stats[2] + train_stats[1])))\n",
    "                #print(\"Val: Sensitivity: \" + str(val_stats[0]/(val_stats[0] + val_stats[3])) + \" Specificity: \" + str(val_stats[1]/(val_stats[2] + val_stats[1])))\n",
    "                # TP, TN, FP, FN\n",
    "                print(\"Train Precision: \", train_stats[0]/(train_stats[0] + train_stats[2]))\n",
    "                print(\"Train Recall: \", train_stats[0]/(train_stats[0] + train_stats[3]))\n",
    "\n",
    "        scheduler.step()\n",
    "        if save_checkpoints is not None:\n",
    "            print(\"Saving to: \", save_checkpoints)\n",
    "            checkpoint_name = save_checkpoints\n",
    "            torch.save({'epoch': full_epoch,\n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict()}, checkpoint_name)\n",
    "        \n",
    "        # Clear unused gpu memory at the end of the epoch\n",
    "        if device == torch.cuda:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "model = LinearTwoHead()\n",
    "model_name = model.name\n",
    "model_id = \"09222020\"\n",
    "model.to(device)\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "cdf=False\n",
    "if cdf:\n",
    "   # For the CDF functions, we need to generate a dataset of new examples\n",
    "    S_new = GeneratedDataset(10*m) \n",
    "gamma = 1e-2\n",
    "print(\"Using lambda val=\", lambda_val)\n",
    "print(\"Using encoding_style=\", encoding_style)\n",
    "classifier(model, train_pos, train_neg, train_gen, val_pos, val_neg, val_gen, gamma, model_id, NUM_EPOCHS, BATCH_SIZE, model.single_alphabet, checkpoint, save_path, cdf=cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     36,
     74,
     86,
     168
    ]
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = BlosumConvNet()\n",
    "model_name = model.name\n",
    "model.to(device)\n",
    "checkpointed_model = 'model_checkpoints/binary/%s/%s.pth' % (model_name, \"09152020\")\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "optimizer = SGD(model.parameters(), lr=1e-3)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "init_epoch = checkpoint['epoch'] +1\n",
    "print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate AUC\n",
    "val_ds = [auc_ds_1, auc_ds_2, auc_ds_3]\n",
    "for ds in val_ds:\n",
    "    auc_tracker = 0\n",
    "    for i in range(10000):\n",
    "        idx = int(random.random()*len(ds))\n",
    "        bio_sample, gen_sample = ds[idx]\n",
    "        if model.single_alphabet:\n",
    "            p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2], single_alphabet=True)\n",
    "            bio_score = model(p)\n",
    "            p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2], single_alphabet=True)\n",
    "            gen_score = model(p)\n",
    "        else:\n",
    "            a, p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2])\n",
    "            bio_score = model(a, p)\n",
    "            a, p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2])\n",
    "            gen_score = model(a, p)\n",
    "        if bio_score > 0.5:\n",
    "            auc_tracker += 1\n",
    "        if gen_score < 0.5:\n",
    "            auc_tracker += 1\n",
    "\n",
    "    print(\"Test Accuracy: \", auc_tracker/20000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the decision threshold\n",
    "def evaluate(model_name, model_id, dataset=\"ds2\", use_gen=False):\n",
    "    if model_name == \"ConvBaseline\":\n",
    "        model = ConvBaseline()\n",
    "    elif model_name == \"LinearBaseline\":\n",
    "        model = LinearBaseline()\n",
    "    elif model_name == \"LinearTwoHead\":\n",
    "        model = LinearTwoHead()\n",
    "    model.to(device)\n",
    "    checkpointed_model = 'model_checkpoints/binary/%s/%s.pth' % (model.name, model_id)\n",
    "    checkpoint = torch.load(checkpointed_model)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    if dataset==\"ds2\":\n",
    "        ds = auc_ds_1\n",
    "    elif dataset=='ds3':\n",
    "        ds = auc_ds_2\n",
    "    elif dataset=='ds4':\n",
    "        ds = auc_ds_3\n",
    "    thresholds = [i for i in range(1, 100)]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    for i, t in enumerate(tqdm.tqdm(thresholds)):\n",
    "        t = t*0.01\n",
    "        tp, tn, fp, fn = 0, 0, 0, 0\n",
    "        for i in range(100):\n",
    "            idx = int(random.random()*len(ds))\n",
    "            bio_sample, neg_sample, gen_sample = ds[idx]\n",
    "            if model.single_alphabet:\n",
    "                p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2], single_alphabet=True)\n",
    "                bio_score = model(p)\n",
    "                if use_gen:\n",
    "                    p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2], single_alphabet=True)\n",
    "                else:\n",
    "                    p, l = convert(neg_sample[0], neg_sample[1], gen_sample[2], single_alphabet=True)\n",
    "                gen_score = model(p) \n",
    "            else:\n",
    "                a, p, l = convert(bio_sample[0], bio_sample[1], bio_sample[2])\n",
    "                bio_score = model(a, p)\n",
    "                if use_gen:\n",
    "                    a, p, l = convert(gen_sample[0], gen_sample[1], gen_sample[2])\n",
    "                else:\n",
    "                    a, p, l = convert(neg_sample[0], neg_sample[1], gen_sample[2])\n",
    "                gen_score = model(a, p)\n",
    "            if bio_score >= t:\n",
    "                tp += 1\n",
    "            elif bio_score < t:\n",
    "                fn += 1    \n",
    "            if gen_score < t:\n",
    "                tn += 1\n",
    "            elif gen_score >= t:\n",
    "                fp += 1\n",
    "\n",
    "\n",
    "        try:\n",
    "            precision = tp/float(tp + fp)\n",
    "        except:\n",
    "            precision = tp/1\n",
    "        try:\n",
    "            recall = tp/float(tp + fn)\n",
    "        except:\n",
    "            recall = tp/1\n",
    "        #print(\"Precision: \", precision)\n",
    "        #print(\"Recall: \", recall)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        # ROC stats\n",
    "        fpr = fp/float(fp+tn)\n",
    "        tpr = tp/float(tp+fn)\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        \n",
    "#     plt.scatter(fprs, tprs)\n",
    "#     plt.title(\"ROC Curve for \" + str(dataset))\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.plot([0, 1], [0,1])\n",
    "#     plt.savefig('plots/binary/%s/%s/roc_%s.png' % (model_name, model_id, dataset), bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "    \n",
    "#     plt.scatter(recalls, precisions, c=\"orange\")\n",
    "#     plt.title(\"Precision/Recall Curve for \" + str(dataset))\n",
    "#     plt.xlabel(\"Recall\")\n",
    "#     plt.ylabel(\"Precision\")\n",
    "#     plt.savefig('plots/binary/%s/%s/roc_%s.png' % (model_name, model_id, dataset), bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "    \n",
    "    roc_curve = zip(fprs, tprs)\n",
    "    pr_curve = zip(recalls, precisions)\n",
    "    \n",
    "    roc_curve = sorted(roc_curve, key=lambda x: x[0])\n",
    "    pr_curve = sorted(pr_curve, key=lambda x: x[0])\n",
    "    \n",
    "    roc_curve = list(zip(*roc_curve))\n",
    "    pr_curve = list(zip(*pr_curve))\n",
    "    return list(roc_curve[0]), list(roc_curve[1]), list(pr_curve[0]), list(pr_curve[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate ROC Curves for each method\n",
    "methods = {'LinearBaseline': ['09202020', '09202020_2'],\n",
    "           'ConvBaseline': ['09202020', '09202020_2'],\n",
    "           'LinearTwoHead': ['09202020_2', '09202020']}\n",
    "\n",
    "colors = [\"lightcoral\", \"crimson\", \"lightgreen\", \"powderblue\", \"mediumpurple\", \"gold\"]\n",
    "use_gen = True\n",
    "for ds in [\"ds2\", \"ds3\", \"ds4\"]:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i, network in enumerate(methods):\n",
    "        blosum_method = methods[network][0]\n",
    "        oh_method = methods[network][1]\n",
    "        \n",
    "        fprs, tprs, recalls, precisions = evaluate(network, blosum_method, ds, use_gen=use_gen)\n",
    "        #plt.scatter(recalls, precisions, c=colors[2*i], label=str(network) + \" blosum\")\n",
    "        #plt.plot(recalls, precisions, c=colors[2*i], label=str(network) + \" blosum\")\n",
    "        plt.plot(fprs, tprs, c=colors[2*i], label=str(network) + \" blosum \")\n",
    "        \n",
    "        fprs, tprs, recalls, precisions = evaluate(network, oh_method, ds, use_gen=use_gen)\n",
    "        #plt.scatter(recalls, precisions, c=colors[2*i + 1], label=str(network) + \" oh\")\n",
    "        #plt.plot(recalls, precisions, c=colors[2*i + 1], label=str(network) + \" oh\")\n",
    "        plt.plot(fprs, tprs, c=colors[2*i + 1], label=str(network) + \" oh \")\n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.legend()\n",
    "    #plt.title(\"Precision/Recall Curve for \" + str(ds))\n",
    "#     plt.xlabel(\"Recall\")\n",
    "#     plt.ylabel(\"Precision\")\n",
    "    \n",
    "    plt.title(\"ROC Curve for \" + str(ds) + \" using generated data=\" + str(use_gen))\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"ConvBaseline\", \"09202020\", \"ds2\")\n",
    "evaluate(\"ConvBaseline\", \"09202020\", \"ds3\")\n",
    "evaluate(\"ConvBaseline\", \"09202020\", \"ds4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walnutree",
   "language": "python",
   "name": "walnutree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
