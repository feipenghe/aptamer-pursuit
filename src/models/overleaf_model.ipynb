{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data --> one hot encoding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide, _ in peptides:\n",
    "            if '_' in peptide:\n",
    "                split = peptide.split('_')\n",
    "                save = split[0]\n",
    "                if len(save) < 8:\n",
    "                    continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8:\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = construct_dataset()\n",
    "random.shuffle(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*len(full_dataset))]\n",
    "test_set = full_dataset[int(0.2*len(full_dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AptamerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(AptamerDataset, self).__init__()\n",
    "        self.training_set = training_set\n",
    "        num_batches = int(len(training_set)/BATCH_SIZE)\n",
    "        self.training_set = training_set[:int(num_batches * BATCH_SIZE)]\n",
    "        \n",
    "    def __len__(self):\n",
    "         return len(self.training_set)\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AptamerDataset(training_set)\n",
    "test_dataset = AptamerDataset(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y']\n",
    "na_list = ['A', 'C', 'G', 'T']\n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence_list, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    \n",
    "    one_hot = np.zeros((len(sequence_list), len(sequence_list[0]), len(letters)))\n",
    "    for j in range(len(sequence_list)):\n",
    "        sequence = sequence_list[j]\n",
    "        for i in range(len(sequence)):\n",
    "            element = sequence[i]\n",
    "            idx = letters.index(element)\n",
    "            one_hot[j][i][idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model --> CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayer, self).__init__()\n",
    "        self.linear_apt_1 = nn.Linear(40, 40)\n",
    "        self.linear_apt_2 = nn.Linear(40, 1)\n",
    "        \n",
    "        self.linear_pep_1 = nn.Linear(8, 8)\n",
    "        self.linear_pep_2 = nn.Linear(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.sequential_pep = nn.Sequential(self.linear_pep_1,\n",
    "                                            self.relu,\n",
    "                                            self.linear_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.linear_apt_1,\n",
    "                                            self.relu,\n",
    "                                            self.linear_apt_2)\n",
    "                \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        print(apt.shape())\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        l = nn.MSELoss()\n",
    "        label = torch.FloatTensor(label)\n",
    "        label = label.reshape((1, 1))\n",
    "        return l(torch.FloatTensor(prediction), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(BATCH_SIZE, 40, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(40, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(BATCH_SIZE, 8, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(8, 1, 1)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_2, \n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(209, BATCH_SIZE)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        l = nn.MSELoss()\n",
    "        label = torch.FloatTensor(label)\n",
    "        return l(torch.FloatTensor(prediction), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(batch_size=BATCH_SIZE)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "\n",
    "#model_2 = TwoLayer()\n",
    "# def weights_init_2(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "#         nn.init.zeros_(m.bias.data)\n",
    "\n",
    "model.apply(weights_init)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "import tqdm\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Come up with a trainloader\n",
    "    for batch_idx, (aptamer, peptide) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        # Peptide and aptamer, one-hot encode them \n",
    "        pep = one_hot(peptide, seq_type='peptide')\n",
    "        apt = one_hot(aptamer, seq_type='aptamer')\n",
    "        \n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1], pep.shape[2])))\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1], apt.shape[2])))\n",
    "        \n",
    "        output = model(apt, pep)\n",
    "        \n",
    "        label = np.ones((1, BATCH_SIZE))\n",
    "        loss = model.loss(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #hyperparameter\n",
    "        clip = 5\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.7f' %\n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 10*BATCH_SIZE))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation --> compare to random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for batch_idx, (aptamer, peptide) in enumerate(tqdm.tqdm(test_loader)):\n",
    "    pep = one_hot(peptide, seq_type='peptide')\n",
    "    apt = one_hot(aptamer, seq_type='aptamer')\n",
    "    \n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1], pep.shape[2])))\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1], apt.shape[2])))\n",
    "\n",
    "    output = model(apt, pep).detach().numpy().flatten()\n",
    "    for i in range(output.shape[0]):\n",
    "        o = output[i]\n",
    "        if o > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    break\n",
    "\n",
    "print('Recall of the network on the test samples: %d %%' % (100* correct/(correct + incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling --> need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1e5\n",
    "\n",
    "# Generate uniformly without replacement\n",
    "def get_samples(kind=\"pep\",num=k):\n",
    "    if kind == \"apt\":\n",
    "        samples = [all_aptamers[i] for i in np.random.choice(len(all_aptamers), num_samples, replace=False)]\n",
    "    else:\n",
    "        samples = [all_peptides[i] for i in np.random.choice(len(all_peptides), num_samples, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# Sample x' from P_X (assume peptides follow NNK)\n",
    "def get_x_prime(k):\n",
    "    x_primes = []\n",
    "    for _ in range(k):\n",
    "        pvals = [0.089]*3 + [0.065]*5 + [0.034]*12\n",
    "        x_idx = np.random.choice(20, 7, p=pvals)\n",
    "        x_prime = \"M\"\n",
    "        for i in x_idx:\n",
    "            x_prime += aa_list[i]\n",
    "        x_primes.append(x_prime)\n",
    "    return x_primes\n",
    "\n",
    "# Sample y' from P_Y (assume apatamers follow uniform)\n",
    "def get_y_prime(k):\n",
    "    y_primes = []\n",
    "    for _ in range(k):\n",
    "        y_idx = np.random.randint(0, 4, 40)\n",
    "        y_prime = \"\"\n",
    "        for i in y_idx:\n",
    "            y_prime += na_list[i]\n",
    "        y_primes.append(y_prime)\n",
    "    return y_primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
