{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data --> one hot encoding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "'''\n",
    "Constructs a dataset that has 10,000 pairs for every class of binding affinity. \n",
    "'''\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    \n",
    "    # Full dataset. The index of the list corresponds to the binding affinity class\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for p, b in peptides:\n",
    "            if len(aptamer) == 40 and len(p) == 8:\n",
    "                full_dataset.append((aptamer, p))\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = construct_dataset()\n",
    "random.shuffle(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*len(full_dataset))]\n",
    "test_set = full_dataset[int(0.8*len(full_dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y']\n",
    "        one_hot_peptide = np.zeros((len(sequence), len(aa_list)))\n",
    "        for i in range(len(sequence)):\n",
    "            aa = sequence[i]\n",
    "            idx = aa_list.index(aa)\n",
    "            one_hot_peptide[i][idx] = 1\n",
    "        return one_hot_peptide\n",
    "    else:\n",
    "        na_list = ['A', 'C', 'G', 'T']\n",
    "        one_hot_aptamer = np.zeros((len(sequence), len(na_list)))\n",
    "        for i in range(len(sequence)):\n",
    "            na = sequence[i]\n",
    "            idx = na_list.index(na)\n",
    "            one_hot_aptamer[i][idx] = 1\n",
    "        return one_hot_aptamer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model --> CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 40, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(40, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 8, 3)\n",
    "        self.cnn_pep_2 = nn.Conv2d(8, 1, 3)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2)\n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu, self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(224, 1)\n",
    "       \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        \n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    def loss(self, prediction, label):\n",
    "        l = nn.MSELoss()\n",
    "        return l(prediction, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/620255 [00:00<41:22, 249.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 221583/620255 [12:28<22:27, 295.81it/s] "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "import tqdm\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()\n",
    "    # Come up with a trainloader\n",
    "    for i, data in enumerate(tqdm.tqdm(training_set)):\n",
    "        # Peptide and aptamer, one-hot encode them\n",
    "        pep = training_set[i][1]\n",
    "        apt = training_set[i][0]\n",
    "        \n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = one_hot(apt, seq_type='aptamer')\n",
    "        \n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1])))\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1])))\n",
    "        \n",
    "        output = model(apt, pep)\n",
    "        loss = model.loss(output, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation --> compare to random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for i, data in enumerate(tqdm.tqdm(testing_set)):\n",
    "    pep = testing_set[i][1]\n",
    "    apt = testing_set[i][0]\n",
    "    \n",
    "    pep = one_hot(pep, seq_type='peptide')\n",
    "    apt = one_hot(apt, seq_type='aptamer')\n",
    "\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1])))\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1])))\n",
    "\n",
    "    output = model(apt, pep)\n",
    "    if output > 0.5:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100* correct/(correct + incorrect)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
