{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data --> one hot encoding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "'''\n",
    "Constructs a dataset that has 10,000 pairs for every class of binding affinity. \n",
    "'''\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    \n",
    "    # Full dataset. The index of the list corresponds to the binding affinity class\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for p, b in peptides:\n",
    "            full_dataset.append((aptamer, p))\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = construct_dataset()\n",
    "random.shuffle(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*len(full_dataset))]\n",
    "test_set = full_dataset[int(0.8*len(full_dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot_peptide(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y']\n",
    "        one_hot_peptide = np.zeros((len(sequence), len(aa_list)))\n",
    "        for i in range(len(sequence)):\n",
    "            aa = sequence[i]\n",
    "            idx = aa_list.index(aa)\n",
    "            one_hot_peptide[i][idx] = 1\n",
    "        return one_hot_peptide\n",
    "    else:\n",
    "        na_list = ['A', 'C', 'G', 'T']\n",
    "        one_hot_aptamer = np.zeros((len(sequence), len(na_list)))\n",
    "        for i in range(len(sequence)):\n",
    "            na = sequence[i]\n",
    "            idx = na_list.index(na)\n",
    "            one_hot_aptamer[i][idx] = 1\n",
    "        return one_hot_aptamer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model --> CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, d_value):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 40)\n",
    "        self.cnn_apt_2 = nn.Conv2d(40, 10)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 8)\n",
    "        self.cnn_pep_2 = nn.Conv2d(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU(num_parameters=1)\n",
    "        \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2)\n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu, self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "       \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.type(torch.FloatTensor)\n",
    "        pep = pep.type(torch.FloatTensor)\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "import tqdm\n",
    "weights = np.zeros(())\n",
    "biases = np.zeros(())\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()\n",
    "    # Come up with a trainloader\n",
    "    for i, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "        # Peptide and aptamer, one-hot encode them\n",
    "        pep = data['peptide']\n",
    "        apt = data['aptamer']\n",
    "        \n",
    "        output = model(pep, apt)\n",
    "        \n",
    "        # Calculate g_k\n",
    "        g_k = \n",
    "        \n",
    "        # Update the weights\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation --> compare to random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
