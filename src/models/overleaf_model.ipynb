{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data --> one hot encoding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide, _ in peptides:\n",
    "            if '_' in peptide:\n",
    "                split = peptide.split('_')\n",
    "                save = split[0]\n",
    "                if len(save) < 8:\n",
    "                    continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8:\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = construct_dataset()\n",
    "random.shuffle(full_dataset)\n",
    "training_set = full_dataset[:int(0.5*len(full_dataset))]\n",
    "test_set = full_dataset[int(0.5*len(full_dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AptamerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(AptamerDataset, self).__init__()\n",
    "        self.training_set = training_set\n",
    "        num_batches = int(len(training_set)/BATCH_SIZE)\n",
    "        self.training_set = training_set[:int(num_batches * BATCH_SIZE)]\n",
    "        \n",
    "    def __len__(self):\n",
    "         return len(self.training_set)\n",
    "         \n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AptamerDataset(training_set)\n",
    "test_dataset = AptamerDataset(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y']\n",
    "na_list = ['A', 'C', 'G', 'T']\n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence_list, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    \n",
    "    one_hot = np.zeros((len(sequence_list), len(sequence_list[0]), len(letters)))\n",
    "    for j in range(len(sequence_list)):\n",
    "        sequence = sequence_list[j]\n",
    "        for i in range(len(sequence)):\n",
    "            element = sequence[i]\n",
    "            idx = letters.index(element)\n",
    "            one_hot[j][i][idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model --> CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayer, self).__init__()\n",
    "        self.linear_apt_1 = nn.Linear(40, 40)\n",
    "        self.linear_apt_2 = nn.Linear(40, 1)\n",
    "        \n",
    "        self.linear_pep_1 = nn.Linear(8, 8)\n",
    "        self.linear_pep_2 = nn.Linear(8, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.sequential_pep = nn.Sequential(self.linear_pep_1,\n",
    "                                            self.relu,\n",
    "                                            self.linear_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.linear_apt_1,\n",
    "                                            self.relu,\n",
    "                                            self.linear_apt_2)\n",
    "                \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        print(apt.shape())\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        l = nn.MSELoss()\n",
    "        label = torch.FloatTensor(label)\n",
    "        label = label.reshape((1, 1))\n",
    "        return l(torch.FloatTensor(prediction), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(BATCH_SIZE, 40, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(40, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(BATCH_SIZE, 8, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(8, 1, 1)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_2, \n",
    "                                            #self.dropout,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(209, BATCH_SIZE)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, prediction, label):\n",
    "        l = nn.MSELoss()\n",
    "        label = torch.FloatTensor(label)\n",
    "        return l(torch.FloatTensor(prediction), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(batch_size=BATCH_SIZE)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "\n",
    "#model_2 = TwoLayer()\n",
    "# def weights_init_2(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "#         nn.init.zeros_(m.bias.data)\n",
    "\n",
    "model.apply(weights_init)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/387 [00:00<?, ?it/s]/Users/aishwaryamandyam/anaconda3/envs/research/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/aishwaryamandyam/anaconda3/envs/research/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  1%|          | 2/387 [00:00<00:31, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/387 [00:00<00:23, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 242.7945644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 23/387 [00:01<00:20, 17.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 114.5482019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 33/387 [00:02<00:19, 17.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 20.0990237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 43/387 [00:02<00:19, 17.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    40] loss: 4.2289428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 53/387 [00:03<00:18, 17.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 3.1223967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 63/387 [00:03<00:18, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    60] loss: 2.9928298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 73/387 [00:04<00:17, 17.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    70] loss: 2.7261840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 83/387 [00:04<00:16, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    80] loss: 2.0039873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 93/387 [00:05<00:15, 18.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    90] loss: 1.9980032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 103/387 [00:05<00:16, 17.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.9957053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 113/387 [00:06<00:16, 17.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   110] loss: 1.8483350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 123/387 [00:07<00:15, 17.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   120] loss: 0.9361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 133/387 [00:07<00:14, 17.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   130] loss: 0.0540376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 143/387 [00:08<00:14, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   140] loss: 0.0000982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 153/387 [00:08<00:13, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   150] loss: 0.0001099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 163/387 [00:09<00:13, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   160] loss: 0.0001251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 173/387 [00:09<00:12, 17.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   170] loss: 0.0001476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 183/387 [00:10<00:11, 17.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   180] loss: 0.0001614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 193/387 [00:11<00:11, 17.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   190] loss: 0.0002004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 203/387 [00:11<00:09, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.0002567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 213/387 [00:12<00:10, 17.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   210] loss: 0.0003003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 223/387 [00:12<00:08, 18.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   220] loss: 0.0004150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 233/387 [00:13<00:08, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   230] loss: 0.0005583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 243/387 [00:13<00:07, 18.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   240] loss: 0.0008001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 249/387 [00:14<00:07, 18.30it/s]"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "import tqdm\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Come up with a trainloader\n",
    "    for batch_idx, (aptamer, peptide) in enumerate(tqdm.tqdm(train_loader)):\n",
    "        # Peptide and aptamer, one-hot encode them \n",
    "        pep = one_hot(peptide, seq_type='peptide')\n",
    "        apt = one_hot(aptamer, seq_type='aptamer')\n",
    "        \n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1], pep.shape[2])))\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1], apt.shape[2])))\n",
    "        \n",
    "        output = model(apt, pep)\n",
    "        \n",
    "        label = np.ones((1, BATCH_SIZE))\n",
    "        loss = model.loss(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #hyperparameter\n",
    "        clip = 5\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.7f' %\n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 10*BATCH_SIZE))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation --> compare to random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for batch_idx, (aptamer, peptide) in enumerate(tqdm.tqdm(test_loader)):\n",
    "    pep = one_hot(peptide, seq_type='peptide')\n",
    "    apt = one_hot(aptamer, seq_type='aptamer')\n",
    "    \n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1], pep.shape[2])))\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1], apt.shape[2])))\n",
    "\n",
    "    output = model(apt, pep).detach().numpy().flatten()\n",
    "    for i in range(output.shape[0]):\n",
    "        o = output[i]\n",
    "        if o > 0.5:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    break\n",
    "\n",
    "print('Accuracy of the network on the test samples: %d %%' % (100* correct/(correct + incorrect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling --> need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1e5\n",
    "\n",
    "# Generate uniformly without replacement\n",
    "def get_samples(kind=\"pep\",num=k):\n",
    "    if kind == \"apt\":\n",
    "        samples = [all_aptamers[i] for i in np.random.choice(len(all_aptamers), num_samples, replace=False)]\n",
    "    else:\n",
    "        samples = [all_peptides[i] for i in np.random.choice(len(all_peptides), num_samples, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# Sample x' from P_X (assume peptides follow NNK)\n",
    "def get_x_prime(k):\n",
    "    x_primes = []\n",
    "    for _ in range(k):\n",
    "        pvals = [0.089]*3 + [0.065]*5 + [0.034]*12\n",
    "        x_idx = np.random.choice(20, 7, p=pvals)\n",
    "        x_prime = \"M\"\n",
    "        for i in x_idx:\n",
    "            x_prime += aa_list[i]\n",
    "        x_primes.append(x_prime)\n",
    "    return x_primes\n",
    "\n",
    "# Sample y' from P_Y (assume apatamers follow uniform)\n",
    "def get_y_prime(k):\n",
    "    y_primes = []\n",
    "    for _ in range(k):\n",
    "        y_idx = np.random.randint(0, 4, 40)\n",
    "        y_prime = \"\"\n",
    "        for i in y_idx:\n",
    "            y_prime += na_list[i]\n",
    "        y_primes.append(y_prime)\n",
    "    return y_primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
