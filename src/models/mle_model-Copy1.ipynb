{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    aptamers = []\n",
    "    peptides = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    full_dataset = list(set(full_dataset)) #removed duplicates\n",
    "    for pair in full_dataset:\n",
    "        aptamers.append(pair[0])\n",
    "        peptides.append(pair[1])\n",
    "    return full_dataset, aptamers, peptides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(TrainDataset, self).__init__() \n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, test_set):\n",
    "        super(TestDataset, self).__init__() \n",
    "        self.test_set = test_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.test_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.test_set[idx]\n",
    "        return aptamer, peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset, aptamers, peptides = construct_dataset()\n",
    "n = len(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*n)]\n",
    "test_set = full_dataset[int(0.8*n):]\n",
    "train_dataset = TrainDataset(training_set)\n",
    "test_dataset = TestDataset(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(40, 20, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(20, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(8, 4, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(4, 3, 1)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_2, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt).cuda()\n",
    "        pep = self.sequential_pep(pep).cuda()\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(40, 100, (3,1))\n",
    "        self.cnn_pep_1 = nn.Conv2d(8, 40, (2,1))\n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            self.relu, \n",
    "                                            self.pool)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            self.relu, \n",
    "                                            self.pool)\n",
    "        \n",
    "        self.fc1 = nn.Linear(960, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt)\n",
    "        pep = self.sequential_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample x from P_X (assume peptides follow NNK)\n",
    "def get_x():\n",
    "    x_idx = np.random.choice(20, 7, p=pvals)\n",
    "    x = \"M\"\n",
    "    for i in x_idx:\n",
    "        x += aa_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_Y (assume apatamers follow uniform)\n",
    "def get_y():\n",
    "    y_idx = np.random.randint(0, 4, 40)\n",
    "    y = \"\"\n",
    "    for i in y_idx:\n",
    "        y += na_list[i]\n",
    "    return y\n",
    "\n",
    "# Generate uniformly from S without replacement\n",
    "def get_xy(k):\n",
    "    samples = [full_dataset[i] for i in np.random.choice(len(full_dataset), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# S' contains S with double the size of S (domain for Importance Sampling)\n",
    "def get_S_prime(k):\n",
    "    S_prime = full_dataset[:]\n",
    "    for _ in range(k):\n",
    "        S_prime.append((get_y(), get_x()))\n",
    "    return list(set(S_prime))\n",
    "\n",
    "# Sample from S' without replacement\n",
    "def get_xy_prime(k):\n",
    "    samples = [S_prime[i] for i in np.random.choice(len(S_prime), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_x_pmf(x):\n",
    "    pmf = 1\n",
    "    for char in x[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf\n",
    "\n",
    "# Returns pmf of an aptamer\n",
    "def get_y_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "S_prime = get_S_prime(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1], 1))).cuda() #(1, 40, 4, 1)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1], 1))).cuda() #(1, 8, 20, 1)\n",
    "    return apt, pep\n",
    "\n",
    "def update(type=\"original\"):\n",
    "    if type == \"original\":\n",
    "        xy = get_xy(1)[0]\n",
    "    else:\n",
    "        xy = get_xy_prime(1)[0]\n",
    "    x, y = convert(xy[0], xy[1])\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return xy, out\n",
    "\n",
    "def apply_param_grad(grads1, grads2, fn):\n",
    "    gs = []\n",
    "    for grad1, grad2 in zip(grads1, grads2):\n",
    "        gs.append(fn(grad1, grad2))\n",
    "    return gs\n",
    "\n",
    "def sgd(t=1000, #num of iter\n",
    "        lamb=1e-5, #hyperparam\n",
    "        gamma=1e-3): #step size\n",
    "    optim = SGD(model.parameters(), lr=gamma)\n",
    "    model.train()\n",
    "    for a, _ in enumerate(tqdm.tqdm(range(t))):\n",
    "        optim.zero_grad()\n",
    "        xy, out = update()\n",
    "        log_out = torch.log(out)\n",
    "        log_out.backward(retain_graph=True)\n",
    "        g1 = []\n",
    "        for param in model.parameters():\n",
    "            g1.append(param.grad)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        xy_prime, out_prime = update(\"prime\")\n",
    "        out_prime = out_prime * get_x_pmf(xy_prime[0]) * get_y_pmf() * 2 * n\n",
    "        out_prime.backward()\n",
    "        g2 = []\n",
    "        for param in model.parameters():\n",
    "            g2.append(param.grad)\n",
    "        \n",
    "        const = 0 if xy_prime in full_dataset else 1 #indicator\n",
    "        gs = apply_param_grad(g1, g2, lambda g1, g2: lamb*const*g2 - g1)\n",
    "        \n",
    "        #Update the weights according to SGD\n",
    "        for param, g in zip(model.parameters(), gs):\n",
    "            param.grad = g\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall on train set of size k to test for overfitting\n",
    "def recall_train(k):\n",
    "    model.eval()\n",
    "    samples = get_xy(k)\n",
    "    correct = 0\n",
    "    train_recall_outputs = []\n",
    "    for (apt, pep) in samples:\n",
    "        apt, pep = convert(apt, pep)\n",
    "        out = model(apt, pep).cpu().detach().numpy().flatten()[0]\n",
    "        train_recall_outputs.append(out)\n",
    "        if out > 0.5:\n",
    "            correct += 1\n",
    "    train_recall = 100*correct/k #recall rate of k samples from training set\n",
    "    return train_recall, train_recall_outputs #list of k outputs\n",
    "\n",
    "\n",
    "# Recall on test set of size k\n",
    "def recall_test(k):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    test_recall_outputs = []\n",
    "    for _, (aptamer, peptide) in enumerate(tqdm.tqdm(test_loader)):\n",
    "        if count > k:\n",
    "            break\n",
    "        apt, pep = convert(aptamer[0], peptide[0])\n",
    "        output = model(apt, pep).cpu().detach().numpy().flatten()[0]\n",
    "        test_recall_outputs.append(output)\n",
    "        if output > 0.5:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    recall_test = 100*correct/k #recall rate of k samples from test set\n",
    "    return recall_test, test_recall_outputs #list of k outputs\n",
    "\n",
    "\n",
    "# Eval on m new unseen pairs(not in our dataset)\n",
    "def eval_unknown(m):\n",
    "    model.eval()\n",
    "    eval_unknown_outputs = []\n",
    "    count = 0\n",
    "    for _ in range(m):\n",
    "        x, y = get_x(), get_y()\n",
    "        apt, pep = convert(y, x)\n",
    "        output = model(apt, pep).cpu().detach().numpy().flatten()[0]\n",
    "        eval_unknown_outputs.append(output)\n",
    "        if output < 0.5:\n",
    "            count += 1\n",
    "    precision = 100 * count / m\n",
    "    return precision, eval_unknown_outputs #list of m outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "train_cdfs = []\n",
    "test_cdfs = []\n",
    "\n",
    "m = int(1e6) # number of unknown samples\n",
    "k = m//10 # number of binding samples (test set size is 118262, k is just some limit we set)\n",
    "\n",
    "for g in range(len(gammas)):\n",
    "    for l in range(len(lambdas)):\n",
    "        model = SimpleConvNet()\n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        \n",
    "        print(\"=============Training=======================\")\n",
    "        sgd(t=1000, gamma=gammas[g], lamb=lambdas[l])\n",
    "        \n",
    "        print(\"=============Evaluating train===============\")\n",
    "        train_recall, train_recall_outputs = recall_train(k)\n",
    "        print(\"Gamma: \", \"%.5f\" % gammas[g], \"Lambda: \", \"%.5f\" % lambdas[l], \\\n",
    "              \"Train recall: \", \"%.2f\" % train_recall)\n",
    "        \n",
    "        print(\"=============Evaluating test================\")\n",
    "        test_recall, test_recall_outputs = recall_test(k)\n",
    "        print(\"Gamma: \", \"%.5f\" % gammas[g], \"Lambda: \", \"%.5f\" % lambdas[l], \\\n",
    "              \"Test recall: \", \"%.2f\" % test_recall)\n",
    "        \n",
    "        print(\"=============Evaluating unknown=============\")\n",
    "        precision, eval_unknown_outputs = eval_unknown(m)\n",
    "        \n",
    "        train_score = np.asarray(eval_unknown_outputs + train_recall_outputs)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        test_score = np.asarray(eval_unknown_outputs + test_recall_outputs)\n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "        train_cdf = np.sum(np.cumsum(train_score), dtype=float)/(np.sum(train_score)*len(train_score))\n",
    "        test_cdf = np.sum(np.cumsum(test_score), dtype=float)/(np.sum(test_score)*len(test_score))\n",
    "        print(\"G: \", \"%.5f\" % gammas[g], \"L: \", \"%.5f\" % lambdas[l], \\\n",
    "              \"Train CDF: \", \"%.3f\" % train_cdf, \"Test CDF: \", \"%.3f\" % test_cdf)\n",
    "        \n",
    "        train_recalls.append(train_recall)\n",
    "        test_recalls.append(test_recall)\n",
    "        train_cdfs.append((gammas[g], lambdas[l], train_cdf))\n",
    "        test_cdfs.append((gammas[g], lambdas[l], test_cdf))\n",
    "\n",
    "print(\"Train CDFs: \", train_cdfs)\n",
    "print(\"Test CDFs: \", test_cdfs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUC Plot(one config for now)\n",
    "def cdf(scores, i):\n",
    "    #tot = len(gammas) * len(lambdas)\n",
    "    #for i in range(tot):\n",
    "    #unknown, binding = scores[i]\n",
    "    #total = unknown + binding\n",
    "    plt.hist(scores, 100, histtype='step', density=True, cumulative=True)\n",
    "    g = gammas[i//len(lambdas)]\n",
    "    l = lambdas[i%len(lambdas)]\n",
    "    label = 'lambda =%.5f' % l  + ' gamma =%.5f' % g\n",
    "    plt.legend([label])\n",
    "    plt.show()\n",
    "\n",
    "i = 0\n",
    "cdf(test_scores[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write to file when output limit exceeded\n",
    "# with open(\"./scores.txt\", \"a\") as f:\n",
    "#     f.write(str(scores[-1]))\n",
    "#     f.close()\n",
    "\n",
    "### Table of recalls with different params\n",
    "# idx = sorted(range(len(recalls)), key=lambda k: recalls[k])\n",
    "# for i in idx:\n",
    "#     g = gammas[i//len(lambdas)]\n",
    "#     l = lambdas[i%len(lambdas)]\n",
    "#     print(\"Gamma: \", \"%.5f\" % g, \"Lambda: \", \"%.5f\" % l, \\\n",
    "#           \"Recall: \", \"%.2f\" % recalls[i], \"Precision: \", \"%.2f\" % precisions[i])\n",
    "\n",
    "### Heatmap of recalls\n",
    "# mat = sns.heatmap(M, vmin=0, vmax=100)\n",
    "# plt.show()\n",
    "\n",
    "### Test other NN models\n",
    "# model = SimpleConvNet()\n",
    "# model.apply(weights_init)\n",
    "# model.cuda()\n",
    "# print(\"Training...\")\n",
    "# sgd(t=1000, gamma=0.01, lamb=0.01)\n",
    "# print(\"Evaluating...\")\n",
    "# # use for AUC\n",
    "# recall, binding_outputs = recall_eval(50)\n",
    "# unknown_outputs = evaluate(500)\n",
    "# # use for heatmap\n",
    "# print(\"Recall with gamma: \"+ str(0.01) + \" , lambda: \" + str(0.01) + \" recall: \", '%.2f'% recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walnutree",
   "language": "python",
   "name": "walnutree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
