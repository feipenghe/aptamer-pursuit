{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import pairwise2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(4000) #use for eval\n",
    "#train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def translate(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        \n",
    "        encoding = np.zeros(len(apt) + len(pep))\n",
    "        \n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            idx = na_list.index(char)\n",
    "            encoding[i] = idx\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            idx = aa_list.index(char)\n",
    "            encoding[i+len(apt)] = idx\n",
    "        return encoding     \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        \n",
    "        encoding = np.zeros(len(sequence))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            idx = letters.index(char)\n",
    "            encoding[i] = idx\n",
    "        return encoding\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(48, )\n",
    "        print(str(pair.shape))\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (1, pair.shape[0]))).to(device)\n",
    "        label = torch.FloatTensor([label]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        apt = translate(apt, seq_type='aptamer') #(40, )\n",
    "        pep = translate(pep, seq_type='peptide') #(8, )\n",
    "        print(\"Apt shape: \", apt.shape)\n",
    "        print(\"Pep shape: \", pep.shape)\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, 1, apt.shape[0]))).to(device) #(1, 1, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, 1, pep.shape[0]))).to(device) #(1, 1, 8)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=True\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateBatchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranslateBatchNet, self).__init__()\n",
    "        self.name = \"TranslateBatchNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(1, 20, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(20, 30, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(30, 20, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(20, 5, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(1, 15, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(15, 30, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(30, 10, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(10, 5, 2, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_4, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(25, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "torch.Size([1, 48])\n"
     ]
    }
   ],
   "source": [
    "apt, pep = S_new[0]\n",
    "baseline_pair, baseline_label = convert(apt, pep, 0, single_alphabet=True)\n",
    "print(str(baseline_pair.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48,)\n",
      "torch.Size([1, 48])\n"
     ]
    }
   ],
   "source": [
    "apt, pep = S_test[0]\n",
    "input_pair, input_label = convert(apt, pep, 1, single_alphabet=True)\n",
    "print(str(input_pair.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateSingleAlphabetBatchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranslateSingleAlphabetBatchNet, self).__init__()\n",
    "        self.name = \"TranslateSingleAlphabetBatchNet\"\n",
    "        \n",
    "        self.cnn_1 = nn.Conv1d(1, 20, 3) \n",
    "        self.cnn_2 = nn.Conv1d(20, 30, 3, padding=2) \n",
    "        self.cnn_3 = nn.Conv1d(30, 20, 3, padding=2) \n",
    "        self.cnn_4 = nn.Conv1d(20, 5, 1) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnns = nn.Sequential(self.cnn_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_2, \n",
    "                                     self.cnn_3, \n",
    "                                     self.cnn_4)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(135, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, pair):\n",
    "        x = self.cnns(pair)\n",
    "        print(str(x.shape))\n",
    "        x = x.view(1, -1)\n",
    "        print(str(x.shape))\n",
    "        #x = x.view(-1, 1).T\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        print(\"Output shape: \", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyModel is the one directly from Captum website\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(48, 24)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(24, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "#         self.lin1.weight = nn.Parameter(torch.arange(-4.0, 44.0).view(24, 48))\n",
    "#         self.lin1.bias = nn.Parameter(torch.zeros(1,48))\n",
    "#         self.lin2.weight = nn.Parameter(torch.arange(-3.0, 93.0).view(2, 48))\n",
    "#         self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.relu(self.lin1(input))\n",
    "        print(str(x.shape))\n",
    "        x = self.lin2(x)\n",
    "        print(str(x.shape))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyModel(\n",
       "  (lin1): Linear(in_features=48, out_features=24, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### checkpoint = None #torch.load('../model_checkpoints/binary//06172020.pth')\n",
    "model = ToyModel()\n",
    "#optim = SGD(model.parameters(), lr=1e-2)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 2])\n",
      "tensor([[-0.8161,  0.6035]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(str(input_pair.shape))\n",
    "x = model(input_pair)\n",
    "print(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 24])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 2])\n",
      "IG Attributions: tensor([[ 0.0269, -0.0290,  0.0370, -0.0000,  0.0240,  0.0000, -0.0000,  0.0453,\n",
      "          0.0448,  0.0117,  0.0417,  0.0000, -0.0598,  0.0188,  0.0383,  0.0245,\n",
      "          0.0000,  0.0172, -0.0638, -0.0000, -0.0133, -0.0093, -0.0136, -0.0427,\n",
      "         -0.0000,  0.0532, -0.0212,  0.0000,  0.0000, -0.0000, -0.2063, -0.0064,\n",
      "         -0.0480,  0.0069,  0.0000,  0.0000, -0.0157, -0.0385, -0.0000, -0.0506,\n",
      "          0.0000, -0.0946,  0.0966, -0.0511, -0.0000, -0.2303,  0.2787,  0.1100]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Convergence Delta: tensor([7.4506e-08], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dl = DeepLift(model)\n",
    "attributions, delta = dl.attribute(input_pair, baseline_pair, target=1, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from Captum's site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 3)\n",
    "baseline = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToyModel is the one directly from Captum website\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(3, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
    "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
    "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
    "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.lin2(self.relu(self.lin1(input)))\n",
    "        print(str(x.shape))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()\n",
    "model.eval()\n",
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLift(model)\n",
    "attributions, delta = dl.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('DeepLift Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = NeuronConductance(model, model.lin1)\n",
    "attributions = nc.attribute(input, neuron_index=2, target=0)\n",
    "print('Neuron Attributions:', attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptamers",
   "language": "python",
   "name": "aptamers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
