{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import pairwise2\n",
    "from captum.attr import (\n",
    "    GradientShap,\n",
    "    DeepLift,\n",
    "    DeepLiftShap,\n",
    "    IntegratedGradients,\n",
    "    LayerConductance,\n",
    "    NeuronConductance,\n",
    "    NoiseTunnel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nupyck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export NUPACKHOME=\"/ssd1/home/aishrm2/aptamer-pursuit/src/models/evaluation/nupyck/lib/nupack\"\n",
    "import nupyck\n",
    "import importlib\n",
    "importlib.reload(nupyck)\n",
    "!pip install biopython\n",
    "from Bio import SeqIO\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nupyck.mfe([\"GAUCCGC\", \"GCGAAUC\"], [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize free energies\n",
    "binding_energies = []\n",
    "free_energies = []\n",
    "f = open(\"binding_energies.txt\", \"r\")\n",
    "for x in f:\n",
    "    binding_energies.append(float(x))\n",
    "\n",
    "f = open('free_energies.txt', 'r')\n",
    "for x in f:\n",
    "    free_energies.append(float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(binding_energies, ax=ax, label='Binding Aptamers')\n",
    "sns.distplot(free_energies, ax=ax, label='Non binding aptamers')\n",
    "plt.legend()\n",
    "plt.title(\"Free Energy Distributions\")\n",
    "plt.xlabel(\"Free Energy Score (MFE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aptamers binding\n",
    "aptamers_binding = SeqIO.parse(\"aptamers_binding.fasta\", \"fasta\")\n",
    "apts = []\n",
    "for record in aptamers_binding:\n",
    "    apts.append(str(record.seq))\n",
    "binding_energies = []\n",
    "for i, apt in enumerate(tqdm.tqdm(apts)):\n",
    "    try:\n",
    "        energy = nupyck.mfe([apt], [1])[0]['energy']\n",
    "        binding_energies.append(energy)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(str(len(binding_energies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('binding_energies.txt', 'w') as f:\n",
    "    for e in binding_energies:\n",
    "        f.write('%s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamers_free = SeqIO.parse(\"aptamers_free.fasta\", \"fasta\")\n",
    "apts = []\n",
    "for record in aptamers_free:\n",
    "    apts.append(str(record.seq))\n",
    "free_energies = []\n",
    "for i, apt in enumerate(tqdm.tqdm(apts)):\n",
    "    try:\n",
    "        energy = nupyck.mfe([apt], [1])[0]['energy']\n",
    "        free_energies.append(energy)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(str(len(free_energies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('free_energies.txt', 'w') as f:\n",
    "    for e in free_energies:\n",
    "        f.write('%s\\n' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "#S_new = get_S_new(10*n) #use for eval\n",
    "#train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModelSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1dModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"LinearConv1dModel\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 1)\n",
    "        self.conv_type = '1d'\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dModelSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModelSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"Conv1dModelSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "        self.conv_type = '1d'\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrueLinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is too complex for our input sequence size\n",
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 300, 1)\n",
    "        self.cnn_apt_3 = nn.Conv1d(300, 150, 1)\n",
    "        self.cnn_apt_4 = nn.Conv1d(150, 75, 1)\n",
    "        self.cnn_apt_5 = nn.Conv1d(25, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 250, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(250, 500, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 250, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(250, 100, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(100, 10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.maxpool, self.relu, self.cnn_apt_3, self.maxpool, self.relu, self.cnn_apt_4, self.maxpool, self.relu, self.cnn_apt_5, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.maxpool, self.relu, self.cnn_pep_3, self.maxpool, self.relu, self.cnn_pep_4, self.maxpool, self.relu, self.cnn_pep_5, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(180, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrueLinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"TrueLinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        self.conv_type = '1d'\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimizedVCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinimizedVCNet, self).__init__()\n",
    "        self.name = \"MinimizedVCNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 500, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(500, 100, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 250, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(250, 100, 3, padding=2)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(800, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        # conv --> relu --> pool after every one\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1]))).cuda() #(1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1]))).cuda() #(1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.cuda()\n",
    "        y = pep.cuda()\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, new_recall, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.plot(new_recall, 'r', label='New recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, i, j, lamb, gamma):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx > 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, i, j, lamb, gamma):\n",
    "    #train_score consisits of [10000 scores generated] + [1000 scores from training set]\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx > 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=ax)\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=ax)\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=ax)\n",
    "    ax.set_title(\"Distribution of Scores\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointed_model = '../../models/model_checkpoints/MinimizedVCNet/05082020.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = MinimizedVCNet()\n",
    "optim = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236524\n",
      "118262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TCAATCTAGGACGACTTCCATTATATTCAAAGTACGAATT', 'MLVAMMTV')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(len(S_prime_test)))\n",
    "print(str(len(S_test)))\n",
    "S_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation set is S_prime_test and S_test\n",
    "validation_set = []\n",
    "for (apt, pep), label in S_prime_test[:118262]:\n",
    "    validation_set.append((apt, pep, label))\n",
    "\n",
    "for (apt, pep) in S_test[:4000]:\n",
    "    validation_set.append((apt, pep, 0))\n",
    "\n",
    "np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "hydrophobicity_binding = []\n",
    "hydrophobicity_free = []\n",
    "\n",
    "arginine_binding = []\n",
    "arginine_free = []\n",
    "\n",
    "gc_binding = []\n",
    "gc_free = []\n",
    "\n",
    "aptamers_binding = []\n",
    "aptamers_free = []\n",
    "\n",
    "peptides_binding = []\n",
    "peptides_free = []\n",
    "\n",
    "# Only look at similarity for these lists\n",
    "dataset_aptamers_binding = []\n",
    "dataset_peptides_binding = []\n",
    "dataset_aptamers_free = []\n",
    "dataset_peptides_free = []\n",
    "\n",
    "generated_aptamers_binding = []\n",
    "generated_peptides_binding = []\n",
    "generated_aptamers_free = []\n",
    "generated_peptides_free = []\n",
    "\n",
    "for (apt, pep, label) in validation_set:\n",
    "    x, y = convert(apt, pep)\n",
    "    score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "    hp = 0\n",
    "    for aa in pep:\n",
    "        hp += hydrophobicity[aa]\n",
    "    \n",
    "    # The sample is from our dataset\n",
    "    if label == 0 and score > 0.6:\n",
    "        dataset_aptamers_binding.append(apt)\n",
    "        dataset_peptides_binding.append(pep)\n",
    "        \n",
    "    # Score is non binding\n",
    "    elif label == 0 and score < 0.3:\n",
    "        dataset_aptamers_free.append(apt)\n",
    "        dataset_peptides_free.append(pep)\n",
    "            \n",
    "    elif label == 1 and score > 0.6:\n",
    "        generated_aptamers_binding.append(apt)\n",
    "        generated_peptides_binding.append(pep)\n",
    "        \n",
    "    # Score is non binding\n",
    "    elif label == 1 and score < 0.3:\n",
    "        generated_aptamers_free.append(apt)\n",
    "        generated_peptides_free.append(pep)\n",
    "        \n",
    "    \n",
    "    gc_count = apt.count('C') + apt.count('G')\n",
    "    \n",
    "    if score < 0.3:\n",
    "        hydrophobicity_free.append(hp)\n",
    "        arginine_free.append(pep.count('R'))\n",
    "        gc_free.append(gc_count)\n",
    "        aptamers_free.append(apt)\n",
    "        peptides_free.append(pep)\n",
    "        \n",
    "    elif score > 0.6:\n",
    "        hydrophobicity_binding.append(hp)\n",
    "        arginine_binding.append(pep.count('R'))\n",
    "        gc_binding.append(gc_count)\n",
    "        aptamers_binding.append(apt)\n",
    "        peptides_binding.append(pep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the aptamers binding and aptamers free to a fasta file to be used with mfold\n",
    "with open(\"aptamers_binding.fasta\", 'w') as f:\n",
    "    count = 1\n",
    "    for apt in aptamers_binding:\n",
    "        f.write('> Sequence: ' + str(count) + '\\n')\n",
    "        f.write(apt +'\\n')\n",
    "        count += 1\n",
    "with open(\"aptamers_free.fasta\", 'w') as f:\n",
    "    count = 1\n",
    "    for apt in aptamers_free:\n",
    "        f.write('> Sequence: ' + str(count) + '\\n')\n",
    "        f.write(apt +'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrophobicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hydrophobicity_binding, bins=10, alpha=0.5, label='Hydrophobicity of Binding Peptides')\n",
    "plt.hist(hydrophobicity_free, bins=10 , alpha=0.5, label='Hydrophobicity of Non-Binding Peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Hydrophobicity Score\")\n",
    "plt.title('Hydrophobicity of Outputs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arginine content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arginine content\n",
    "plt.hist(arginine_binding, bins=8, alpha=0.5, label='Number of arginines in binding peptides')\n",
    "plt.hist(arginine_free, bins=8 , alpha=0.5, label='Number of arginines in non-binding peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Number of Arginines\")\n",
    "plt.title('Arginine Count in Peptides')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gc_binding, bins=8, alpha=0.5, label='GC Count in Binding Aptamers')\n",
    "plt.hist(gc_free, bins=8 , alpha=0.5, label='GC Count in Non-Binding aptamers')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"GC Count\")\n",
    "plt.title('GC Count in Aptamers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: binding aptamers that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_binding_similarity = []\n",
    "generated_binding_similarity = []\n",
    "\n",
    "# For aptamers in our dataset\n",
    "for i, apt in enumerate(tqdm.tqdm(dataset_aptamers_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -3, -1, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_binding_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, apt in enumerate(tqdm.tqdm(generated_aptamers_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -3, -1, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_binding_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_binding_similarity = np.asarray(dataset_binding_similarity)\n",
    "generated_binding_similarity = np.asarray(generated_binding_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_binding_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_binding_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Aptamers in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for aptamers that we predict will bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: binding peptides that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_binding_similarity = []\n",
    "generated_binding_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, pep in enumerate(tqdm.tqdm(dataset_peptides_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_binding_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, pep in enumerate(tqdm.tqdm(generated_peptides_binding)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_binding_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_binding_similarity = np.asarray(dataset_binding_similarity)\n",
    "generated_binding_similarity = np.asarray(generated_binding_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_binding_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_binding_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Peptides in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for peptides that we predict will bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: non binding peptides that are from our dataset and generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_free_similarity = []\n",
    "generated_free_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, pep in enumerate(tqdm.tqdm(dataset_peptides_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_free_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, pep in enumerate(tqdm.tqdm(generated_peptides_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(pep, train_pep, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_free_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_free_similarity = np.asarray(dataset_free_similarity)\n",
    "generated_free_similarity = np.asarray(generated_free_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_free_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_free_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Peptides in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for peptides that we predict will not bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity to train set: non binding aptamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_free_similarity = []\n",
    "generated_free_similarity = []\n",
    "\n",
    "# For peptides in our dataset\n",
    "for i, apt in enumerate(tqdm.tqdm(dataset_aptamers_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    dataset_free_similarity.append(max_score_train)\n",
    "\n",
    "# Generated    \n",
    "for i, apt in enumerate(tqdm.tqdm(generated_aptamers_free)):\n",
    "    max_score_train = 0\n",
    "    for (train_apt, train_pep) in S_train[:10000]:\n",
    "        # 2 points for matching, -1 points for mismatch, -2 for opening gap, -0.5 for continuing a gap\n",
    "        sequence_similarity = pairwise2.align.globalms(apt, train_apt, 2, -1, -2, -0.5, score_only=True)\n",
    "        max_score_train = max(max_score_train, sequence_similarity)\n",
    "    generated_free_similarity.append(max_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the similarities\n",
    "dataset_free_similarity = np.asarray(dataset_free_similarity)\n",
    "generated_free_similarity = np.asarray(generated_free_similarity)\n",
    "\n",
    "hist_dataset, dataset_edges = np.histogram(dataset_free_similarity, bins=10)\n",
    "hist_generated, generated_edges = np.histogram(generated_free_similarity, bins=10)\n",
    "\n",
    "hist_dataset = hist_dataset/np.max(hist_dataset)\n",
    "hist_generated = hist_generated/np.max(hist_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "ax.bar(dataset_edges[:-1], hist_dataset, alpha=0.5, label=\"Aptamers in the dataset\")\n",
    "ax.bar(generated_edges[:-1], hist_generated, alpha=0.5, label=\"Generated aptamers\")\n",
    "plt.xlabel(\"Alignment Score\")\n",
    "plt.title('Alignment scores to train set for aptamers that we predict will not bind')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinimizedVCNet(\n",
       "  (cnn_apt_1): Conv1d(4, 1000, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_2): Conv1d(1000, 500, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_3): Conv1d(500, 100, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_1): Conv1d(20, 500, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_2): Conv1d(500, 250, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_3): Conv1d(250, 100, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=800, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointed_model = '../../models/model_checkpoints/MinimizedVCNet/05082020.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = MinimizedVCNet()\n",
    "optim = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 8 and 40 in dimension 1 at /opt/conda/conda-bld/pytorch_1579061855666/work/aten/src/THC/generic/THCTensorMath.cu:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-40619b648395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_pep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbaseline_apt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_pep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_apt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 8 and 40 in dimension 1 at /opt/conda/conda-bld/pytorch_1579061855666/work/aten/src/THC/generic/THCTensorMath.cu:71"
     ]
    }
   ],
   "source": [
    "apt = 'A'* 40\n",
    "pep = 'R' * 8\n",
    "\n",
    "baseline_apt, baseline_pep = convert(apt, pep)\n",
    "input_apt = torch.rand(1, 40, 4)\n",
    "input_pep = torch.randn(1, 8, 20)\n",
    "\n",
    "baseline = torch.cat([baseline_apt, baseline_pep])\n",
    "input = torch.cat([input_apt, input_pep])\n",
    "\n",
    "baseline.shape\n",
    "\n",
    "\n",
    "# ig = IntegratedGradients(model)\n",
    "# attributions, delta = ig.attribute(input_apt, input_pep, baseline_apt, baseline_pep, target=0, return_convergence_delta=True)\n",
    "# # # print('IG Attributions:', attributions)\n",
    "# # print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToyModel(\n",
       "  (lin1): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=3, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to set it up their way with a model that only takes one input\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(3, 2)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))\n",
    "        self.lin1.bias = nn.Parameter(torch.zeros(1,3))\n",
    "        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))\n",
    "        self.lin2.bias = nn.Parameter(torch.ones(1,2))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.lin2(self.relu(self.lin1(input)))\n",
    "model = ToyModel()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(2, 3)\n",
    "baseline = torch.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Attributions: tensor([[-1.0208, -0.1956, -1.4237],\n",
      "        [-1.4338, -2.6831, -2.3976]])\n",
      "Convergence Delta: tensor([2.3842e-07, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('IG Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd1/home/aishrm2/anaconda3/envs/aptamers/lib/python3.8/site-packages/captum/attr/_utils/gradient.py:31: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/ssd1/home/aishrm2/anaconda3/envs/aptamers/lib/python3.8/site-packages/captum/attr/_core/deep_lift.py:298: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepLift Attributions: tensor([[-1.0208, -0.1956, -1.4237],\n",
      "        [-1.4338, -2.6831, -2.3976]], grad_fn=<MulBackward0>)\n",
      "Convergence Delta: tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "dl = DeepLift(model)\n",
    "attributions, delta = dl.attribute(input, baseline, target=0, return_convergence_delta=True)\n",
    "print('DeepLift Attributions:', attributions)\n",
    "print('Convergence Delta:', delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron Attributions: tensor([[-0.9902, -0.1898, -1.3810],\n",
      "        [-1.3907, -2.6026, -2.3256]])\n"
     ]
    }
   ],
   "source": [
    "nc = NeuronConductance(model, model.lin1)\n",
    "attributions = nc.attribute(input, neuron_index=2, target=0)\n",
    "print('Neuron Attributions:', attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
