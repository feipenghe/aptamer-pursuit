{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "k = 5000 # number of samples used to calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 120, (4,4)) #similar to 4-gram\n",
    "        self.cnn_apt_2 = nn.Conv2d(120, 50, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(50, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 50, (4,20))\n",
    "        self.cnn_pep_2 = nn.Conv2d(50, 10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(1900, 1)\n",
    "        self.name = \"SimpleConvNet\"\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoreComplexNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 500, 1) \n",
    "        self.cnn_apt_2 = nn.Conv2d(500, 1000, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(1000, 500, 2)\n",
    "        self.cnn_apt_4 = nn.Conv2d(500, 100, 2)\n",
    "        self.cnn_apt_5 = nn.Conv2d(100, 10, 2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 250, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(250, 500, 1)\n",
    "        self.cnn_pep_3 = nn.Conv2d(500, 250, 3)\n",
    "        self.cnn_pep_4 = nn.Conv2d(250, 100, 2)\n",
    "        self.cnn_pep_5 = nn.Conv2d(100, 10, 2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu, self.cnn_apt_3, self.relu, self.cnn_apt_4, self.relu, self.cnn_apt_5)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2, self.relu, self.cnn_pep_3, self.relu, self.cnn_pep_4, self.relu, self.cnn_pep_5)\n",
    "        \n",
    "        self.name = \"MoreComplexNet\"\n",
    "        self.fc1 = nn.Linear(1010, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NLPModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 1000, 1)\n",
    "        self.cnn_apt_3 = nn.Conv1d(1000, 500, 1)\n",
    "        self.cnn_apt_4 = nn.Conv1d(500, 100, 1)\n",
    "        self.cnn_apt_5 = nn.Conv1d(100, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(1, 250, 1)\n",
    "        self.cnn_pep_2 = nn.Conv1d(250, 500, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 250, 3)\n",
    "        self.cnn_pep_4 = nn.Conv1d(250, 100, 2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(100, 10, 2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"NLPModel\"\n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu, self.cnn_apt_3, self.relu, self.cnn_apt_4, self.relu, self.cnn_apt_5)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2, self.relu, self.cnn_pep_3, self.relu, self.cnn_pep_4, self.relu, self.cnn_pep_5)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(1010, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, conv_type='2d'): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    if conv_type == '2d':\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1]))).cuda() #(1, 1, 40, 4)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1]))).cuda() #(1, 1, 8, 20)\n",
    "    else:\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (apt.shape[0], 1, apt.shape[1]))).cuda() #(1, 1, 40, 4)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (pep.shape[0], 1, pep.shape[1]))).cuda() #(1, 1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train', conv_type='2d'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep, conv_type=conv_type)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train', conv_type='2d'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep, conv_type=conv_type)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.cuda()\n",
    "        y = pep.cuda()\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, new_recall, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.plot(new_recall, 'r', label='New recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cdf(train_cdf, test_cdf, i, j, lamb, gamma):\n",
    "    plt.plot(train_cdf, 'b', label='Train CDF')\n",
    "    plt.plot(test_cdf, 'y', label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=axes[0, 0])\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=axes[1, 0])\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=axes[0, 1])\n",
    "    axes[0,0].set_title(\"New: not in dataset\")\n",
    "    axes[1,0].set_title(\"Train: in dataset\")\n",
    "    axes[0, 1].set_title(\"Test: in dataset\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model_name, lamb=1e28, #hyperparam\n",
    "        gamma=1e-3, #step size\n",
    "        save_checkpoints=False): #save checkpoints\n",
    "    \n",
    "    #checkpointed_model = '../models/model_checkpoints/mle_model.pth'\n",
    "    #checkpoint = torch.load(checkpointed_model)\n",
    "    optim = SGD(model.parameters(), lr=gamma)\n",
    "    #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.to(device)\n",
    "    epoch = 0\n",
    "    \n",
    "    if model_name == 'NLPModel':\n",
    "        conv_type = '1d'\n",
    "    while epoch < EPOCHS:\n",
    "        train_losses = []\n",
    "        train_recalls = []\n",
    "        train_recall_outputs = [] \n",
    "\n",
    "        test_losses = []\n",
    "        test_recalls = []\n",
    "        test_recall_outputs = []\n",
    "\n",
    "        new_outputs = []\n",
    "        new_recalls = []\n",
    "\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "        new_correct = 0\n",
    "        \n",
    "        for i, (aptamer, peptide, (apt_prime, pep_prime), indicator) in enumerate(tqdm.tqdm(train_ds)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            model.train()\n",
    "            optim.zero_grad() \n",
    "            x, y = convert(aptamer, peptide, conv_type=conv_type) #sample x,y from S_train\n",
    "            out = update(x, y) #get S_train output/score\n",
    "            log_out = torch.log(out) \n",
    "            \n",
    "            train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "            if train_score > 0.6:\n",
    "                train_correct += 1 \n",
    "            train_recall_outputs.append(train_score) \n",
    "\n",
    "            optim.zero_grad() \n",
    "            y_pmf = get_y_pmf(pep_prime)\n",
    "            x_prime, y_prime = convert(apt_prime, pep_prime, conv_type=conv_type) #sample x', y' from S_prime_train\n",
    "            out_prime = update(x_prime, y_prime) #get score from S_prime_train\n",
    "            if indicator == 0:\n",
    "                factor = (2*m*get_x_pmf()*y_pmf)/(1+m*get_x_pmf()*y_pmf)\n",
    "            else:\n",
    "                factor = 2\n",
    "            out_prime = out_prime*factor #adjust for IS\n",
    "            #print(\"Obj first part: \", out_prime.cpu().detach().numpy().flatten()[0]*lamb*indicator)\n",
    "            #print(\"Obj second part: \", log_out.cpu().detach().numpy().flatten()[0])\n",
    "            # Retain graph retains the graph for further operations\n",
    "            (lamb*indicator*out_prime - log_out).backward(retain_graph=True) \n",
    "            optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "            x_test, y_test = convert(S_test[i%(n-m)][0], S_test[i%(n-m)][1], conv_type=conv_type) #sample x,y from test set\n",
    "            test_score = model(x_test, y_test).cpu().detach().numpy().flatten()[0]\n",
    "            test_recall_outputs.append(test_score) \n",
    "            if test_score > 0.6:\n",
    "                test_correct += 1 \n",
    "\n",
    "            #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "            for x, y in S_new[10*i:10*(i+1)]:\n",
    "                x_new, y_new = convert(x, y, conv_type=conv_type) #generate unseen x'' and y'' from S_new\n",
    "                new_score = model(x_new, y_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                new_outputs.append(new_score)\n",
    "                if new_score < 0.3:\n",
    "                    new_correct += 1\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                train_loss = lamb*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                #print(\"Train loss first part: \", lamb*get_out_prime(\"train\"))\n",
    "                #print(\"Train loss second part: \", get_log_out('train'))\n",
    "                test_loss = (m/(n-m))*lamb*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                #print(\"Test loss first part: \", lamb*get_out_prime(\"test\"))\n",
    "                #print(\"Test loss second part: \", get_log_out('test'))\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                train_recall = 100*train_correct/i #training recall\n",
    "                train_recalls.append(train_recall) \n",
    "                test_recall = 100*test_correct/i #test recall\n",
    "                test_recalls.append(test_recall)\n",
    "                new_recall = 100*new_correct/(i*10) #generated dataset recall\n",
    "                new_recalls.append(new_recall)\n",
    "                if i > 1000:\n",
    "                    train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                    test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                else:\n",
    "                    train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                    test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "                train_cdf = np.cumsum(train_score)/np.sum(train_score) #train cdf\n",
    "                test_cdf = np.cumsum(test_score)/np.sum(test_score) #test cdf\n",
    "\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                plot_recall(train_recalls, test_recalls, new_recalls, i, epoch, lamb, gamma)\n",
    "                plot_loss(train_losses, test_losses, i, epoch, lamb, gamma)\n",
    "                plot_cdf(train_cdf, test_cdf, i, epoch, lamb, gamma)\n",
    "                histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:])\n",
    "                print(\"New score: \", np.average(new_outputs[-50:]))\n",
    "                print(\"Train score: \", np.average(train_score[-50:]))\n",
    "                print(\"Test score: \", np.average(test_score[-50:]))\n",
    "        # Save after every epoch\n",
    "        if save_checkpoints:\n",
    "            checkpoint_name = '../models/model_checkpoints/' + str(model_name) + '_lambda=' + str(lamb) + '_gamma=' + str(gamma) + '.pth'\n",
    "            torch.save({'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optim.state_dict()}, checkpoint_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "gammas = [1e-3, 1e-2]\n",
    "lambdas = [5, 10, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 5 Gamma: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/473047 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6266d7eb76bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_checkpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-9734abe5ba53>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(model_name, lamb, gamma, save_checkpoints)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maptamer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeptide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sample x,y from S_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get S_train output/score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mlog_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6c301ae8af06>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e9d0291d428d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, apt, pep)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_apt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mpep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_pep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1). Kernel size: (2). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "for g in gammas:\n",
    "    for l in lambdas:\n",
    "        print(\"Lambda: \" + str(l) + \" Gamma: \" + str(g))\n",
    "        model = NLPModel()\n",
    "        \n",
    "        # Generate Loss Samples\n",
    "        if model.name == 'NLPModel':\n",
    "            \n",
    "            train_loss_samples = loss_samples(k, 'train', conv_type='1d')\n",
    "            test_loss_samples = loss_samples(k, 'test', conv_type='1d')\n",
    "            prime_train_loss_samples = prime_loss_samples(k, 'train', conv_type='1d')\n",
    "            prime_test_loss_samples = prime_loss_samples(k, 'test', conv_type='1d')\n",
    "        else:\n",
    "            train_loss_samples = loss_samples(k, 'train', conv_type='2d')\n",
    "            test_loss_samples = loss_samples(k, 'test', conv_type='2d')\n",
    "            prime_train_loss_samples = prime_loss_samples(k, 'train', conv_type='2d')\n",
    "            prime_test_loss_samples = prime_loss_samples(k, 'test', conv_type='2d')\n",
    "        \n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        sgd(model_name=model.name, gamma=g, lamb=l, save_checkpoints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointed_model = '../models/model_checkpoints/mle_model.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = MoreComplexNet()\n",
    "optim = SGD(model.parameters(), lr=1e-3)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(S_prime_test)))\n",
    "print(str(len(S_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is S_prime_test and S_test\n",
    "validation_set = []\n",
    "for (apt, pep), label in S_prime_test[:118262]:\n",
    "    validation_set.append((apt, pep, label))\n",
    "\n",
    "for (apt, pep) in S_test[:4000]:\n",
    "    validation_set.append((apt, pep, 0))\n",
    "\n",
    "np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "hydrophobicity_binding = []\n",
    "hydrophobicity_free = []\n",
    "arginine_binding = []\n",
    "arginine_free = []\n",
    "\n",
    "for (apt, pep, label) in validation_set:\n",
    "    x, y = convert(apt, pep)\n",
    "    score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "    hp = 0\n",
    "    for aa in pep:\n",
    "        hp += hydrophobicity[aa]\n",
    "    \n",
    "    if score < 0.3:\n",
    "        hydrophobicity_free.append(hp)\n",
    "        arginine_free.append(pep.count('R'))\n",
    "    elif score > 0.6:\n",
    "        hydrophobicity_binding.append(hp)\n",
    "        arginine_binding.append(pep.count('R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hydrophobicity_binding, bins=10, label='Hydrophobicity of Binding Peptides')\n",
    "plt.hist(hydrophobicity_free, bins=10 , label='Hydrophobicity of Non-Binding Peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Hydrophobicity Score\")\n",
    "plt.title('Hydrophobicity of Test Set Outputs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arginine content\n",
    "plt.hist(arginine_binding, bins=8, label='Number of arginines in binding peptides')\n",
    "plt.hist(arginine_free, bins=8 , label='Number of arginines in non-binding peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Number of Arginines\")\n",
    "plt.title('Arginine Count in Peptides')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
