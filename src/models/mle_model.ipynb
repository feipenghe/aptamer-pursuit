{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "k = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    full_dataset = list(set(full_dataset)) #removed duplicates\n",
    "    return full_dataset\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(TrainDataset, self).__init__() \n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide\n",
    "    \n",
    "\n",
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "full_dataset = construct_dataset()\n",
    "n = len(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*n)]\n",
    "test_set = full_dataset[int(0.8*n):]\n",
    "train_dataset = TrainDataset(training_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 120, (4,4)) #similar to 4-gram\n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 50, (4,20))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4690, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt_1(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep_1(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of S_new: ', 5913090)\n"
     ]
    }
   ],
   "source": [
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S' contains S with double the size of S (domain for Importance Sampling)\n",
    "# Return S_prime, and S_new (all unseen samples)\n",
    "def get_S_prime(k):\n",
    "    S_prime_dict = dict.fromkeys(training_set, 0) #indicator 0 means in the original dataset\n",
    "    S_new = []\n",
    "    for _ in range(k):\n",
    "    \n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in the original dataset\n",
    "        S_new.append(pair)\n",
    "    while len(S_new) < 10 * k:\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()]\n",
    "    random.shuffle(S_prime)\n",
    "    return S_prime, S_new\n",
    "\n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf\n",
    "\n",
    "\n",
    "S_prime, S_new = get_S_prime(n) #use for sgd and eval\n",
    "print(\"Length of S_new: \", len(S_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1]))).cuda() #(1, 1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1]))).cuda() #(1, 1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "def update(x, y):\n",
    "    pmf = get_y_pmf(y)\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return pmf, out\n",
    "\n",
    "def generate_loss_samples(k, dataset='train'):\n",
    "    if dataset == 'train':\n",
    "        dset = training_set\n",
    "    elif dataset == 'test':\n",
    "        dset = test_set\n",
    "    else:\n",
    "        dset = S_new\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "train_loss_samples = generate_loss_samples(k, 'train')\n",
    "test_loss_samples = generate_loss_samples(k, 'test')\n",
    "eval_loss_samples = generate_loss_samples(k, 's_prime')\n",
    "    \n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        _, out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "def get_out_prime():\n",
    "    outs = []\n",
    "    for (apt, pep) in eval_loss_samples:\n",
    "        _, out = update(apt, pep)\n",
    "        outs.append(out.cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# AUC Plot\n",
    "def cdf(scores1, scores2): # i is the index\n",
    "    _, ax = plt.subplots()\n",
    "    ax.hist(scores1, 100, histtype='step', density=True, cumulative=True, color='red', label='train cdf')\n",
    "    ax.hist(scores2, 100, histtype='step', density=True, cumulative=True, color='black', label='test cdf')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot(eval_scores, train_scores, test_scores, i):\n",
    "    f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='Eval: not in dataset', ax=axes[0, 0])\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=axes[1, 0])\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=axes[0, 1])\n",
    "    axes[0,0].set_title(\"Eval: not in dataset\")\n",
    "    axes[1,0].set_title(\"Train: in dataset\")\n",
    "    axes[0, 1].set_title(\"Test: in dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, lamb, gamma):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(train_losses, 'g', label='Train loss')\n",
    "    ax.plot(test_losses, 'p', label='Test loss')\n",
    "    ax.set_title('Loss after ' + str(i) + \" iterations, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, i, lamb, gamma):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(train_recalls, 'b', label='Train recall')\n",
    "    ax.plot(test_recalls, 'y', label='Test recall')\n",
    "    ax.legend()\n",
    "    ax.set_title('Recall after ' + str(i) + \" iterations, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(t=1, #num of iter over the training set\n",
    "        lamb=1e-1, #hyperparam\n",
    "        gamma=1e-2): #step size\n",
    "    optim = SGD(model.parameters(), lr=gamma)\n",
    "    for _ in range(t):\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "        last_eval_unknown_outputs_length = 0\n",
    "        last_train_scores_length = 0\n",
    "        last_test_scores_length = 0\n",
    "        for i, (apt, pep) in enumerate(tqdm.tqdm(train_loader)):\n",
    "            # Start timer here\n",
    "            if i == 0:\n",
    "                continue\n",
    "            model.train()\n",
    "            optim.zero_grad() #reset gradients after update\n",
    "            \n",
    "            x, y = convert(apt[0], pep[0]) #sample x,y from training set S\n",
    "            _, out = update(x, y) #get train score\n",
    "            log_out = torch.log(out) #take log\n",
    "            \n",
    "            \n",
    "            train_score = out.cpu().detach().numpy().flatten()[0] #get score in float\n",
    "            if train_score > 0.75:\n",
    "                train_correct += 1 #contribute to train recall\n",
    "            train_recall_outputs.append(train_score) #store the train scores\n",
    "            \n",
    "            optim.zero_grad() #reset gradients for the second branch\n",
    "            \n",
    "            x_prime, y_prime = convert(S_prime[i][0][0], S_prime[i][0][1]) #sample x', y' from S'\n",
    "            y_pmf, out_prime = update(x_prime, y_prime) #get score'\n",
    "            out_prime = out_prime*y_pmf*get_x_pmf()*2*n #adjust for IS\n",
    "            \n",
    "            const = S_prime[i][1] #indicator\n",
    "            (lamb*const*out_prime - log_out).backward(retain_graph=True) #backprop\n",
    "            optim.step() #gradient update\n",
    "            \n",
    "            \n",
    "            if i < int(0.2*n): #check if idx out of range\n",
    "                model.eval()\n",
    "                x_test, y_test = convert(test_set[i][0], test_set[i][1]) #sample x,y from test set\n",
    "                test_score = model(x_test, y_test).cpu().detach().numpy().flatten()[0] #get test score in float\n",
    "                test_recall_outputs.append(test_score) #store the test scores\n",
    "                if test_score > 0.75:\n",
    "                    test_correct += 1 #contribute to test recall\n",
    "            else:\n",
    "                test_correct += 1\n",
    "            #generate 10 unseen examples from S_new as compared 1 example from validate/train for cdfs\n",
    "            for x, y in S_new[-(10*i+1):9-10*i]:\n",
    "                x_eval, y_eval = convert(x, y) #generate unseen x'' and y''\n",
    "                score_eval = model(x_eval, y_eval).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                eval_unknown_outputs.append(score_eval) #store the unknown scores\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                out_prime = get_out_prime()\n",
    "                train_loss = lamb*out_prime - get_log_out('train') #training loss\n",
    "                test_loss = lamb*out_prime - get_log_out('test') #test loss\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                train_recall = 100*train_correct/i #training recall\n",
    "                train_recalls.append(train_recall) \n",
    "                test_recall = 100*test_correct/i #test recall\n",
    "                test_recalls.append(test_recall)\n",
    "                \n",
    "                train_score = np.asarray(eval_unknown_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                test_score = np.asarray(eval_unknown_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "                train_cdf = np.sum(np.cumsum(train_score), dtype=float)/(np.sum(train_score)*len(train_score)) #train cdf\n",
    "                test_cdf = np.sum(np.cumsum(test_score), dtype=float)/(np.sum(test_score)*len(test_score)) #test cdf\n",
    "                train_cdfs.append(train_cdf)\n",
    "                test_cdfs.append(test_cdf)\n",
    "                \n",
    "                \n",
    "            if i % 10000 == 0:\n",
    "                #T = [j for j in range(501, i, 10000//500)]\n",
    "                plot_recall(train_recalls, test_recalls, i, lamb, gamma)\n",
    "                plot_loss(train_losses, test_losses, i, lamb, gamma)\n",
    "                print(str(train_recalls))\n",
    "                print(str(test_recalls))\n",
    "                print(\"Train cdfs: \", train_cdfs[-2])\n",
    "                print(\"Test cdfs: \", test_cdfs[-2])\n",
    "                print(\"Train scores unkown: \", train_score[:2])\n",
    "                print(\"Train scores train: \", train_score[-2:])\n",
    "                print(\"Test scores unkown: \", test_score[:2])\n",
    "                print(\"Test scores train: \", test_score[-2:])\n",
    "                cdf(train_score, test_score)\n",
    "                # Create a scatter plot where we map train recall outputs, test recall outputs, eval outputs\n",
    "                scatterplot(eval_unknown_outputs, train_recall_outputs, test_recall_outputs, i)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [1e-2]\n",
    "lambdas = [1e-3, 10, 1e-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3023/473047 [00:55<5:09:49, 25.28it/s] "
     ]
    }
   ],
   "source": [
    "for g in range(len(gammas)):\n",
    "    for l in range(len(lambdas)):\n",
    "        train_losses = []\n",
    "        train_recalls = []\n",
    "        train_recall_outputs = [] \n",
    "        train_cdfs = []\n",
    "        \n",
    "        test_losses = []\n",
    "        test_recalls = []\n",
    "        test_recall_outputs = []\n",
    "        test_cdfs = []\n",
    "        \n",
    "        eval_unknown_outputs = []\n",
    "        \n",
    "        model = SimpleConvNet()\n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        \n",
    "        sgd(t=2, gamma=gammas[g], lamb=lambdas[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
