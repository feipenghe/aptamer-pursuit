{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_list = ['A', 'C', 'G', 'T']\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y']\n",
    "pvals = [0.089]*3 + [0.065]*5 + [0.034]*12\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed empty slots\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8:#making sure right length\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    return list(set(full_dataset)) #removed duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = construct_dataset()\n",
    "n = len(full_dataset)\n",
    "aptamers = [p[0] for p in full_dataset]\n",
    "peptides = [p[1] for p in full_dataset]\n",
    "training_set = full_dataset[:int(0.8*n)]\n",
    "test_set = full_dataset[int(0.8*n):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(TrainDataset, self).__init__() \n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, test_set):\n",
    "        super(TestDataset, self).__init__() \n",
    "        self.test_set = test_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.test_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.test_set[idx]\n",
    "        return aptamer, peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(training_set)\n",
    "test_dataset = TestDataset(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence_list, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    \n",
    "    one_hot = np.zeros((len(sequence_list), len(sequence_list[0]), len(letters)))\n",
    "    \n",
    "    for j in range(len(sequence_list)):\n",
    "        sequence = sequence_list[j]\n",
    "        for i in range(len(sequence)):\n",
    "            element = sequence[i]\n",
    "            idx = letters.index(element)\n",
    "            one_hot[j][i][idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(40, 20, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(20, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(8, 4, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(4, 3, 1)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_2, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt).cuda()\n",
    "        pep = self.sequential_pep(pep).cuda()\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (cnn_apt_1): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cnn_apt_2): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cnn_apt_3): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc_apt_1): Linear(in_features=160, out_features=1, bias=True)\n",
       "  (cnn_pep_1): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cnn_pep_2): Conv2d(4, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc_pep_1): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (sequential_pep): Sequential(\n",
       "    (0): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(4, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sequential_apt): Sequential(\n",
       "    (0): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(20, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "\n",
    "model.apply(weights_init)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample x from P_X (assume peptides follow NNK)\n",
    "def get_x():\n",
    "    x_idx = np.random.choice(20, 7, p=pvals)\n",
    "    x = \"M\"\n",
    "    for i in x_idx:\n",
    "        x += aa_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_Y (assume apatamers follow uniform)\n",
    "def get_y():\n",
    "    y_idx = np.random.randint(0, 4, 40)\n",
    "    y = \"\"\n",
    "    for i in y_idx:\n",
    "        y += na_list[i]\n",
    "    return y\n",
    "\n",
    "# Generate uniformly from S without replacement\n",
    "def get_xy(k):\n",
    "    samples = [full_dataset[i] for i in np.random.choice(len(full_dataset), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# S' contains S with double the size of S (domain for Importance Sampling)\n",
    "def get_S_prime(k):\n",
    "    S_prime = full_dataset[:]\n",
    "    for _ in range(k):\n",
    "        S_prime.append((get_y(), get_x()))\n",
    "    return list(set(S_prime))\n",
    "\n",
    "# Sample from S' without replacement\n",
    "def get_xy_prime(k):\n",
    "    samples = [S_prime[i] for i in np.random.choice(len(S_prime), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_x_pmf(x):\n",
    "    pmf = 1\n",
    "    for char in x[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf\n",
    "\n",
    "# Returns pmf of an aptamer\n",
    "def get_y_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "S_prime = get_S_prime(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(type=\"original\"):\n",
    "    if type == \"original\":\n",
    "        xy = get_xy(1)[0]\n",
    "    else:\n",
    "        xy = get_xy_prime(1)[0]\n",
    "    x = one_hot(xy[0], seq_type='aptamer') \n",
    "    y = one_hot(xy[1], seq_type='peptide') \n",
    "    x = torch.FloatTensor(np.reshape(x, (1, x.shape[0], x.shape[1], x.shape[2])))\n",
    "    y = torch.FloatTensor(np.reshape(y, (1, y.shape[0], y.shape[1], y.shape[2])))\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return xy, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(t=500, #num of iter\n",
    "        lamb=1e-5, #hyperparam\n",
    "        gamma=1e-5): #step size\n",
    "    \n",
    "    model.train()\n",
    "    for a, _ in enumerate(tqdm.tqdm(range(t))):\n",
    "        \n",
    "        xy, out = update()\n",
    "        out.retain_grad()\n",
    "        model.zero_grad()\n",
    "        out.backward()\n",
    "        \n",
    "        xy_prime, out_prime = update(\"prime\")\n",
    "        out_prime = out_prime * get_x_pmf(xy_prime[0]) * get_y_pmf() * 2 * n\n",
    "        out_prime.retain_grad()\n",
    "        model.zero_grad()\n",
    "        out_prime.backward()\n",
    "        \n",
    "        const = 0 if xy_prime in full_dataset else 1 #indicator\n",
    "        g = torch.log(out.grad) - lamb*const*out_prime.grad\n",
    "        g = g.item()\n",
    "        \n",
    "        # Update the weights according to SGD\n",
    "        for param in model.parameters():\n",
    "            param.data += gamma * g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    model.eval()\n",
    "    for _, (aptamer, peptide) in enumerate(tqdm.tqdm(test_loader)):\n",
    "        pep = one_hot(peptide, seq_type='peptide')\n",
    "        apt = one_hot(aptamer, seq_type='aptamer')\n",
    "\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[1], pep.shape[2], pep.shape[0]))).cuda()\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[1], apt.shape[2], apt.shape[0]))).cuda()\n",
    "        \n",
    "        output = model(apt, pep).cpu().detach().numpy().flatten()\n",
    "        for i in range(output.shape[0]):\n",
    "            o = output[i]\n",
    "            if o > 0.5:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        \n",
    "    return (100* correct/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:52<00:00,  4.30it/s]\n",
      "  0%|          | 8/118262 [00:00<25:29, 77.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118262/118262 [11:17<00:00, 174.46it/s]\n",
      "  0%|          | 1/1000 [00:00<02:46,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 0 , lambda: 0 recall:  100.0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 96478/118262 [05:41<01:15, 288.05it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 33%|███▎      | 38632/118262 [03:39<07:46, 170.62it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 81%|████████  | 95444/118262 [09:00<01:56, 196.21it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1000/1000 [04:09<00:00,  4.01it/s]\n",
      "  0%|          | 10/118262 [00:00<20:41, 95.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 40582/118262 [05:56<13:02, 99.32it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [13:29<00:00, 146.04it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 0 , lambda: 3 recall:  16.700208012717525\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:47<00:00,  4.40it/s]\n",
      "  0%|          | 19/118262 [00:00<11:28, 171.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11201/118262 [01:06<09:46, 182.41it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████▉| 117873/118262 [11:13<00:02, 141.94it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 68%|██████▊   | 80237/118262 [07:10<03:42, 170.78it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [11:14<00:00, 175.42it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 1 , lambda: 0 recall:  100.0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:24<00:00,  3.78it/s]\n",
      "  0%|          | 7/118262 [00:00<34:02, 57.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 42728/118262 [06:24<14:09, 88.92it/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [16:15<00:00, 121.22it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 1 , lambda: 1 recall:  90.86857993269182\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:23<00:00,  3.79it/s]\n",
      "  0%|          | 10/118262 [00:00<19:57, 98.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3776/118262 [00:21<09:33, 199.56it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [11:49<00:00, 166.73it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 1 , lambda: 2 recall:  50.3509157633052\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 242/1000 [00:54<03:11,  3.96it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 95%|█████████▍| 111934/118262 [10:43<00:37, 169.54it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|█████████ | 107292/118262 [09:46<00:31, 353.71it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [06:17<00:00, 312.98it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 2 , lambda: 0 recall:  43.423077573523194\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:08<00:00,  5.32it/s]\n",
      "  0%|          | 38/118262 [00:00<05:13, 377.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 16725/118262 [00:55<05:35, 302.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [06:14<00:00, 315.52it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 2 , lambda: 1 recall:  24.9513791412288\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:12<00:00,  5.20it/s]\n",
      "  0%|          | 23/118262 [00:00<08:46, 224.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 69653/118262 [03:34<02:43, 296.97it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 88%|████████▊ | 103639/118262 [05:22<00:47, 309.97it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s]it/s]\n",
      "  0%|          | 24/118262 [00:00<08:18, 236.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 80589/118262 [04:23<02:10, 287.69it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [08:28<00:00, 232.36it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 3 , lambda: 2 recall:  73.68808239333006\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 214/1000 [00:40<03:21,  3.90it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [06:16<00:00, 314.14it/s]\n",
      "  0%|          | 1/1000 [00:00<02:58,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 3 , lambda: 4 recall:  92.73477532935347\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:20<00:00,  4.98it/s]\n",
      "  0%|          | 29/118262 [00:00<06:51, 287.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 43034/118262 [02:13<03:46, 331.86it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 87%|████████▋ | 102449/118262 [05:22<00:47, 331.56it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 118262/118262 [06:09<00:00, 319.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall with gamma: 4 , lambda: 4 recall:  35.76465813194433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "results = []\n",
    "\n",
    "M = np.zeros((len(gammas), len(lambdas)))\n",
    "for g in range(len(gammas)):\n",
    "    for l in range(len(lambdas)):\n",
    "        model = ConvNet()\n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        print(\"Training...\")\n",
    "        sgd(t=1000, gamma=gammas[g], lamb=lambdas[l])\n",
    "        print(\"Evaluating...\")\n",
    "        recall = evaluate()\n",
    "        M[g][l] += recall\n",
    "        results.append(recall)\n",
    "        print(\"Recall with gamma: \"+ str(gammas[g]) + \" , lambda: \" + str(lambdas[l]) + \" recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma:  0.00010 Lambda:  0.01000 Recall:  14.20\n",
      "Gamma:  0.10000 Lambda:  0.00010 Recall:  16.70\n",
      "Gamma:  0.00100 Lambda:  0.00010 Recall:  19.89\n",
      "Gamma:  0.00100 Lambda:  0.01000 Recall:  24.95\n",
      "Gamma:  0.00001 Lambda:  0.00100 Recall:  25.45\n",
      "Gamma:  0.00010 Lambda:  0.10000 Recall:  29.36\n",
      "Gamma:  0.00001 Lambda:  0.10000 Recall:  32.14\n",
      "Gamma:  0.00001 Lambda:  0.00001 Recall:  35.76\n",
      "Gamma:  0.00100 Lambda:  0.10000 Recall:  43.42\n",
      "Gamma:  0.00010 Lambda:  0.00010 Recall:  44.87\n",
      "Gamma:  0.10000 Lambda:  0.00100 Recall:  48.91\n",
      "Gamma:  0.01000 Lambda:  0.00010 Recall:  49.61\n",
      "Gamma:  0.01000 Lambda:  0.00100 Recall:  50.35\n",
      "Gamma:  0.10000 Lambda:  0.00001 Recall:  56.65\n",
      "Gamma:  0.00100 Lambda:  0.00001 Recall:  60.81\n",
      "Gamma:  0.00001 Lambda:  0.00010 Recall:  62.21\n",
      "Gamma:  0.00010 Lambda:  0.00100 Recall:  73.69\n",
      "Gamma:  0.01000 Lambda:  0.00001 Recall:  75.56\n",
      "Gamma:  0.00001 Lambda:  0.01000 Recall:  83.36\n",
      "Gamma:  0.01000 Lambda:  0.01000 Recall:  90.87\n",
      "Gamma:  0.00100 Lambda:  0.00100 Recall:  91.06\n",
      "Gamma:  0.00010 Lambda:  0.00001 Recall:  92.73\n",
      "Gamma:  0.10000 Lambda:  0.10000 Recall:  100.00\n",
      "Gamma:  0.10000 Lambda:  0.01000 Recall:  100.00\n",
      "Gamma:  0.01000 Lambda:  0.10000 Recall:  100.00\n"
     ]
    }
   ],
   "source": [
    "# Table of recalls with different params\n",
    "idx = sorted(range(len(results)), key=lambda k: results[k])\n",
    "for i in idx:\n",
    "    g = gammas[i//len(gammas)]\n",
    "    l = lambdas[i%len(lambdas)]\n",
    "    print(\"Gamma: \", \"%.5f\" % g, \"Lambda: \", \"%.5f\" % l, \"Recall: \", \"%.2f\" % results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap of recalls\n",
    "mat = sns.heatmap(M, vmin=0, vmax=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC --> Ordering graphic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "walnutree",
   "language": "python",
   "name": "walnutree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
