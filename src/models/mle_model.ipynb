{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "\n",
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    full_dataset = []\n",
    "    aptamers = []\n",
    "    peptides = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                full_dataset.append((aptamer, peptide))\n",
    "    full_dataset = list(set(full_dataset)) #removed duplicates\n",
    "    for pair in full_dataset:\n",
    "        aptamers.append(pair[0])\n",
    "        peptides.append(pair[1])\n",
    "    return full_dataset, aptamers, peptides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_set):\n",
    "        super(TrainDataset, self).__init__() \n",
    "        self.training_set = training_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.training_set[idx]\n",
    "        return aptamer, peptide\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, test_set):\n",
    "        super(TestDataset, self).__init__() \n",
    "        self.test_set = test_set\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.test_set)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        aptamer, peptide = self.test_set[idx]\n",
    "        return aptamer, peptide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset, aptamers, peptides = construct_dataset()\n",
    "n = len(full_dataset)\n",
    "training_set = full_dataset[:int(0.8*n)]\n",
    "test_set = full_dataset[int(0.8*n):]\n",
    "train_dataset = TrainDataset(training_set)\n",
    "test_dataset = TestDataset(test_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence_list, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    \n",
    "    one_hot = np.zeros((len(sequence_list), len(sequence_list[0]), len(letters)))\n",
    "    \n",
    "    for j in range(len(sequence_list)):\n",
    "        sequence = sequence_list[j]\n",
    "        for i in range(len(sequence)):\n",
    "            element = sequence[i]\n",
    "            idx = letters.index(element)\n",
    "            one_hot[j][i][idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(40, 20, 1)\n",
    "        self.cnn_apt_2 = nn.Conv2d(20, 10, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(10, 1, 1)\n",
    "        self.fc_apt_1 = nn.Linear(160, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(8, 4, 1)\n",
    "        self.cnn_pep_2 = nn.Conv2d(4, 3, 1)\n",
    "        self.fc_pep_1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_pep_2)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_2, \n",
    "                                            self.relu, \n",
    "                                            self.pool, \n",
    "                                            self.cnn_apt_3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.sequential_apt(apt).cuda()\n",
    "        pep = self.sequential_pep(pep).cuda()\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(40, 10, (3,1))\n",
    "        self.cnn_pep_1 = nn.Conv2d(8, 4, (2,1))\n",
    "        self.pool = nn.MaxPool2d(1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "        self.sequential_pep = nn.Sequential(self.cnn_pep_1,\n",
    "                                            self.relu, \n",
    "                                            self.pool)\n",
    "        \n",
    "        self.sequential_apt = nn.Sequential(self.cnn_apt_1, \n",
    "                                            self.relu, \n",
    "                                            self.pool)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        print(apt.shape)\n",
    "        print(pep.shape)\n",
    "        apt = self.sequential_apt(apt).cuda()\n",
    "        pep = self.sequential_pep(pep).cuda()\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample x from P_X (assume peptides follow NNK)\n",
    "def get_x():\n",
    "    x_idx = np.random.choice(20, 7, p=pvals)\n",
    "    x = \"M\"\n",
    "    for i in x_idx:\n",
    "        x += aa_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_Y (assume apatamers follow uniform)\n",
    "def get_y():\n",
    "    y_idx = np.random.randint(0, 4, 40)\n",
    "    y = \"\"\n",
    "    for i in y_idx:\n",
    "        y += na_list[i]\n",
    "    return y\n",
    "\n",
    "# Generate uniformly from S without replacement\n",
    "def get_xy(k):\n",
    "    samples = [full_dataset[i] for i in np.random.choice(len(full_dataset), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# S' contains S with double the size of S (domain for Importance Sampling)\n",
    "def get_S_prime(k):\n",
    "    S_prime = full_dataset[:]\n",
    "    for _ in range(k):\n",
    "        S_prime.append((get_y(), get_x()))\n",
    "    return list(set(S_prime))\n",
    "\n",
    "# Sample from S' without replacement\n",
    "def get_xy_prime(k):\n",
    "    samples = [S_prime[i] for i in np.random.choice(len(S_prime), k, replace=False)]\n",
    "    return samples\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_x_pmf(x):\n",
    "    pmf = 1\n",
    "    for char in x[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf\n",
    "\n",
    "# Returns pmf of an aptamer\n",
    "def get_y_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "S_prime = get_S_prime(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(type=\"original\"):\n",
    "    if type == \"original\":\n",
    "        xy = get_xy(1)[0]\n",
    "    else:\n",
    "        xy = get_xy_prime(1)[0]\n",
    "    x = one_hot(xy[0], seq_type='aptamer') \n",
    "    y = one_hot(xy[1], seq_type='peptide') \n",
    "    #x = torch.FloatTensor(np.reshape(x, (1, x.shape[0], x.shape[1], x.shape[2]))) #(1, 40, 1, 4) original\n",
    "    x = torch.FloatTensor(np.reshape(x, (1, x.shape[0], x.shape[2], x.shape[1]))) # (1, 40, 4, 1)\n",
    "    #y = torch.FloatTensor(np.reshape(y, (1, y.shape[0], y.shape[1], y.shape[2]))) #(1, 8, 1, 20) original\n",
    "    y = torch.FloatTensor(np.reshape(y, (1, y.shape[0], y.shape[2], y.shape[1]))) #(1, 8, 20, 1)\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    out = model(x, y)\n",
    "    return xy, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(t=1000, #num of iter\n",
    "        lamb=1e-5, #hyperparam\n",
    "        gamma=1e-4): #step size\n",
    "    \n",
    "    model.train()\n",
    "    for a, _ in enumerate(tqdm.tqdm(range(t))):\n",
    "        xy, out = update()\n",
    "        out.retain_grad()\n",
    "        log_out = torch.log(out)\n",
    "        log_out.retain_grad()\n",
    "        model.zero_grad()\n",
    "        log_out.backward()\n",
    "        \n",
    "        xy_prime, out_prime = update(\"prime\")\n",
    "        out_prime = out_prime * get_x_pmf(xy_prime[0]) * get_y_pmf() * 2 * n\n",
    "        out_prime.retain_grad()\n",
    "        model.zero_grad()\n",
    "        out_prime.backward()\n",
    "        \n",
    "        const = 0 if xy_prime in full_dataset else 1 #indicator\n",
    "        g = log_out.grad - lamb*const*out_prime.grad\n",
    "        g = g.item()\n",
    "        \n",
    "        #Update the weights according to SGD\n",
    "        for param in model.parameters():\n",
    "            param.data += gamma * g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval on test set of size k (split from our dataset)\n",
    "def recall_eval(k):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    binding_outputs = []\n",
    "    model.eval()\n",
    "    for _, (aptamer, peptide) in enumerate(tqdm.tqdm(test_loader)):\n",
    "        if count > k:\n",
    "            break\n",
    "        pep = one_hot(peptide, seq_type='peptide')\n",
    "        apt = one_hot(aptamer, seq_type='aptamer')\n",
    "        #pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[1], pep.shape[2], pep.shape[0]))).cuda()\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[2], pep.shape[1], pep.shape[0]))).cuda()\n",
    "        #apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[1], apt.shape[2], apt.shape[0]))).cuda()\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[2], apt.shape[1], apt.shape[0]))).cuda()\n",
    "        output = model(apt, pep).cpu().detach().numpy().flatten()[0]\n",
    "        binding_outputs.append('%.2f'% output)\n",
    "        if output > 0.5:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    recall = 100*correct/count #recall rate of k samples\n",
    "    return recall, binding_outputs #list of k outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 1, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 1, 20)\n",
    "    #apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[2], apt.shape[1]))).cuda() #(1, 40, 4, 1)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[1], apt.shape[0], apt.shape[2]))).cuda() #(1, 1, 40, 4)\n",
    "    #pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[2], pep.shape[1]))).cuda() #(1, 8, 20, 1)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[1], pep.shape[0], pep.shape[2]))).cuda() #(1, 1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Eval on m new unseen pairs(not in our dataset)\n",
    "def evaluate(m):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for _ in range(m):\n",
    "        x, y = get_x(), get_y()\n",
    "        apt, pep = convert(y, x)\n",
    "        output = model(apt, pep).cpu().detach().numpy().flatten()[0]\n",
    "        outputs.append('%.2f'% output)\n",
    "    return outputs #list of m outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 10000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d956d85f59b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise RuntimeError(\n\u001b[1;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd1/home/aishrm2/anaconda3/envs/research/lib/python2.7/site-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 10000).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "lambdas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "recalls = []\n",
    "scores = []\n",
    "\n",
    "m = int(1e2) # number of unknown samples\n",
    "k = int(30) # number of binding samples (test set size is 118262, k is just some limit we set)\n",
    "\n",
    "M = np.zeros((len(gammas), len(lambdas)))\n",
    "for g in range(len(gammas)):\n",
    "    for l in range(len(lambdas)):\n",
    "        model = SimpleConvNet()\n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        print(\"Training...\")\n",
    "        sgd(t=100, gamma=gammas[g], lamb=lambdas[l])\n",
    "        print(\"Evaluating...\")\n",
    "        # use for AUC\n",
    "        recall, binding_outputs = recall_eval(k)\n",
    "        unknown_outputs = evaluate(m)\n",
    "        scores.append((unknown_outputs, binding_outputs))\n",
    "        # use for heatmap\n",
    "        M[g][l] += recall\n",
    "        recalls.append(recall)\n",
    "        print(\"Recall with gamma: \"+ str(gammas[g]) + \" , lambda: \" + str(lambdas[l]) + \" recall: \", '%.2f'% recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of recalls with different params\n",
    "idx = sorted(range(len(recalls)), key=lambda k: recalls[k])\n",
    "for i in idx:\n",
    "    g = gammas[i//len(gammas)]\n",
    "    l = lambdas[i%len(lambdas)]\n",
    "    print(\"Gamma: \", \"%.5f\" % g, \"Lambda: \", \"%.5f\" % l, \"Recall: \", \"%.2f\" % recalls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of recalls\n",
    "mat = sns.heatmap(M, vmin=0, vmax=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC (one config for now)\n",
    "unknown, binding = scores[0]\n",
    "total = unknown + binding\n",
    "plt.hist(total, 50, histtype='step', density=True, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvNet()\n",
    "model.apply(weights_init)\n",
    "model.cuda()\n",
    "print(\"Training...\")\n",
    "sgd(t=1000, gamma=0.01, lamb=0.01)\n",
    "print(\"Evaluating...\")\n",
    "# use for AUC\n",
    "recall, binding_outputs = recall_eval(50)\n",
    "unknown_outputs = evaluate(500)\n",
    "# use for heatmap\n",
    "print(\"Recall with gamma: \"+ str(0.01) + \" , lambda: \" + str(0.01) + \" recall: \", '%.2f'% recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
