{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1d, self).__init__()\n",
    "        self.name = \"LinearConv1d\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 10, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(10, 25, 3) \n",
    "        self.cnn_apt_3 = nn.Conv1d(25, 50, 3) \n",
    "        self.cnn_apt_4 = nn.Conv1d(50, 100, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(200, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        #print(apt.size())\n",
    "        #print(pep.size())\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSimple, self).__init__()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(100, 250, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 250, 1)\n",
    "       \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(500, 550)\n",
    "        self.fc2 = nn.Linear(550, 600)\n",
    "        self.fc3 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        \n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "\n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "    \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc3(self.fc2(self.fc1(x)))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetComplex, self).__init__()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(100, 200, 3)\n",
    "        self.cnn_apt_5 = nn.Conv1d(200, 400, 1)\n",
    "        self.cnn_apt_6 = nn.Conv1d(400, 800, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 200, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(200, 400, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(400, 800, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(1600, 1800)\n",
    "        self.fc2 = nn.Linear(1800, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.relu(self.cnn_apt_1(apt))\n",
    "        apt = self.relu(self.cnn_apt_2(apt))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_5(apt)))        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_6(apt)))        \n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        pep = self.relu(self.cnn_pep_2(pep))\n",
    "        pep = self.relu(self.cnn_pep_3(pep))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_4(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_5(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariedChannelsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariedChannelsNet, self).__init__()\n",
    "        self.name = \"VariedChannelsNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 800, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(800, 600, 3, padding=2)\n",
    "        self.cnn_apt_4 = nn.Conv1d(600, 400, 3, padding=2)\n",
    "        self.cnn_apt_5 = nn.Conv1d(400, 200, 3, padding=2)\n",
    "        self.cnn_apt_6 = nn.Conv1d(200, 100, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 450, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(450, 400, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(400, 350, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(350, 250, 3, padding=2)\n",
    "        self.cnn_pep_6 = nn.Conv1d(250, 200, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_5(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_6(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_4(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_5(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_6(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimizedVCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinimizedVCNet, self).__init__()\n",
    "        self.name = \"MinimizedVCNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 500, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(500, 100, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 250, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(250, 100, 3, padding=2)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(800, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing NN models\n",
    "# model = ConvNetComplex()\n",
    "# model.apply(weights_init)\n",
    "# model.to(device)\n",
    "# aptamer, peptide, (apt_prime, pep_prime), indicator = train_ds[0]\n",
    "# a, p = convert(aptamer, peptide)\n",
    "# output = model(a, p)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[0], apt.shape[1]))).to(device) #(1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[0], pep.shape[1]))).to(device) #(1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.to(device)\n",
    "        y = pep.to(device)\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_train_loss(train_loss, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.ylabel(\"Train loss\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Train loss at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/train_loss.png' %model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_loss(test_loss, iters, epoch, lamb, gamma, model_name):\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Test loss\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Test loss at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/test_loss.png' %model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"%d number of samples\" %iters)\n",
    "    plt.title(\"Recall at epoch %d,\" %epoch + \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/recall.png' %model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_ecdf_test(test_score, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Test CDF at epoch %d' %epoch+ \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/test_cdf.png' %model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, iters, epoch, lamb, gamma, model_name, model_id):\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Train CDF at epoch %d' %epoch+ \" Lambda :%.5f\" %lamb + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/train_cdf.png' %model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores, model_name, model_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=ax)\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=ax)\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=ax)\n",
    "    ax.set_title(\"Distribution of Scores\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.savefig('plots/mle/%s/%s/histogram.png' % model_name, model_id, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_samples = loss_samples(k, 'train')\n",
    "test_loss_samples = loss_samples(k, 'test')\n",
    "prime_train_loss_samples = prime_loss_samples(k, 'train')\n",
    "prime_test_loss_samples = prime_loss_samples(k, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lamb = hyperparameter\n",
    "gamma = step size\n",
    "run_from_checkpoint = path to a checkpointed model\n",
    "save_checkpoings = file name\n",
    "'''\n",
    "def sgd(model_id, num_epochs=50,\n",
    "        batch_size=64,\n",
    "        l=5, #lambda\n",
    "        gamma=5e-3, #starting lr\n",
    "        run_from_checkpoint=None, \n",
    "        save_checkpoints=None): \n",
    "    \n",
    "    init_epoch = 0\n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim = SGD(model.parameters(), lr=gamma)\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "        optim = SGD(model.parameters(), lr=gamma)\n",
    "        \n",
    "    total_epochs = init_epoch\n",
    "    train_losses, train_recalls, train_recall_outputs = [], [], []\n",
    "    test_losses, test_recalls, test_recall_outputs = [], [], []\n",
    "    new_outputs = []\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "    iters = 0\n",
    "    model_name = model.name\n",
    "    scheduler = StepLR(optim, step_size=2, gamma=0.9) #Decays lr by gamma factor every step_size epochs. \n",
    "    \n",
    "    for epoch in range(init_epoch, num_epochs+init_epoch):\n",
    "        scheduler.step()\n",
    "        print(\"Training Epoch: \", total_epochs, \" with learning rate: \", scheduler.get_lr())\n",
    "        total_train_loss = 0\n",
    "        for i, (aptamer, peptide, (apt_prime, pep_prime), indicator) in enumerate(tqdm.tqdm(train_ds)):\n",
    "            iters += 1\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            optim.zero_grad() \n",
    "            x, y = convert(aptamer, peptide) #sample x,y from S_train\n",
    "            out = update(x, y) #get S_train output/score\n",
    "            log_out = torch.log(out) \n",
    "            train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "            if train_score > 0.5:\n",
    "                train_correct += 1 \n",
    "            train_recall_outputs.append(train_score) \n",
    "\n",
    "            optim.zero_grad() \n",
    "            y_pmf = get_y_pmf(pep_prime)\n",
    "            x_prime, y_prime = convert(apt_prime, pep_prime) #sample x', y' from S_prime_train\n",
    "            out_prime = update(x_prime, y_prime) #get score from S_prime_train\n",
    "            if indicator == 0:\n",
    "                factor = (2*m*get_x_pmf()*y_pmf)/(1+m*get_x_pmf()*y_pmf)\n",
    "            else:\n",
    "                factor = 2\n",
    "            out_prime = out_prime*factor #adjust for IS\n",
    "            \n",
    "            total_train_loss += l*indicator*out_prime - log_out\n",
    "            if i % batch_size == 0:\n",
    "                #(total_train_loss/batch_size).backward(retain_graph=True) \n",
    "                (total_train_loss/batch_size).backward() \n",
    "                optim.step()\n",
    "                total_train_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "\n",
    "            x_test, y_test = convert(S_test[i%(n-m)][0], S_test[i%(n-m)][1]) #sample x,y from test set\n",
    "            test_score = model(x_test, y_test).cpu().detach().numpy().flatten()[0]\n",
    "            test_recall_outputs.append(test_score) \n",
    "            if test_score > 0.5:\n",
    "                test_correct += 1 \n",
    "\n",
    "            #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "            for x, y in S_new[10*i:10*(i+1)]:\n",
    "                x_new, y_new = convert(x, y) #generate unseen x'' and y'' from S_new\n",
    "                new_score = model(x_new, y_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                new_outputs.append(new_score)\n",
    "\n",
    "            if i % 20000 == 0:\n",
    "                train_loss = l*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                test_loss = (m/(n-m))*l*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                train_recall = 100*train_correct/iters #training recall\n",
    "                train_recalls.append(train_recall) \n",
    "                test_recall = 100*test_correct/iters #test recall\n",
    "                test_recalls.append(test_recall)\n",
    "                if i > 1000:\n",
    "                    train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                    test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                else:\n",
    "                    train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                    test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "\n",
    "            if i % 200000 == 0:\n",
    "                plot_recall(train_recalls, test_recalls, iters, total_epochs, l, gamma, model_name, model_id)\n",
    "                plot_train_loss(train_losses, iters, total_epochs, l, gamma, model_name, model_id)\n",
    "                plot_test_loss(test_losses, iters, total_epochs, l, gamma, model_name, model_id)\n",
    "                plot_ecdf_train(train_score, iters, total_epochs, l, gamma, model_name, model_id)\n",
    "                plot_ecdf_test(test_score, iters, total_epochs, l, gamma, model_name, model_id)\n",
    "                histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:], model_name, model_id)\n",
    "                print(\"New score: \", np.average(new_outputs[-500:]))\n",
    "                print(\"Train score: \", np.average(train_score[-500:]))\n",
    "                print(\"Test score: \", np.average(test_score[-500:]))\n",
    "\n",
    "        # Save after every epoch\n",
    "        total_epochs += 1\n",
    "        if save_checkpoints is not None:\n",
    "            print(\"Saving to: \", save_checkpoints)\n",
    "            checkpoint_name = save_checkpoints\n",
    "            torch.save({'epoch': total_epochs,\n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optim.state_dict()}, checkpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LAMBDA = 3\n",
    "GAMMA = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MinimizedVCNet()\n",
    "model.to(device)\n",
    "model_name = model.name\n",
    "model_id = '05112020'\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/%s/%s.pth' % model_name, model_id\n",
    "\n",
    "sgd(model_id=model_id,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    l=LAMBDA, \n",
    "    gamma=GAMMA,\n",
    "    run_from_checkpoint=checkpoint, \n",
    "    save_checkpoints=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
