{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        for peptide in peptides:\n",
    "            ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1d, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"LinearConv1d\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetComplex, self).__init__()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 1000, 3, padding=2)\n",
    "        self.cnn_apt_3 = nn.Conv1d(1000, 1000, 3, padding=2)\n",
    "        self.cnn_apt_4 = nn.Conv1d(1000, 1000, 3, padding=2)\n",
    "        self.cnn_apt_5 = nn.Conv1d(1000, 1000, 3, padding=2)\n",
    "        self.cnn_apt_6 = nn.Conv1d(1000, 1000, 3, padding=2)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 500, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 500, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(500, 500, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(500, 500, 3, padding=2)\n",
    "        self.cnn_pep_6 = nn.Conv1d(500, 500, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(3000, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_5(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_6(apt)))\n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.pool1(self.relu(self.cnn_pep_1(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_4(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_5(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_6(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[0], apt.shape[1]))).to(device) #(1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[0], pep.shape[1]))).to(device) #(1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.to(device)\n",
    "        y = pep.to(device)\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, i, j, lamb, gamma):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, i, j, lamb, gamma):\n",
    "    #train_score consisits of [10000 scores generated] + [1000 scores from training set]\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=ax)\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=ax)\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=ax)\n",
    "    ax.set_title(\"Distribution of Scores\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_samples = loss_samples(k, 'train')\n",
    "test_loss_samples = loss_samples(k, 'test')\n",
    "prime_train_loss_samples = prime_loss_samples(k, 'train')\n",
    "prime_test_loss_samples = prime_loss_samples(k, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lamb = hyperparameter\n",
    "gamma = step size\n",
    "run_from_checkpoint = path to a checkpointed model\n",
    "save_checkpoings = file name\n",
    "'''\n",
    "def sgd(epochs=[1, 2, 3], \n",
    "        lamb=[10, 10, 10], \n",
    "        gamma=[1e-3, 1e-4, 1e-5], \n",
    "        run_from_checkpoint=None, \n",
    "        save_checkpoints=None): \n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        \n",
    "        optim = SGD(model.parameters(), lr=gamma[0])\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        reloaded_epoch = checkpoint['epoch']\n",
    "        print(\"Reloading model: \" + str(model.name) + \" at epoch: \" + str(reloaded_epoch))\n",
    "        epoch = reloaded_epoch\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    total_epochs = 0\n",
    "    for i in range(len(epochs)):\n",
    "        g = gamma[i]\n",
    "        l = lamb[i]\n",
    "        eps = epochs[i]\n",
    "        epoch = 0\n",
    "        optim = SGD(model.parameters(), lr=g)\n",
    "        while epoch < eps:\n",
    "            train_recalls = []\n",
    "            train_recall_outputs = [] \n",
    "            test_recalls = []\n",
    "            test_recall_outputs = []\n",
    "            new_outputs = []\n",
    "            train_correct = 0\n",
    "            test_correct = 0\n",
    "            print(\"Training Epoch: \", total_epochs)\n",
    "            for i, (aptamer, peptide, (apt_prime, pep_prime), indicator) in enumerate(tqdm.tqdm(train_ds)):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                model.train()\n",
    "                optim.zero_grad() \n",
    "                x, y = convert(aptamer, peptide) #sample x,y from S_train\n",
    "                out = update(x, y) #get S_train output/score\n",
    "                log_out = torch.log(out) \n",
    "\n",
    "                train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "                if train_score > 0.6:\n",
    "                    train_correct += 1 \n",
    "                train_recall_outputs.append(train_score) \n",
    "\n",
    "                optim.zero_grad() \n",
    "                y_pmf = get_y_pmf(pep_prime)\n",
    "                x_prime, y_prime = convert(apt_prime, pep_prime) #sample x', y' from S_prime_train\n",
    "                out_prime = update(x_prime, y_prime) #get score from S_prime_train\n",
    "                if indicator == 0:\n",
    "                    factor = (2*m*get_x_pmf()*y_pmf)/(1+m*get_x_pmf()*y_pmf)\n",
    "                else:\n",
    "                    factor = 2\n",
    "                out_prime = out_prime*factor #adjust for IS\n",
    "                #print(\"Obj first part: \", out_prime.cpu().detach().numpy().flatten()[0]*lamb*indicator)\n",
    "                #print(\"Obj second part: \", log_out.cpu().detach().numpy().flatten()[0])\n",
    "                # Retain graph retains the graph for further operations\n",
    "                (l*indicator*out_prime - log_out).backward(retain_graph=True) \n",
    "                optim.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "\n",
    "                x_test, y_test = convert(S_test[i%(n-m)][0], S_test[i%(n-m)][1]) #sample x,y from test set\n",
    "                test_score = model(x_test, y_test).cpu().detach().numpy().flatten()[0]\n",
    "                test_recall_outputs.append(test_score) \n",
    "                if test_score > 0.6:\n",
    "                    test_correct += 1 \n",
    "\n",
    "                #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "                for x, y in S_new[10*i:10*(i+1)]:\n",
    "                    x_new, y_new = convert(x, y) #generate unseen x'' and y'' from S_new\n",
    "                    new_score = model(x_new, y_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                    new_outputs.append(new_score)\n",
    "\n",
    "                if i % 10000 == 0:\n",
    "                    train_loss = l*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                    #print(\"Train loss first part: \", lamb*get_out_prime(\"train\"))\n",
    "                    #print(\"Train loss second part: \", get_log_out('train'))\n",
    "                    test_loss = (m/(n-m))*l*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                    #print(\"Test loss first part: \", lamb*get_out_prime(\"test\"))\n",
    "                    #print(\"Test loss second part: \", get_log_out('test'))\n",
    "                    train_losses.append(train_loss)\n",
    "                    test_losses.append(test_loss)\n",
    "\n",
    "                    train_recall = 100*train_correct/(total_epochs*m + i) #training recall\n",
    "                    train_recalls.append(train_recall) \n",
    "                    test_recall = 100*test_correct/(total_epochs*m + i) #test recall\n",
    "                    test_recalls.append(test_recall)\n",
    "                    if i > 1000:\n",
    "                        train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                        test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                    else:\n",
    "                        train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                        test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "\n",
    "\n",
    "                if i % 100000 == 0:\n",
    "                    plot_recall(train_recalls, test_recalls, i, total_epochs, l, g)\n",
    "                    plot_loss(train_losses, test_losses, i, total_epochs, l, g)\n",
    "                    plot_ecdf_train(train_score, i, total_epochs, l, g)\n",
    "                    plot_ecdf_test(test_score, i, total_epochs, l, g)\n",
    "                    histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:])\n",
    "                    print(\"New score: \", np.average(new_outputs[-500:]))\n",
    "                    print(\"Train score: \", np.average(train_score[-500:]))\n",
    "                    print(\"Test score: \", np.average(test_score[-500:]))\n",
    "        \n",
    "            # Save after every epoch\n",
    "            total_epochs += 1\n",
    "            epoch += 1\n",
    "            if save_checkpoints is not None:\n",
    "                print(\"Saving to: \", save_checkpoints)\n",
    "                checkpoint_name = save_checkpoints\n",
    "                torch.save({'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optim.state_dict()}, checkpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "gamma = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "lamb = [10, 10, 10, 10]\n",
    "EPOCHS = [2, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/473047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2880/473047 [02:37<7:03:12, 18.52it/s]"
     ]
    }
   ],
   "source": [
    "model = ConvNetComplex()\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/ConvNetComplex/04212020.pth'\n",
    "\n",
    "sgd(epochs=EPOCHS, lamb=lamb, gamma=gamma, run_from_checkpoint=checkpoint, save_checkpoints=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointed_model = '../models/model_checkpoints/mle_model.pth'\n",
    "# checkpoint = torch.load(checkpointed_model)\n",
    "# model = Conv1dModelSimple()\n",
    "# optim = SGD(model.parameters(), lr=1e-3)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# model.to(device)\n",
    "# print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(len(S_prime_test)))\n",
    "# print(str(len(S_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is S_prime_test and S_test\n",
    "# validation_set = []\n",
    "# for (apt, pep), label in S_prime_test[:118262]:\n",
    "#     validation_set.append((apt, pep, label))\n",
    "\n",
    "# for (apt, pep) in S_test[:4000]:\n",
    "#     validation_set.append((apt, pep, 0))\n",
    "\n",
    "# np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# hydrophobicity_binding = []\n",
    "# hydrophobicity_free = []\n",
    "# arginine_binding = []\n",
    "# arginine_free = []\n",
    "\n",
    "# for (apt, pep, label) in validation_set:\n",
    "#     if 'Conv1' in model.name:\n",
    "#         conv_type='1d'\n",
    "#     else:\n",
    "#         conv_type='2d'\n",
    "#     x, y = convert(apt, pep, conv_type=conv_type)\n",
    "#     score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "#     hp = 0\n",
    "#     for aa in pep:\n",
    "#         hp += hydrophobicity[aa]\n",
    "    \n",
    "#     if score < 0.3:\n",
    "#         hydrophobicity_free.append(hp)\n",
    "#         arginine_free.append(pep.count('R'))\n",
    "#     elif score > 0.6:\n",
    "#         hydrophobicity_binding.append(hp)\n",
    "#         arginine_binding.append(pep.count('R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "# print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "# print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "# print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(hydrophobicity_binding, bins=10, label='Hydrophobicity of Binding Peptides')\n",
    "# plt.hist(hydrophobicity_free, bins=10 , label='Hydrophobicity of Non-Binding Peptides')\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.xlabel(\"Hydrophobicity Score\")\n",
    "# plt.title('Hydrophobicity of Test Set Outputs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arginine content\n",
    "# plt.hist(arginine_binding, bins=8, label='Number of arginines in binding peptides')\n",
    "# plt.hist(arginine_free, bins=8 , label='Number of arginines in non-binding peptides')\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.xlabel(\"Number of Arginines\")\n",
    "# plt.title('Arginine Count in Peptides')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
