{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "k = 10000 # number of samples used to calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                ds.append((aptamer, peptide))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform)\n",
    "def get_x():\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    return x\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK)\n",
    "def get_y():\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    return y\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\"):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(), get_y())\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset()\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]\n",
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd \n",
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv2d(1, 120, (4,4)) #similar to 4-gram\n",
    "        self.cnn_apt_2 = nn.Conv2d(120, 50, 1)\n",
    "        self.cnn_apt_3 = nn.Conv2d(50, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv2d(1, 50, (4,20))\n",
    "        self.cnn_pep_2 = nn.Conv2d(50, 10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(1900, 1)\n",
    "        self.name = \"SimpleConvNet\"\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NLPModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 1000, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(1000, 500, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(500, 100, 3)\n",
    "        self.cnn_apt_5 = nn.Conv1d(100, 10, 3)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 250, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(250, 500, 3)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 250, 3)\n",
    "        self.cnn_pep_4 = nn.Conv1d(250, 100, 3)\n",
    "        self.cnn_pep_5 = nn.Conv1d(100, 10, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"Conv1Model\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.maxpool, self.relu, self.cnn_apt_3, self.maxpool, self.relu, self.cnn_apt_4, self.maxpool, self.relu, self.cnn_apt_5, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.maxpool, self.relu, self.cnn_pep_3, self.maxpool, self.relu, self.cnn_pep_4, self.maxpool, self.relu, self.cnn_pep_5, self.relu)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(180, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dModelSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NLPModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 150, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(150, 100, 3)\n",
    "\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 3)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 3)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"Conv1ModelSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.maxpool, self.relu, self.cnn_apt_3, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.maxpool, self.relu, self.cnn_pep_3, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(180, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, conv_type='2d'): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    if conv_type == '2d':\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, 1, apt.shape[0], apt.shape[1]))).cuda() #(1, 1, 40, 4)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, 1, pep.shape[0], pep.shape[1]))).cuda() #(1, 1, 8, 20)\n",
    "    else:\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (1, apt.shape[0], apt.shape[1]))).cuda() #(1, 40, 4)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (1, pep.shape[0], pep.shape[1]))).cuda() #(1, 8, 20)\n",
    "    return apt, pep\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train', conv_type='2d'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep) in dset[:k]:\n",
    "        x, y = convert(apt, pep, conv_type=conv_type)\n",
    "        pairs.append((x, y))\n",
    "    return pairs\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train', conv_type='2d'): # S_prime_train/S_prime_test\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep), ind in dset[:k]:\n",
    "        pmf = get_y_pmf(pep)\n",
    "        x, y = convert(apt, pep, conv_type=conv_type)\n",
    "        pairs.append((x, y, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.cuda()\n",
    "        y = pep.cuda()\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, new_specificity, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.plot(new_specificity, 'r', label='New specificity')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall/specificity after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cdf(train_cdf, test_cdf, i, j, lamb, gamma):\n",
    "    plt.plot(train_cdf, 'b', label='Train CDF')\n",
    "    plt.plot(test_cdf, 'y', label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0,)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=axes[0, 0])\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=axes[1, 0])\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=axes[0, 1])\n",
    "    axes[0,0].set_title(\"New: not in dataset\")\n",
    "    axes[1,0].set_title(\"Train: in dataset\")\n",
    "    axes[0, 1].set_title(\"Test: in dataset\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model_name, lamb=10, #hyperparam\n",
    "        gamma=1e-3, #step size\n",
    "        save_checkpoints=False): #save checkpoints\n",
    "    \n",
    "    optim = SGD(model.parameters(), lr=gamma)\n",
    "    model.to(device)\n",
    "    epoch = 0\n",
    "    \n",
    "    if 'Conv1' in model_name:\n",
    "        conv_type = '1d'\n",
    "    while epoch < EPOCHS:\n",
    "        train_losses = []\n",
    "        train_recalls = []\n",
    "        train_recall_outputs = [] \n",
    "\n",
    "        test_losses = []\n",
    "        test_recalls = []\n",
    "        test_recall_outputs = []\n",
    "\n",
    "        new_outputs = []\n",
    "        new_specificity = []\n",
    "\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "        new_correct = 0\n",
    "        \n",
    "        for i, (aptamer, peptide, (apt_prime, pep_prime), indicator) in enumerate(tqdm.tqdm(train_ds)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            model.train()\n",
    "            optim.zero_grad() \n",
    "            x, y = convert(aptamer, peptide, conv_type=conv_type) #sample x,y from S_train\n",
    "            out = update(x, y) #get S_train output/score\n",
    "            log_out = torch.log(out) \n",
    "            \n",
    "            train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "            if train_score > 0.6:\n",
    "                train_correct += 1 \n",
    "            train_recall_outputs.append(train_score) \n",
    "\n",
    "            optim.zero_grad() \n",
    "            y_pmf = get_y_pmf(pep_prime)\n",
    "            x_prime, y_prime = convert(apt_prime, pep_prime, conv_type=conv_type) #sample x', y' from S_prime_train\n",
    "            out_prime = update(x_prime, y_prime) #get score from S_prime_train\n",
    "            if indicator == 0:\n",
    "                factor = (2*m*get_x_pmf()*y_pmf)/(1+m*get_x_pmf()*y_pmf)\n",
    "            else:\n",
    "                factor = 2\n",
    "            out_prime = out_prime*factor #adjust for IS\n",
    "            print(\"Obj first part: \", out_prime.cpu().detach().numpy().flatten()[0]*lamb*indicator)\n",
    "            print(\"Obj second part: \", log_out.cpu().detach().numpy().flatten()[0])\n",
    "            # Retain graph retains the graph for further operations\n",
    "            (lamb*indicator*out_prime - log_out).backward(retain_graph=True) \n",
    "            optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "            x_test, y_test = convert(S_test[i%(n-m)][0], S_test[i%(n-m)][1], conv_type=conv_type) #sample x,y from test set\n",
    "            test_score = model(x_test, y_test).cpu().detach().numpy().flatten()[0]\n",
    "            test_recall_outputs.append(test_score) \n",
    "            if test_score > 0.6:\n",
    "                test_correct += 1 \n",
    "\n",
    "            #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "            for x, y in S_new[10*i:10*(i+1)]:\n",
    "                x_new, y_new = convert(x, y, conv_type=conv_type) #generate unseen x'' and y'' from S_new\n",
    "                new_score = model(x_new, y_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                new_outputs.append(new_score)\n",
    "                if new_score < 0.3:\n",
    "                    new_correct += 1\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                train_loss = lamb*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                print(\"Train loss first part: \", lamb*get_out_prime(\"train\"))\n",
    "                print(\"Train loss second part: \", get_log_out('train'))\n",
    "                test_loss = (m/(n-m))*lamb*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                print(\"Test loss first part: \", lamb*get_out_prime(\"test\"))\n",
    "                print(\"Test loss second part: \", get_log_out('test'))\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                train_recall = 100*train_correct/i #training recall\n",
    "                train_recalls.append(train_recall) \n",
    "                test_recall = 100*test_correct/i #test recall\n",
    "                test_recalls.append(test_recall)\n",
    "                new_specificity = 100*new_correct/(i*10) #generated dataset specificity\n",
    "                new_specificity.append(new_specificity)\n",
    "                if i > 1000:\n",
    "                    train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                    test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                else:\n",
    "                    train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                    test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "                train_cdf = np.cumsum(train_score)/np.sum(train_score) #train cdf\n",
    "                test_cdf = np.cumsum(test_score)/np.sum(test_score) #test cdf\n",
    "\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                plot_recall(train_recalls, test_recalls, new_specificity, i, epoch, lamb, gamma)\n",
    "                plot_loss(train_losses, test_losses, i, epoch, lamb, gamma)\n",
    "                plot_cdf(train_cdf, test_cdf, i, epoch, lamb, gamma)\n",
    "                histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:])\n",
    "                print(\"New score: \", np.average(new_outputs[-1000:]))\n",
    "                print(\"Train score: \", np.average(train_score[-1000:]))\n",
    "                print(\"Test score: \", np.average(test_score[-1000:]))\n",
    "        # Save after every epoch\n",
    "        if save_checkpoints:\n",
    "            checkpoint_name = '../models/model_checkpoints/' + str(model_name) + '_lambda=' + str(lamb) + '_gamma=' + str(gamma) + '.pth'\n",
    "            torch.save({'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optim.state_dict()}, checkpoint_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "gammas = [1e-3]\n",
    "lambdas = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for g in gammas:\n",
    "    for l in lambdas:\n",
    "        print(\"Lambda: \" + str(l) + \" Gamma: \" + str(g))\n",
    "        model = Conv1dModelSimple()\n",
    "        # Generate Loss Samples\n",
    "        if 'Conv1' in model.name:\n",
    "            train_loss_samples = loss_samples(k, 'train', conv_type='1d')\n",
    "            test_loss_samples = loss_samples(k, 'test', conv_type='1d')\n",
    "            prime_train_loss_samples = prime_loss_samples(k, 'train', conv_type='1d')\n",
    "            prime_test_loss_samples = prime_loss_samples(k, 'test', conv_type='1d')\n",
    "        else:\n",
    "            train_loss_samples = loss_samples(k, 'train', conv_type='2d')\n",
    "            test_loss_samples = loss_samples(k, 'test', conv_type='2d')\n",
    "            prime_train_loss_samples = prime_loss_samples(k, 'train', conv_type='2d')\n",
    "            prime_test_loss_samples = prime_loss_samples(k, 'test', conv_type='2d')\n",
    "        \n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        sgd(model_name=model.name, gamma=g, lamb=l, save_checkpoints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointed_model = '../models/model_checkpoints/mle_model.pth'\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "model = Conv1dModelSimple()\n",
    "optim = SGD(model.parameters(), lr=1e-3)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.to(device)\n",
    "print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(S_prime_test)))\n",
    "print(str(len(S_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is S_prime_test and S_test\n",
    "validation_set = []\n",
    "for (apt, pep), label in S_prime_test[:118262]:\n",
    "    validation_set.append((apt, pep, label))\n",
    "\n",
    "for (apt, pep) in S_test[:4000]:\n",
    "    validation_set.append((apt, pep, 0))\n",
    "\n",
    "np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "hydrophobicity_binding = []\n",
    "hydrophobicity_free = []\n",
    "arginine_binding = []\n",
    "arginine_free = []\n",
    "\n",
    "for (apt, pep, label) in validation_set:\n",
    "    if 'Conv1' in model.name:\n",
    "        conv_type='1d'\n",
    "    else:\n",
    "        conv_type='2d'\n",
    "    x, y = convert(apt, pep, conv_type=conv_type)\n",
    "    score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "    hp = 0\n",
    "    for aa in pep:\n",
    "        hp += hydrophobicity[aa]\n",
    "    \n",
    "    if score < 0.3:\n",
    "        hydrophobicity_free.append(hp)\n",
    "        arginine_free.append(pep.count('R'))\n",
    "    elif score > 0.6:\n",
    "        hydrophobicity_binding.append(hp)\n",
    "        arginine_binding.append(pep.count('R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hydrophobicity_binding, bins=10, label='Hydrophobicity of Binding Peptides')\n",
    "plt.hist(hydrophobicity_free, bins=10 , label='Hydrophobicity of Non-Binding Peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Hydrophobicity Score\")\n",
    "plt.title('Hydrophobicity of Test Set Outputs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arginine content\n",
    "plt.hist(arginine_binding, bins=8, label='Number of arginines in binding peptides')\n",
    "plt.hist(arginine_free, bins=8 , label='Number of arginines in non-binding peptides')\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"Number of Arginines\")\n",
    "plt.title('Arginine Count in Peptides')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
