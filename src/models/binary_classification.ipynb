{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        def construct_binary_dataset(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    ds.append((aptamer, peptide, 1))\n",
    "                    ds.append((get_x(), get_y(), 0))\n",
    "            ds = list(set(ds)) #removed duplicates, random order\n",
    "            return ds\n",
    "\n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "\n",
    "        self.binary_ds=construct_binary_dataset(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.binary_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        def construct_generated_dataset(k):\n",
    "            S_new = []\n",
    "            for _, i in enumerate(tqdm.tqdm(range(k))):\n",
    "                pair = (get_x(), get_y())\n",
    "                S_new.append(pair)\n",
    "            np.random.shuffle(S_new)\n",
    "            return S_new\n",
    "        \n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "        self.gen_ds = construct_generated_dataset(n)\n",
    "    def __len__(self):\n",
    "        return len(self.gen_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        def construct_generated_dataset(k):\n",
    "            S_new = []\n",
    "            for _, i in enumerate(tqdm.tqdm(range(k))):\n",
    "                pair = (get_x(), get_y(), np.random.randint(2))\n",
    "                S_new.append(pair)\n",
    "            np.random.shuffle(S_new)\n",
    "            return S_new\n",
    "        \n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "        self.gen_ds = construct_generated_dataset(n)\n",
    "    def __len__(self):\n",
    "        return len(self.gen_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ds=BinaryDataset(filepath=\"../data/aptamer_dataset.json\")\n",
    "n = len(binary_ds)\n",
    "m = int(0.8*n) #length of train\n",
    "binary_train = binary_ds[:m]\n",
    "binary_val = binary_ds[m:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Random Control Experiment\n",
    "random_ds = RandomDataset(2*m)\n",
    "binary_train = random_ds[:m]\n",
    "binary_val = random_ds[m:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAlphabetLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleAlphabetLinearNet, self).__init__()\n",
    "        self.name = \"SingleAlphabetLinearNet\"\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1152, 1200) \n",
    "        self.fc_2 = nn.Linear(1200, 1700)\n",
    "        self.fc_3 = nn.Linear(1700, 2000)\n",
    "        self.fc_4 = nn.Linear(2000, 2200)\n",
    "        self.fc_5 = nn.Linear(2200, 2500)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fcs = nn.Sequential(self.fc_1, self.fc_2, self.fc_3, self.fc_4, self.fc_5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2500, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1)\n",
    "        \n",
    "    def forward(self, pair):\n",
    "        pair = pair.view(-1, 1).T\n",
    "\n",
    "        pair = self.fcs(pair)\n",
    "\n",
    "        x = self.fc2(self.fc1(pair))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1d, self).__init__()\n",
    "        self.name = \"LinearConv1d\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 10, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(10, 25, 3) \n",
    "        self.cnn_apt_3 = nn.Conv1d(25, 50, 3) \n",
    "        self.cnn_apt_4 = nn.Conv1d(50, 100, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(200, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        #print(apt.size())\n",
    "        #print(pep.size())\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSimple, self).__init__()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(100, 250, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 250, 1)\n",
    "       \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(500, 550)\n",
    "        self.fc2 = nn.Linear(550, 600)\n",
    "        self.fc3 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        \n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "\n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "    \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc3(self.fc2(self.fc1(x)))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetComplex, self).__init__()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(100, 200, 3)\n",
    "        self.cnn_apt_5 = nn.Conv1d(200, 400, 1)\n",
    "        self.cnn_apt_6 = nn.Conv1d(400, 800, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 200, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(200, 400, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(400, 800, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(1600, 1800)\n",
    "        self.fc2 = nn.Linear(1800, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [1, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.relu(self.cnn_apt_1(apt))\n",
    "        apt = self.relu(self.cnn_apt_2(apt))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_5(apt)))        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_6(apt)))        \n",
    "\n",
    "        # pep input size [1, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        pep = self.relu(self.cnn_pep_2(pep))\n",
    "        pep = self.relu(self.cnn_pep_3(pep))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_4(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_5(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAlphabetNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleAlphabetNet, self).__init__()\n",
    "        self.name = \"SingleAlphabetNet\"\n",
    "        \n",
    "        self.cnn_1 = nn.Conv1d(24, 25, 3) \n",
    "        self.cnn_2 = nn.Conv1d(25, 50, 3)\n",
    "        self.cnn_3 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_4 = nn.Conv1d(100, 200, 3)\n",
    "        self.cnn_5 = nn.Conv1d(200, 400, 1)\n",
    "        self.cnn_6 = nn.Conv1d(400, 800, 1)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(1600, 1800)\n",
    "        self.fc2 = nn.Linear(1800, 1)\n",
    "        \n",
    "    def forward(self, pair):\n",
    "        # pair input size [1, 48, 24]\n",
    "        pair = pair.permute(0, 2, 1)\n",
    "        \n",
    "        pair = self.relu(self.cnn_1(pair))\n",
    "        pair = self.relu(self.cnn_2(pair))\n",
    "        pair = self.pool1(self.relu(self.cnn_3(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_4(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_5(pair)))        \n",
    "        pair = self.pool1(self.relu(self.cnn_6(pair)))        \n",
    "\n",
    "        pair = pair.view(-1, 1).T\n",
    "        \n",
    "        pair = self.fc2(self.fc1(pair))\n",
    "        x = torch.sigmoid(pair)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAlphabetComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleAlphabetComplexNet, self).__init__()\n",
    "        self.name = \"SingleAlphabetComplexNet\"\n",
    "        \n",
    "        self.cnn_1 = nn.Conv1d(24, 50, 3) \n",
    "        self.cnn_2 = nn.Conv1d(50, 100, 3)\n",
    "        self.cnn_3 = nn.Conv1d(100, 200, 3)\n",
    "        self.cnn_4 = nn.Conv1d(200, 400, 3)\n",
    "        self.cnn_5 = nn.Conv1d(400, 800, 3)\n",
    "        self.cnn_6 = nn.Conv1d(800, 1000, 3, padding=2)\n",
    "        self.cnn_7 = nn.Conv1d(1000, 800, 3, padding=2)\n",
    "        self.cnn_8 = nn.Conv1d(800, 700, 3, padding=2)\n",
    "        self.cnn_9 = nn.Conv1d(700, 500, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(500, 1800)\n",
    "        self.fc2 = nn.Linear(1800, 1)\n",
    "        \n",
    "    def forward(self, pair):\n",
    "        # pair input size [1, 48, 24]\n",
    "        pair = pair.permute(0, 2, 1)\n",
    "        \n",
    "        pair = self.relu(self.cnn_1(pair))\n",
    "        pair = self.relu(self.cnn_2(pair))\n",
    "        pair = self.pool1(self.relu(self.cnn_3(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_4(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_5(pair)))        \n",
    "        pair = self.pool1(self.relu(self.cnn_6(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_7(pair))) \n",
    "        pair = self.pool1(self.relu(self.cnn_8(pair)))\n",
    "        pair = self.pool1(self.relu(self.cnn_9(pair))) \n",
    "\n",
    "        pair = pair.view(-1, 1).T\n",
    "        \n",
    "        pair = self.fc2(self.fc1(pair))\n",
    "        x = torch.sigmoid(pair)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexLinearNet, self).__init__()\n",
    "        self.name = \"ComplexLinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        self.fc_apt_4 = nn.Linear(300, 400)\n",
    "        self.fc_apt_5 = nn.Linear(400, 500)\n",
    "        self.fc_apt_6 = nn.Linear(500, 450)\n",
    "        self.fc_apt_7 = nn.Linear(450, 400)\n",
    "        self.fc_apt_8 = nn.Linear(400, 350)\n",
    "        self.fc_apt_9 = nn.Linear(350, 300)\n",
    "        self.fc_apt_10 = nn.Linear(300, 250)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        self.fc_pep_3 = nn.Linear(250, 200)\n",
    "        self.fc_pep_4 = nn.Linear(200, 150)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3,\n",
    "                                    self.fc_apt_4, self.fc_apt_5, self.fc_apt_6,\n",
    "                                    self.fc_apt_7, self.fc_apt_8,\n",
    "                                    self.fc_apt_9, self.fc_apt_10)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2, self.fc_pep_3, self.fc_pep_4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(400, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = one_hot([apt, pep], single_alphabet=True)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "        pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[0], apt.shape[1]))).to(device) #(1, 40, 4)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[0], pep.shape[1]))).to(device) #(1, 8, 20)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=True\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out\n",
    "\n",
    "## Plotting functions\n",
    "def plot_loss(iters, train_losses, val_losses, model_name, model_id):\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/loss.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(iters, train_acc, val_acc, model_name, model_id):\n",
    "    plt.title(\"Training Accuracy Curve\")\n",
    "    plt.plot(train_acc, label=\"Train\")\n",
    "    plt.plot(val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/accuracy.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    \n",
    "    sns.distplot(train_gen_scores , color=\"skyblue\", label='Generated Train Samples', ax=ax)\n",
    "    sns.distplot(val_gen_scores, color='dodgerblue', label='Generated Validation Samples')\n",
    "    sns.distplot(train_scores , color=\"lightcoral\", label='Dataset Train Samples', ax=ax)\n",
    "    sns.distplot(val_scores, color='red', label='Dataset Validation Samples', ax=ax)\n",
    "    \n",
    "    ax.set_title(\"Categorizing the output scores of the model\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/histogram.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, iters, epoch, gamma, model_name, model_id):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Test CDF at epoch %d' %epoch + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/test_cdf.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, iters, epoch, gamma, model_name, model_id):\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" % iters)\n",
    "    plt.title('Train CDF at epoch %d' %epoch+ \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def auc_cdf(train, new, model_name, model_id, val=False): \n",
    "    #train is the sorted list of outputs from the model with training pairs as inputs\n",
    "    #new is the list of outputs from the model with generated pairs as inputs\n",
    "    a = train + new\n",
    "    n = len(a)\n",
    "    m = len(train)\n",
    "    train = np.asarray(train)\n",
    "    new = np.asarray(new)\n",
    "    y = np.arange(0, m+2)/m\n",
    "    gamma = [0]\n",
    "    for x in train:\n",
    "        gamma.append(sum(a<=x)/n)\n",
    "    gamma.append(1)\n",
    "    plt.plot(gamma, y)\n",
    "    if val:\n",
    "        plt.title(\"Validation CDF\")\n",
    "    else:\n",
    "        plt.title(\"Train CDF\")\n",
    "    plt.xlim([0,1])\n",
    "    if val:\n",
    "        plt.savefig('plots/binary/%s/%s/val_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig('plots/binary/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return np.trapz(y, gamma)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test the regular one_hot method\n",
    "oh = one_hot('LL', seq_type='peptide')\n",
    "print(str(oh.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test the one_hot method\n",
    "oh = one_hot([\"GGGG\", \"LL\"], single_alphabet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, \n",
    "               train, \n",
    "               val,\n",
    "               lr,\n",
    "               model_id,\n",
    "               num_epochs=50,\n",
    "               batch_size=16,\n",
    "               single_alphabet=False,\n",
    "               run_from_checkpoint=None, \n",
    "               save_checkpoints=None, \n",
    "               cdf=False):\n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        init_epoch = checkpoint['epoch'] + 1\n",
    "        print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "        init_epoch = 0\n",
    "    \n",
    "    train_losses, val_losses, train_losses_avg, val_losses_avg, train_acc, val_acc = [], [], [], [], [], []\n",
    "    iters, train_correct, val_correct = 0, 0, 0\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=0.9, last_epoch=-1) #Decays lr by gamma factor every step_size epochs. \n",
    "    \n",
    "    # Keep track of the scores across four classes\n",
    "    train_scores, train_gen_scores, val_scores, val_gen_scores = [], [], [], []\n",
    "    # Used for the CDF (generated pair outputs)\n",
    "    gen_outputs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        full_epoch = epoch + init_epoch\n",
    "        print(\"Starting epoch: %d\" % full_epoch, \" with learning rate: \", scheduler.get_lr())\n",
    "        for i, (apt, pep, label) in enumerate(tqdm.tqdm(train)):\n",
    "            model_name = model.name\n",
    "            model.train()\n",
    "            if single_alphabet:\n",
    "                p, l = convert(apt, pep, label, single_alphabet=True)\n",
    "                train_score = update(None, None, p, single_alphabet=True)\n",
    "            else:\n",
    "                a, p, l = convert(apt, pep, label, single_alphabet=False)\n",
    "                train_score = update(a, p, None, single_alphabet=False)\n",
    "                \n",
    "            if (train_score.item() >= 0.5 and label == 1.0) or (train_score.item() <= 0.5 and label == 0.0):\n",
    "                train_correct += 1\n",
    "            \n",
    "            if label == 0.0:\n",
    "                train_gen_scores.append(train_score.item())\n",
    "            elif label == 1.0:\n",
    "                train_scores.append(train_score.item())\n",
    "                \n",
    "            iters += 1\n",
    "            train_loss = criterion(train_score, l) \n",
    "            total_train_loss += train_loss\n",
    "            \n",
    "            if iters % batch_size == 0:\n",
    "                ave_train_loss = total_train_loss/batch_size\n",
    "                train_losses.append(ave_train_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                ave_train_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if iters % 5000 == 0:\n",
    "                train_acc.append(100*train_correct/iters)\n",
    "                train_losses_avg.append(np.average(train_losses[-5000:]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "            if single_alphabet:\n",
    "                p_val, l_val = convert(val[iters%(n-m)][0], val[iters%(n-m)][1], val[iters%(n-m)][2], single_alphabet=True)\n",
    "                val_score = model(p_val)\n",
    "            else:\n",
    "                a_val, p_val, l_val = convert(val[iters%(n-m)][0], val[iters%(n-m)][1], val[iters%(n-m)][2])\n",
    "                val_score = model(a_val, p_val)\n",
    "            if (val_score.item() >= 0.5 and val[iters%(n-m)][2] == 1.0) or (val_score.item() <= 0.5 and val[iters%(n-m)][2] == 0.0):\n",
    "                val_correct += 1\n",
    "            \n",
    "            if l_val.item() == 1.0:\n",
    "                val_scores.append(val_score.item())\n",
    "            if l_val.item() == 0.0:\n",
    "                val_gen_scores.append(val_score.item())\n",
    "            \n",
    "            if cdf:\n",
    "                #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "                for x, y in S_new[10*i:10*(i+1)]:\n",
    "                    if single_alphabet:\n",
    "                        p_val, l_val = convert(x, y, 0, single_alphabet=True)\n",
    "                        gen_score = model(p_val)\n",
    "                    else:\n",
    "                        a_val, p_val, l_val = convert(x, y, 0)\n",
    "                        gen_score = model(a_val, p_val)\n",
    "                    gen_outputs.append(gen_score.item())\n",
    "\n",
    "                # Generate CDF plots\n",
    "                if len(train_scores) > 1000:\n",
    "                    train_cdf = np.asarray(gen_outputs[-10000:] + train_scores[-1000:]) \n",
    "                    test_cdf = np.asarray(gen_outputs[-10000:] + val_scores[-1000:])\n",
    "                else:\n",
    "                    train_cdf = np.asarray(gen_outputs + train_scores) #combine train and unknown scores\n",
    "                    test_cdf = np.asarray(gen_outputs + val_scores)\n",
    "            \n",
    "            val_loss = criterion(val_score, l_val) \n",
    "            total_val_loss += val_loss\n",
    "            if iters % batch_size == 0:\n",
    "                ave_val_loss = total_val_loss/batch_size\n",
    "                val_losses.append(ave_val_loss.item())\n",
    "                total_val_loss = 0\n",
    "            \n",
    "            if iters % 5000 == 0:\n",
    "                val_acc.append(100*val_correct/iters)\n",
    "                val_losses_avg.append(np.average(val_losses[-5000:]))\n",
    "\n",
    "            if iters % 50000 == 0:\n",
    "                plot_loss(iters, train_losses_avg, val_losses_avg, model_name, model_id)\n",
    "                plot_accuracy(iters, train_acc, val_acc, model_name, model_id)\n",
    "                plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id)\n",
    "                val_auc = auc_cdf(sorted(val_scores[-1000:]), sorted(val_gen_scores[-10000:]), model_name, model_id, val=True)\n",
    "                train_auc = auc_cdf(sorted(train_scores[-1000:]), sorted(train_gen_scores[-10000:]), model_name, model_id)\n",
    "                print(\"Training AUC at epoch %d: {}\".format(train_auc) % full_epoch)\n",
    "                print(\"Validation AUC epoch %d: {}\".format(val_auc) % full_epoch)\n",
    "    \n",
    "                if cdf:\n",
    "                    plot_ecdf_train(train_cdf, iters, full_epoch, lr, model_name, model_id)\n",
    "                    plot_ecdf_test(test_cdf, iters, full_epoch, lr, model_name, model_id)\n",
    "                \n",
    "                print(\"Training Accuracy at epoch %d: {}\".format(train_acc[-1]) % full_epoch)\n",
    "                print(\"Validation Accuracy epoch %d: {}\".format(val_acc[-1]) % full_epoch)\n",
    "        scheduler.step()\n",
    "        if save_checkpoints is not None:\n",
    "            print(\"Saving to: \", save_checkpoints)\n",
    "            checkpoint_name = save_checkpoints\n",
    "            torch.save({'epoch': full_epoch,\n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict()}, checkpoint_name)\n",
    "        \n",
    "        # Clear unused gpu memory at the end of the epoch\n",
    "        if device == torch.cuda:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleAlphabetLinearNet()\n",
    "model_name = model.name\n",
    "model_id = \"07102020\"\n",
    "model.to(device)\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "single_alphabet = True\n",
    "cdf=False\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "gamma = 1e-2\n",
    "classifier(model, binary_train, binary_val, gamma, model_id, NUM_EPOCHS, BATCH_SIZE, single_alphabet, checkpoint, save_path, cdf=cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptamers",
   "language": "python",
   "name": "aptamers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
