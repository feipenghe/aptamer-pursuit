{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "FILE_PATH = '/media/scratch/yuhaowan/data/'\n",
    "NGRAM = False\n",
    "m = 473047 #size of training set\n",
    "n = 591309 #size of full dataset\n",
    "if NGRAM:\n",
    "    S_train = open(FILE_PATH + 's_tr_n.pkl', 'rb') # (apt, pep, apt_prime, pep_prime, pep_prime_pmf, indicator)\n",
    "    S_test = open(FILE_PATH + 's_te_n.pkl', 'rb') # (apt, pep)\n",
    "    S_new = open(FILE_PATH + 's_new_n.pkl', 'rb') # (apt, pep)\n",
    "    train_loss_samples = open(FILE_PATH + 'tr_loss_n.pkl', 'rb') # (apt, pep)\n",
    "    test_loss_samples = open(FILE_PATH + 'te_loss_n.pkl', 'rb') # (apt, pep)\n",
    "    prime_train_loss_samples = open(FILE_PATH + 'ptr_loss_n.pkl', 'rb') # (apt, pep, pep_pmf, indicator)\n",
    "    prime_test_loss_samples = open(FILE_PATH + 'pte_loss_n.pkl', 'rb') # (apt, pep, pep_pmf, indicator)\n",
    "else:\n",
    "    S_train = open(FILE_PATH + 's_tr.pkl', 'rb')\n",
    "    S_test = open(FILE_PATH + 's_te.pkl', 'rb')\n",
    "    S_new = open(FILE_PATH + 's_new.pkl', 'rb')\n",
    "    train_loss_samples = open(FILE_PATH + 'tr_loss.pkl', 'rb')\n",
    "    test_loss_samples = open(FILE_PATH + 'te_loss.pkl', 'rb')\n",
    "    prime_train_loss_samples = open(FILE_PATH + 'ptr_loss.pkl', 'rb')\n",
    "    prime_test_loss_samples = open(FILE_PATH + 'pte_loss.pkl', 'rb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModelSimple, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(25, 10, 1)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(275, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 3) \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"LinearConv1d\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrueLinearNet, self).__init__()\n",
    "        self.lin_apt_1 = nn.Linear(160, 100) \n",
    "        self.lin_apt_2 = nn.Linear(100, 50)\n",
    "        self.lin_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.lin_pep_1 = nn.Linear(160, 50)\n",
    "        self.lin_pep_2 = nn.Linear(50, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.lin_apt = nn.Sequential(self.lin_apt_1, self.lin_apt_2, self.lin_apt_3)\n",
    "        self.lin_pep = nn.Sequential(self.lin_pep_1, self.lin_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.lin_apt(apt)\n",
    "        pep = self.lin_pep(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is too complex for our input sequence size\n",
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1dModel, self).__init__()\n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 300, 1)\n",
    "        self.cnn_apt_3 = nn.Conv1d(300, 150, 1)\n",
    "        self.cnn_apt_4 = nn.Conv1d(150, 75, 1)\n",
    "        self.cnn_apt_5 = nn.Conv1d(25, 10, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 250, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(250, 500, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(500, 250, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(250, 100, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(100, 10, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, self.cnn_apt_2, self.maxpool, self.relu, self.cnn_apt_3, self.maxpool, self.relu, self.cnn_apt_4, self.maxpool, self.relu, self.cnn_apt_5, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu, self.cnn_pep_2, self.maxpool, self.relu, self.cnn_pep_3, self.maxpool, self.relu, self.cnn_pep_4, self.maxpool, self.relu, self.cnn_pep_5, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(180, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        apt = self.relu(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        pep = self.relu(pep)\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for _ in range(10000):\n",
    "        apt, pep, _ = pickle.load()\n",
    "        out = update(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples \n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for _ in range(10000):\n",
    "        apt, pep, pmf, ind = pickle.load(dset)\n",
    "        x = apt.to(device)\n",
    "        y = pep.to(device)\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "## Plotting functions\n",
    "\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, new_recall, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.plot(new_recall, 'r', label='New recall')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, i, j, lamb, gamma):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, i, j, lamb, gamma):\n",
    "    #train_score consisits of [10000 scores generated] + [1000 scores from training set]\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.xlabel(\"CDF\")\n",
    "    plt.ylabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=ax)\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=ax)\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=ax)\n",
    "    ax.set_title(\"Distribution of Scores\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lamb = hyperparameter\n",
    "gamma = step size\n",
    "run_from_checkpoint = path to a checkpointed model\n",
    "save_checkpoings = file name\n",
    "'''\n",
    "def sgd(epochs=[1, 2, 3], \n",
    "        lamb=[10, 10, 10], \n",
    "        gamma=[1e-3, 1e-4, 1e-5], \n",
    "        run_from_checkpoint=None, \n",
    "        save_checkpoints=None): \n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        \n",
    "        optim = SGD(model.parameters(), lr=gamma[0])\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        reloaded_epoch = checkpoint['epoch']\n",
    "        print(\"Reloading model: \" + str(model.name) + \" at epoch: \" + str(reloaded_epoch))\n",
    "        epoch = reloaded_epoch\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    total_epochs = 0\n",
    "    for i in range(len(epochs)):\n",
    "        g = gamma[i]\n",
    "        l = lamb[i]\n",
    "        eps = epochs[i]\n",
    "        epoch = 0\n",
    "        optim = SGD(model.parameters(), lr=g)\n",
    "        while epoch < eps:\n",
    "            train_recalls = []\n",
    "            train_recall_outputs = [] \n",
    "            test_recalls = []\n",
    "            test_recall_outputs = []\n",
    "            new_outputs = []\n",
    "            new_recalls = []\n",
    "            train_correct = 0\n",
    "            test_correct = 0\n",
    "            new_correct = 0\n",
    "            print(\"Training Epoch: \", total_epochs)\n",
    "            for i in range(1, m+1)\n",
    "                apt, pep, apt_prime, pep_prime, pep_prime_pmf, indicator = pickle.load(S_train)\n",
    "                print(apt, pep, apt_prime, pep_prime, pep_prime_pmf, indicator)\n",
    "                model.train()\n",
    "                optim.zero_grad() \n",
    "                out = update(apt, pep) #get S_train output/score\n",
    "                log_out = torch.log(out) \n",
    "\n",
    "                train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "                if train_score > 0.6:\n",
    "                    train_correct += 1 \n",
    "                train_recall_outputs.append(train_score) \n",
    "\n",
    "                optim.zero_grad() \n",
    "                out_prime = update(apt_prime, pep_prime) #get score from S_prime_train\n",
    "                if indicator == 0:\n",
    "                    factor = (2*m*get_x_pmf()*pep_prime_pmf)/(1+m*get_x_pmf()*pep_prime_pmf)\n",
    "                else:\n",
    "                    factor = 2\n",
    "                out_prime = out_prime*factor #adjust for IS\n",
    "                #print(\"Obj first part: \", out_prime.cpu().detach().numpy().flatten()[0]*lamb*indicator)\n",
    "                #print(\"Obj second part: \", log_out.cpu().detach().numpy().flatten()[0])\n",
    "                # Retain graph retains the graph for further operations\n",
    "                (l*indicator*out_prime - log_out).backward(retain_graph=True) \n",
    "                optim.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                try:\n",
    "                    apt_test, pep_test = pickle.load(S_test)\n",
    "                    print(\"apt_test, pep_test\", apt_test, pep_test)\n",
    "                except EOFError:\n",
    "                    S_test.close()\n",
    "                    S_test = open(FILE_PATH + 's_te.pkl', 'rb')\n",
    "                    apt_test, pep_test = pickle.load(S_test)\n",
    "                \n",
    "                test_score = model(apt_test, pep_test).cpu().detach().numpy().flatten()[0]\n",
    "                test_recall_outputs.append(test_score) \n",
    "                if test_score > 0.6:\n",
    "                    test_correct += 1 \n",
    "\n",
    "                #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "                for _ in range(10):\n",
    "                    apt_new, pep_new = pickle.load(S_new)\n",
    "                    new_score = model(apt_new, pep_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                    new_outputs.append(new_score)\n",
    "                    if new_score < 0.3:\n",
    "                        new_correct += 1\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    train_loss = l*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                    #print(\"Train loss first part: \", lamb*get_out_prime(\"train\"))\n",
    "                    #print(\"Train loss second part: \", get_log_out('train'))\n",
    "                    test_loss = (m/(n-m))*l*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                    #print(\"Test loss first part: \", lamb*get_out_prime(\"test\"))\n",
    "                    #print(\"Test loss second part: \", get_log_out('test'))\n",
    "                    train_losses.append(train_loss)\n",
    "                    test_losses.append(test_loss)\n",
    "\n",
    "                    train_recall = 100*train_correct/(total_epochs*m + i) #training recall\n",
    "                    train_recalls.append(train_recall) \n",
    "                    test_recall = 100*test_correct/(total_epochs*m + i) #test recall\n",
    "                    test_recalls.append(test_recall)\n",
    "                    new_recall = 100*new_correct/(i*10) #generated dataset recall\n",
    "                    new_recalls.append(new_recall)\n",
    "                    if i > 1000:\n",
    "                        train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                        test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                    else:\n",
    "                        train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                        test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "\n",
    "\n",
    "                if i % 200 == 0:\n",
    "                    plot_recall(train_recalls, test_recalls, new_recalls, i, total_epochs, l, g)\n",
    "                    plot_loss(train_losses, test_losses, i, total_epochs, l, g)\n",
    "                    plot_ecdf_train(train_score, i, total_epochs, l, g)\n",
    "                    plot_ecdf_test(test_score, i, total_epochs, l, g)\n",
    "                    histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:])\n",
    "                    print(\"New score: \", np.average(new_outputs[-100:]))\n",
    "                    print(\"Train score: \", np.average(train_score[-100:]))\n",
    "                    print(\"Test score: \", np.average(test_score[-100:]))\n",
    "        \n",
    "            # Save after every epoch\n",
    "            total_epochs += 1\n",
    "            epoch += 1\n",
    "            if save_checkpoints is not None:\n",
    "                print(\"Saving to: \", save_checkpoints)\n",
    "                checkpoint_name = save_checkpoints\n",
    "                torch.save({'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optim.state_dict()}, checkpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "gamma = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "lamb = [10, 10, 10, 10]\n",
    "EPOCHS = [2, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNetComplex()\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/ConvNetComplex/04202020.pth'\n",
    "\n",
    "sgd(epochs=EPOCHS, lamb=lamb, gamma=gamma, run_from_checkpoint=checkpoint, save_checkpoints=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance of learned motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointed_model = '../models/model_checkpoints/mle_model.pth'\n",
    "# checkpoint = torch.load(checkpointed_model)\n",
    "# model = Conv1dModelSimple()\n",
    "# optim = SGD(model.parameters(), lr=1e-3)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# model.to(device)\n",
    "# print(str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(len(S_prime_test)))\n",
    "# print(str(len(S_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set is S_prime_test and S_test\n",
    "# validation_set = []\n",
    "# for (apt, pep), label in S_prime_test[:118262]:\n",
    "#     validation_set.append((apt, pep, label))\n",
    "\n",
    "# for (apt, pep) in S_test[:4000]:\n",
    "#     validation_set.append((apt, pep, 0))\n",
    "\n",
    "# np.random.shuffle(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# hydrophobicity_binding = []\n",
    "# hydrophobicity_free = []\n",
    "# arginine_binding = []\n",
    "# arginine_free = []\n",
    "\n",
    "# for (apt, pep, label) in validation_set:\n",
    "#     if 'Conv1' in model.name:\n",
    "#         conv_type='1d'\n",
    "#     else:\n",
    "#         conv_type='2d'\n",
    "#     x, y = convert(apt, pep, conv_type=conv_type)\n",
    "#     score = model(x, y).cpu().detach().numpy().flatten()[0]\n",
    "#     hp = 0\n",
    "#     for aa in pep:\n",
    "#         hp += hydrophobicity[aa]\n",
    "    \n",
    "#     if score < 0.3:\n",
    "#         hydrophobicity_free.append(hp)\n",
    "#         arginine_free.append(pep.count('R'))\n",
    "#     elif score > 0.6:\n",
    "#         hydrophobicity_binding.append(hp)\n",
    "#         arginine_binding.append(pep.count('R'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average Hydrophobicity of binding peptides: \", np.mean(np.asarray(hydrophobicity_binding)))\n",
    "# print(\"Average Hydrophobicity of non-binding peptides: \", np.mean(np.asarray(hydrophobicity_free)))\n",
    "# print(\"Average Number of Arginines in binding peptides: \", np.mean(np.asarray(arginine_binding)))\n",
    "# print(\"Average Number of Arginines in non-binding peptides: \", np.mean(np.asarray(arginine_free)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(hydrophobicity_binding, bins=10, label='Hydrophobicity of Binding Peptides')\n",
    "# plt.hist(hydrophobicity_free, bins=10 , label='Hydrophobicity of Non-Binding Peptides')\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.xlabel(\"Hydrophobicity Score\")\n",
    "# plt.title('Hydrophobicity of Test Set Outputs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arginine content\n",
    "# plt.hist(arginine_binding, bins=8, label='Number of arginines in binding peptides')\n",
    "# plt.hist(arginine_free, bins=8 , label='Number of arginines in non-binding peptides')\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.xlabel(\"Number of Arginines\")\n",
    "# plt.title('Arginine Count in Peptides')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
