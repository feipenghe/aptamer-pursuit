{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding strategies\n",
    "* This file implements different encoding strategies like direct translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        def construct_binary_dataset(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    ds.append((aptamer, peptide, 1))\n",
    "                    ds.append((get_x(), get_y(), 0))\n",
    "            ds = list(set(ds)) #removed duplicates, random order\n",
    "            return ds\n",
    "\n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "\n",
    "        self.binary_ds=construct_binary_dataset(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.binary_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        def construct_generated_dataset(k):\n",
    "            S_new = []\n",
    "            for _, i in enumerate(tqdm.tqdm(range(k))):\n",
    "                pair = (get_x(), get_y())\n",
    "                S_new.append(pair)\n",
    "            np.random.shuffle(S_new)\n",
    "            return S_new\n",
    "        \n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "        self.gen_ds = construct_generated_dataset(n)\n",
    "    def __len__(self):\n",
    "        return len(self.gen_ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.gen_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ds=BinaryDataset(filepath=\"../data/aptamer_dataset.json\")\n",
    "n = len(binary_ds)\n",
    "m = int(0.8*n) #length of train\n",
    "binary_train = binary_ds[:m]\n",
    "binary_val = binary_ds[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7476678/9460940 [11:28<03:02, 10863.10it/s]"
     ]
    }
   ],
   "source": [
    "# For the CDF functions, we need to generate a dataset of new examples\n",
    "S_new = GeneratedDataset(10*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranslateNet, self).__init__()\n",
    "        self.name = \"TranslateNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 25, 2, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 15, 2, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(15, 10, 2, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(10, 5, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 15, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(15, 5, 1, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(15, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.permute(1, 2, 0)\n",
    "        pep = pep.permute(1, 2, 0)\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TranslateComplexNet, self).__init__()\n",
    "        self.name = \"TranslateComplexNet\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(40, 100, 2, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 250, 2, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(250, 500, 2, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(500, 250, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(8, 50, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 100, 2, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(100, 200, 2, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(200, 150, 2, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_4, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.permute(1, 2, 0)\n",
    "        pep = pep.permute(1, 2, 0)\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def translate(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        \n",
    "        encoding = np.zeros(len(apt) + len(pep))\n",
    "        \n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            idx = na_list.index(char)\n",
    "            encoding[i] = idx\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            idx = aa_list.index(char)\n",
    "            encoding[i+len(apt)] = idx\n",
    "        return encoding     \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        \n",
    "        encoding = np.zeros(len(sequence))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            idx = letters.index(char)\n",
    "            encoding[i] = idx\n",
    "        return encoding\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        apt = translate(apt, seq_type='aptamer') #(40, )\n",
    "        pep = translate(pep, seq_type='peptide') #(8, )\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, 1, apt.shape[0]))).to(device) #(1, 1, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, 1, pep.shape[0]))).to(device) #(1, 1, 8)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=True\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out\n",
    "\n",
    "## Plotting functions\n",
    "def plot_loss(iters, train_losses, val_losses, model_name, model_id):\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/loss.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(iters, train_acc, val_acc, model_name, model_id):\n",
    "    plt.title(\"Training Accuracy Curve\")\n",
    "    plt.plot(train_acc, label=\"Train\")\n",
    "    plt.plot(val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/%s/accuracy.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim(0, 1.1)\n",
    "    \n",
    "    sns.distplot(train_gen_scores , color=\"skyblue\", label='Generated Train Samples', ax=ax)\n",
    "    sns.distplot(val_gen_scores, color='dodgerblue', label='Generated Validation Samples')\n",
    "    sns.distplot(train_scores , color=\"lightcoral\", label='Dataset Train Samples', ax=ax)\n",
    "    sns.distplot(val_scores, color='red', label='Dataset Validation Samples', ax=ax)\n",
    "    \n",
    "    ax.set_title(\"Categorizing the output scores of the model\")\n",
    "    ax.figure.set_size_inches(7, 4)\n",
    "    ax.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/histogram.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_test(test_score, iters, epoch, gamma, model_name, model_id):\n",
    "    test_idx = np.argsort(test_score)\n",
    "    test_id = test_idx >= 10000\n",
    "    test = np.sort(test_score)\n",
    "    test_c = \"\"\n",
    "    for m in test_id:\n",
    "        if m:\n",
    "            test_c += \"y\"\n",
    "        else:\n",
    "            test_c += \"g\"\n",
    "    n = test_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, test, c=test_c, label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" %iters)\n",
    "    plt.title('Test CDF at epoch %d' %epoch + \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/test_cdf.png' %(model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_ecdf_train(train_score, iters, epoch, gamma, model_name, model_id):\n",
    "    train_idx = np.argsort(train_score)\n",
    "    train_id = train_idx >= 10000\n",
    "    train = np.sort(train_score)\n",
    "    train_c = \"\" #colors\n",
    "    for l in train_id:\n",
    "        if l:\n",
    "            train_c += \"r\"\n",
    "        else:\n",
    "            train_c += \"b\"\n",
    "    n = train_score.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    plt.scatter(y, train, c=train_c, label='Train CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples after training %d samples\" % iters)\n",
    "    plt.title('Train CDF at epoch %d' %epoch+ \", Gamma:%.5f\" %gamma)\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/binary/%s/%s/train_cdf.png' % (model_name, model_id), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the regular translate method\n",
    "oh = translate('LL', seq_type='peptide')\n",
    "oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the translate method with single alphabet\n",
    "oh = translate([\"GGGG\", \"LL\"], single_alphabet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.cuda:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, \n",
    "               train, \n",
    "               val,\n",
    "               lr,\n",
    "               model_id,\n",
    "               num_epochs=50,\n",
    "               batch_size=16,\n",
    "               single_alphabet=False,\n",
    "               run_from_checkpoint=None, \n",
    "               save_checkpoints=None):\n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        optimizer = SGD(model.parameters(), lr=5e-3)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        init_epoch = checkpoint['epoch'] +1\n",
    "        print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "        init_epoch = 0\n",
    "    \n",
    "    train_losses, val_losses, train_losses_avg, val_losses_avg, train_acc, val_acc = [], [], [], [], [], []\n",
    "    \n",
    "    iters, train_correct, val_correct = 0, 0, 0\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=0.9) #Decays lr by gamma factor every step_size epochs. \n",
    "    \n",
    "    # Keep track of the scores across four classes\n",
    "    train_scores, train_gen_scores, val_scores, val_gen_scores = [], [], [], []\n",
    "    # Used for the CDF (generated pair outputs)\n",
    "    gen_outputs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        full_epoch = epoch + init_epoch\n",
    "        scheduler.step()\n",
    "        print(\"Starting epoch: %d\" % full_epoch, \" with learning rate: \", scheduler.get_lr())\n",
    "        for i, (apt, pep, label) in enumerate(train):\n",
    "            model_name = model.name\n",
    "            model.train()\n",
    "            if single_alphabet:\n",
    "                p, l = convert(apt, pep, label, single_alphabet=True)\n",
    "                train_score = update(None, None, p, single_alphabet=True)\n",
    "            else:\n",
    "                a, p, l = convert(apt, pep, label, single_alphabet=False)\n",
    "                train_score = update(a, p, None, single_alphabet=False)\n",
    "                \n",
    "            if (train_score.item() >= 0.5 and label == 1.0) or (train_score.item() <= 0.5 and label == 0.0):\n",
    "                train_correct += 1\n",
    "            \n",
    "            if label == 0.0:\n",
    "                train_gen_scores.append(train_score.item())\n",
    "            elif label == 1.0:\n",
    "                train_scores.append(train_score.item())\n",
    "                \n",
    "            iters += 1\n",
    "            train_loss = criterion(train_score, l) \n",
    "            total_train_loss += train_loss\n",
    "            \n",
    "            if iters % batch_size == 0:\n",
    "                ave_train_loss = total_train_loss/batch_size\n",
    "                train_losses.append(ave_train_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                ave_train_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss = 0\n",
    "\n",
    "            if iters % 5000 == 0:\n",
    "                train_acc.append(100*train_correct/iters)\n",
    "                train_losses_avg.append(np.average(train_losses[-5000:]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "            if single_alphabet:\n",
    "                p_val, l_val = convert(val[iters%(n-m)][0], val[iters%(n-m)][1], val[iters%(n-m)][2], single_alphabet=True)\n",
    "                val_score = model(p_val)\n",
    "            else:\n",
    "                a_val, p_val, l_val = convert(val[iters%(n-m)][0], val[iters%(n-m)][1], val[iters%(n-m)][2])\n",
    "                val_score = model(a_val, p_val)\n",
    "            if (val_score.item() >= 0.5 and val[iters%(n-m)][2] == 1.0) or (val_score.item() <= 0.5 and val[iters%(n-m)][2] == 0.0):\n",
    "                val_correct += 1\n",
    "            \n",
    "            if l_val.item() == 1.0:\n",
    "                val_scores.append(val_score.item())\n",
    "            if l_val.item() == 0.0:\n",
    "                val_gen_scores.append(val_score.item())\n",
    "            \n",
    "            #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "            for x, y in S_new[10*i:10*(i+1)]:\n",
    "                if single_alphabet:\n",
    "                    pass\n",
    "                else:\n",
    "                    a_val, p_val, l_val = convert(x, y, 0)\n",
    "                    gen_score = model(a_val, p_val)\n",
    "                gen_outputs.append(gen_score.item())\n",
    "            \n",
    "            # Generate CDF plots\n",
    "            if len(train_scores) > 1000:\n",
    "                train_cdf = np.asarray(gen_outputs[-10000:] + train_scores[-1000:]) \n",
    "                test_cdf = np.asarray(gen_outputs[-10000:] + val_scores[-1000:])\n",
    "            else:\n",
    "                train_cdf = np.asarray(gen_outputs + train_scores) #combine train and unknown scores\n",
    "                test_cdf = np.asarray(gen_outputs + val_scores)\n",
    "\n",
    "            val_loss = criterion(val_score, l_val) \n",
    "            total_val_loss += val_loss\n",
    "            if iters % batch_size == 0:\n",
    "                ave_val_loss = total_val_loss/batch_size\n",
    "                val_losses.append(ave_val_loss.item())\n",
    "                total_val_loss = 0\n",
    "            if iters % 5000 == 0:\n",
    "                val_acc.append(100*val_correct/iters)\n",
    "                val_losses_avg.append(np.average(val_losses[-5000:]))\n",
    "\n",
    "            if iters % 50000 == 0:\n",
    "                plot_loss(iters, train_losses_avg, val_losses_avg, model_name, model_id)\n",
    "                plot_accuracy(iters, train_acc, val_acc, model_name, model_id)\n",
    "                plot_histogram(train_gen_scores, train_scores, val_gen_scores, val_scores, model_name, model_id)\n",
    "                plot_ecdf_train(train_cdf, iters, full_epoch, lr, model_name, model_id)\n",
    "                plot_ecdf_test(test_cdf, iters, full_epoch, lr, model_name, model_id)\n",
    "                \n",
    "                print(\"Training Accuracy at epoch %d: {}\".format(train_acc[-1]) %epoch)\n",
    "                print(\"Validation Accuracy epoch %d: {}\".format(val_acc[-1]) %epoch)\n",
    "        if save_checkpoints is not None:\n",
    "            print(\"Saving to: \", save_checkpoints)\n",
    "            checkpoint_name = save_checkpoints\n",
    "            torch.save({'epoch': full_epoch,\n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict()}, checkpoint_name)\n",
    "        \n",
    "        # Clear unused gpu memory at the end of the epoch\n",
    "        if device == torch.cuda:\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranslateComplexNet()\n",
    "model_name = model.name\n",
    "model_id = \"06172020\"\n",
    "model.to(device)\n",
    "checkpoint = 'model_checkpoints/binary/%s/06162020.pth' % model_name\n",
    "save_path = 'model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "single_alphabet = False\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "gamma = 1e-3\n",
    "classifier(model, binary_train, binary_val, gamma, model_id, NUM_EPOCHS, BATCH_SIZE, single_alphabet, checkpoint, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aptamers",
   "language": "python",
   "name": "aptamers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
