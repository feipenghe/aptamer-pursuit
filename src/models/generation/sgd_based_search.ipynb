{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide = \"MTATRLST\"\n",
    "aptamer_0 = np.full((40, 4), 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model:  BlosumLinearNet  at epoch:  34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlosumLinearNet(\n",
       "  (fc_apt_1): Linear(in_features=160, out_features=200, bias=True)\n",
       "  (fc_apt_2): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (fc_apt_3): Linear(in_features=250, out_features=300, bias=True)\n",
       "  (fc_pep_1): Linear(in_features=160, out_features=200, bias=True)\n",
       "  (fc_pep_2): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc_apt): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=250, bias=True)\n",
       "    (2): Linear(in_features=250, out_features=300, bias=True)\n",
       "  )\n",
       "  (fc_pep): Sequential(\n",
       "    (0): Linear(in_features=160, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=250, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=550, out_features=600, bias=True)\n",
       "  (fc2): Linear(in_features=600, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = BlosumLinearNet()\n",
    "model_name = model.name\n",
    "model_id = \"07132020\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, \"07102020\")\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "optimizer = SGD(model.parameters(), lr=1e-2)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "init_epoch = checkpoint['epoch'] +1\n",
    "print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        pep = blosum62_encoding(pep, seq_type='peptide') \n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(curr_aptamer[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTTTGGATCGCCCCCTGGGGGAGTGAAACCCGGATTGGAC\n",
      "GTATGCTACCGATCCAGCGTGAATGACAGCCATACTGGAC\n",
      "GTGTGATAACACTCTGGTGTGACTGGCAACTTTACTGGAC\n",
      "CTGTGCTAAGACTCCAGTGAAACTGGCAACTCCACTGGAC\n",
      "CTTTGCACTGGCCCCATTAACACCGGCAAATCGACCAGAA\n",
      "CACTGTATTGTACCCAGTTACAGCAGCCACTCGGCAGGAA\n",
      "CGCCGTATATTTGCCAGTAACAGAAGTTACTATGCCGGAA\n",
      "CGCCGTACATTTCCCAACACCAGAAGTTTCTACGCCGGAA\n",
      "TGCTGTACTTCTCCCAGCAACAGAAGTGCCTGCTTGTGAA\n",
      "CCTTGAACTTCTCCCAGGAGCAGTAGCTCAGGCGCAGGAA\n",
      "TCTTGAACTCGACCTAGGAGCAGCAGCATATGCGCAGACA\n",
      "GTCTCATACCCACCTAGGAGCAGGAGTTTATGCTGGTATA\n",
      "GGCTCAAACAGACCGAGATGAAGGAGCTTAGGCAGGTACA\n",
      "GGCTCATACCGGCCCAGATGCAGCAGCTGTGGGTGGAAAA\n",
      "GGCTAATAAAGACCCTGATGCTGCAGCACCGGGAGGTAAG\n",
      "GGCCCCAGAAGATCCTGATGCTGCAGCAGCGGAGGGAGTA\n",
      "CGCCCGCGTAGATCTCGATCGAGCAGCAGCTGAGGCAGCA\n",
      "TACTCCCGTAGACCCAGATGGTGTTACAGCAGATGAACAA\n",
      "TGGTCGAGTAGACCCAGCTGGAGTTACACCCGTGGCAGCA\n",
      "CTGAGTAGTCATCCTAGCTGTTGCTGCACCTGCGGCAACG\n",
      "CGCTGCAGGAATACTTGAAGGTGCTGCGCCTGCTGCAGCG\n",
      "CACAACAGCAGTCCATCGTCGTGCTGCGTATGCTACACCA\n",
      "TACTACAGCTGTACTTCCTCCTGCTGCATTACTTGGACCA\n",
      "TGCAATAGCAGTTCCTGCTGCGTTTCTTTTACTTGGACCA\n",
      "TGCAACACCAGGACCTACTGCGTCCCCGTTAGTATGAGCG\n",
      "TGCAACCCCAGGACCTACTGCACCCGCTACTGTTGTAGTG\n",
      "TTTTGCAGCGCCACCTGGGGGAGTTAAACCCGGATTGGAG\n",
      "TTGTGCAACCGATCCAGCGGGAGCAACAGCCGTACTAGCG\n",
      "TTGTCCAACGGCTCCGGCGGCACCAGCAACTGTTCTAGCC\n",
      "CGGTGCAACGACTCCTGCGCCACCAGCAACTCCTCTAGCC\n",
      "CTTTGCACCGGCCCCAGCAGCACCAGCAACTCGTGGAGAA\n",
      "CGCTGCAACGTCTCCAGCTGCAGCAGCAACTCGGGGGGAA\n",
      "CGCTGTAAATTTTCCAGCAGCAGCAGTTACTCTTGCGGAG\n",
      "CGCCGCACAATTCCCAACACCAGAAGCTTCTGCTGCAGCA\n",
      "TGCTGCACTATTCCCAGCAGCAGAAGCGCCTGCTTTTGCA\n",
      "TGCTGCACTTCTCCCAGGAGCAGTAGCTCAGGCGGTAGCA\n",
      "TCTTGAACTCGACCTAGGAGCAGCAGCATATGCTGTGACA\n",
      "TGCTCATACCGACCCAGCAGCAGCAGTTTCTGCTGGTACA\n",
      "GGCTCAAACAGAACCAGATGCAGCAGCTTCGGCTGGTACA\n",
      "GGCTCAAACAGGTCCAGCTGCAGCAGCTATGGGTGGAACA\n",
      "GGCTAATAAAGGCCCTGCTGCTGCAGCACCGGGTGGAACG\n",
      "GGCCCCAGAAGATCCTGCTGCTGCAGCAGCGGGGGGAGTA\n",
      "CGCCCCCGTAGATCTTGCTGGAGCAGCAGCTGAGGCAGCA\n",
      "TGCTCCAGTAGACCCTGATGGTGTTACAGCAGATGCAGAA\n",
      "TGGTCCAGTAGACCCAGCTGGAGCTGCACCTGTGGCAGCA\n",
      "CTCAGCAGTCGTCCTAGCTGCTGCTGCACCTGCGGCAGCG\n",
      "CGCTGCAGCAATACTTGCAGCTGCTGCGTCTGCTGCAGCG\n",
      "CACTACAGCAGTCCCTGGTCCTGCTGCGTCTGCTGCAGCA\n",
      "TACTACAGCAGTACCTCCTGCTGCTGCGTTACTTGGACCG\n",
      "TGCTATAGCAGTTCCTGCTGCTTCTCTTTTAGTTGGACCG\n",
      "TGCAACACCAGGACCTACTGCGGCCGCGTCAGTAGGAGCG\n",
      "TGCAACCCCAGGACCTACTGCACCTGCTACTGTTGTAGTG\n",
      "TGTTGCAGCGGCACCTGGGGCAGCTAAACCCGGTTTAGCG\n",
      "TTGTGCAACAGATCCAGCAGGAGCAACAGCTGTACTAGCG\n",
      "TTCTCCAACAGCTCCGGCAGCACCAGCAACTGTTGTAGCG\n",
      "CGCTGCAACAACTCCTGCAGCAGCAGCAACTGCTCTAGCG\n",
      "CGTTGCACCGGCTCCAGCAGCAGCAGCAACTCGTGGAGAA\n",
      "CGCTGCAACGTCTCCAGCTGCAGCAGCAACTCGTGGAGAA\n",
      "CGCTGTAAAAGTTCCAGCAGCAGCAGCTACTCTTGCGGAG\n",
      "CGCCGCACCAGTCCCAACAGCAGCAGCTTCTGCTGCAGCA\n",
      "TGCTGCACTAGTCCCAGCAGCAGCAGCGCCTGCTGTTGCA\n",
      "TGCTGCACCTCTCCCAGGAGCAGTAGCTCATGCTGTAGCA\n",
      "TGTTGAACCCGACCTAGCAGCAGCAGCATCTGCTGTGGCA\n",
      "TGCTCAAACCGACCCAGCAGCAGCAGCTTCTGCTGGTACA\n",
      "CGCTCAAACAGAACCAGATGCAGCAGCTTCTGCTGGTACA\n",
      "GGCTCAAACAGGTCCAGCTGCAGCAGCTATGGGTGGAGCA\n",
      "GGCTAATAAAGGCCCTGCTGCTGCAGCACCGGGTGGAGCG\n",
      "GGCTCCAGAAGGTCCTGCTGCTGCAGCAGCTGGGGGAGCA\n",
      "CGCCCCCGTAGATCTTGCTGCAGCAGCAGCTGATGCAGCA\n",
      "TGCTCCAGTAGACCCTGATGGTGCTACAGCTGGTGCAGCA\n",
      "TGCTCCAGTAGACCCAGCTGGAGCTGCACCTGTGGCAGCA\n",
      "CGCTGCAGTAGTCCTAGCTGCTGCTGCACCTGCTGCAGCG\n",
      "CGCTGCAGCAATACTTGCAGCTGCTGCGTCTGCTGCAGCG\n",
      "CACTACAGCAGTCCCTGCTGCTGCTGCGTCTGCTGCAGCA\n",
      "TACTACAGCAGTACCTGCTGCTGCTGCGTTACTTGGACCG\n",
      "TGCTATAGCAGTTCCTGCTGCTTCTGTTTCTGTTGGACCG\n",
      "TGCAACACCAGGACCTACTGCGGCTGCGTCTGTTGGAGCG\n",
      "TGCAACACCAGGACCTACTGCACCTGCTACTGTTGTAGCG\n",
      "TGCTGCAGCGGCACCTGCTGCAGCTACACCCGGTGTAGCG\n",
      "TTCTGCAACAGATCCAGCAGGAGCAGCAGCTGTACTAGCG\n",
      "TTCTCCAACAGCTCCGGCAGCACCAGCAACTGTTGTAGCG\n",
      "CGCTGCAACAACTCCTGCAGCAGCAGCAACTGCTCTAGCG\n",
      "CGCTGCACCGGCTCCAGCAGCAGCAGCAACTCGTGGAGAA\n",
      "CGCTGCAACGTCTCCAGCTGCAGCAGCAACTCGTGGAGAA\n",
      "CGCTGCAACAGTTCCAGCAGCAGCAGCTACTCTTGCGGAG\n",
      "CGCCGCACCAGTCCCAGCAGCAGCAGCTTCTGCTGCAGCA\n",
      "TGCTGCACCAGTCCCAGCAGCAGCAGCGCCTGCTGTAGCA\n",
      "TGCTGCACCTGTCCCAGCAGCAGCAGCTCCTGCTGTAGCA\n",
      "TGTTGAACCCGACCCAGCAGCAGCAGCATCTGCTGTGGCA\n",
      "TGCTCAAACCGACCCAGCAGCAGCAGCTTCTGCTGGTACA\n",
      "CGCTCAAACAGAACCAGCTGCAGCAGCTTCTGCTGGTGCA\n",
      "GGCTCCAACAGGTCCAGCTGCAGCAGCTACGGGTGGAGCA\n",
      "GGCTACTACAGGCCCTGCTGCTGCAGCACCTGGTGGAGCG\n",
      "GGCTCCAGAAGGTCCTGCTGCTGCAGCAGCTGGGGGAGCA\n",
      "CGCCCCAGTAGATCCTGCTGCAGCAGCAGCTGATGCAGCA\n",
      "TGCTCCAGTAGACCCTGATGGTGCTGCAGCTGGTGCAGCA\n",
      "TGCTCCAGTAGACCCTGCTGGAGCTGCACCTGTTGCAGCA\n",
      "CGCTGCAGTAGTCCCAGCTGCTGCTGCACCTGCTGCAGCG\n",
      "CGCTGCAGCAATACTTGCAGCTGCTGCGTCTGCTGCAGCG\n",
      "CGCTACAGCAGTACCTGCTGCTGCTGCGTCTGCTGCAGCA\n"
     ]
    }
   ],
   "source": [
    "curr_aptamer = aptamer_0\n",
    "for k in range(100):\n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    train_score = update(a, p, None, single_alphabet=False)\n",
    "    train_score.backward()\n",
    "    new_aptamer = np.zeros((40, 4, 1))\n",
    "    alpha_k = 1/(2*(k + 1))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(a.grad[:, :, i].cpu().numpy())\n",
    "        for j in range(4):\n",
    "            # new_aptamer.shape = 40, 4, 1\n",
    "            # curr_aptamer.shape = 1, 4, 40\n",
    "            new_aptamer[i, j, 0] = (1 - alpha_k)*a[0, j, i] + alpha_k*(j == ind)\n",
    "    \n",
    "    curr_aptamer = new_aptamer\n",
    "    # Round the aptamer and find the resulting string\n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    aptamer_string = stringify(rounded_aptamer)\n",
    "    print(str(aptamer_string))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
