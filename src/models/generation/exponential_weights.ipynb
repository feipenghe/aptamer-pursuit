{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.reshape((-1, 1)).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-634d513fbc57>:210: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  torch.nn.init.xavier_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model:  ResnetSeparated  at epoch:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetSeparated(\n",
       "  (conv1_apt): Conv1d(4, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv1_pep): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (bn1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer0): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(192, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(192, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=96, out_features=1, bias=True)\n",
       "  (apt_initial): Sequential(\n",
       "    (0): Conv1d(4, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       "  (pep_initial): Sequential(\n",
       "    (0): Conv1d(20, 192, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  )\n",
       "  (conv_merge): Conv1d(256, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv1d(192, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv1d(192, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (conv2): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Conv1d(256, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (vlp): VariableLengthPooling()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = ResNetSeparated(BasicBlock, [1, 1,], num_classes=2)\n",
    "model_name = model.name\n",
    "model_id = \"08102020\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "optimizer = SGD(model.parameters(), lr=7e-4)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "init_epoch = checkpoint['epoch'] +1\n",
    "print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(apt[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbed_uniform():\n",
    "    aptamer_0 = np.full((40, 4), 0.25)\n",
    "    for i in range(40):\n",
    "        # Perturb this column 25% of the time\n",
    "        if random.randint(0, 3) == 1:\n",
    "            # Find an index to perturb\n",
    "            inds = random.sample(range(0, 4), 2)\n",
    "            ind_0 = inds[0]\n",
    "            ind_1 = inds[1]\n",
    "            \n",
    "            # Find an amount to perturb\n",
    "            amt = random.randint(0, 25)\n",
    "            amt /= 100\n",
    "            \n",
    "            # Transaction\n",
    "            aptamer_0[i][ind_0] = 0.25 + amt\n",
    "            aptamer_0[i][ind_1] = 0.25 - amt\n",
    "    \n",
    "    return aptamer_0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_random_initialization():\n",
    "    aptamer_0 = np.zeros((40, 4))\n",
    "    for i in range(40):\n",
    "        random_nums = []\n",
    "        for j in range(4):\n",
    "            u_i = random.randint(1, 20)\n",
    "            random_nums.append(math.exp(1/u_i))\n",
    "        random_nums = [x/sum(random_nums) for x in random_nums]\n",
    "        for j in range(4):\n",
    "            aptamer_0[i][j] = random_nums[j]\n",
    "    return aptamer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialization():\n",
    "    aptamer_0 = np.zeros((40, 4))\n",
    "    for i in range(40):\n",
    "        random_nums = []\n",
    "        for j in range(4):\n",
    "            u_i = random.randint(1, 6)\n",
    "            random_nums.append(u_i)\n",
    "        random_nums = [x/sum(random_nums) for x in random_nums]\n",
    "        for j in range(4):\n",
    "            aptamer_0[i][j] = random_nums[j]\n",
    "    return aptamer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3       , 0.3       , 0.25      , 0.15      ],\n",
       "       [0.16666667, 0.33333333, 0.27777778, 0.22222222],\n",
       "       [0.22222222, 0.22222222, 0.44444444, 0.11111111],\n",
       "       [0.23076923, 0.46153846, 0.15384615, 0.15384615],\n",
       "       [0.33333333, 0.33333333, 0.16666667, 0.16666667],\n",
       "       [0.38461538, 0.07692308, 0.07692308, 0.46153846],\n",
       "       [0.42857143, 0.21428571, 0.28571429, 0.07142857],\n",
       "       [0.21052632, 0.21052632, 0.26315789, 0.31578947],\n",
       "       [0.375     , 0.125     , 0.125     , 0.375     ],\n",
       "       [0.2       , 0.13333333, 0.26666667, 0.4       ],\n",
       "       [0.22222222, 0.33333333, 0.16666667, 0.27777778],\n",
       "       [0.22222222, 0.22222222, 0.11111111, 0.44444444],\n",
       "       [0.22222222, 0.33333333, 0.27777778, 0.16666667],\n",
       "       [0.23076923, 0.15384615, 0.46153846, 0.15384615],\n",
       "       [0.27272727, 0.18181818, 0.36363636, 0.18181818],\n",
       "       [0.33333333, 0.11111111, 0.22222222, 0.33333333],\n",
       "       [0.3       , 0.2       , 0.2       , 0.3       ],\n",
       "       [0.23076923, 0.30769231, 0.23076923, 0.23076923],\n",
       "       [0.15384615, 0.23076923, 0.38461538, 0.23076923],\n",
       "       [0.07692308, 0.38461538, 0.46153846, 0.07692308],\n",
       "       [0.26315789, 0.31578947, 0.21052632, 0.21052632],\n",
       "       [0.13333333, 0.33333333, 0.13333333, 0.4       ],\n",
       "       [0.16666667, 0.41666667, 0.33333333, 0.08333333],\n",
       "       [0.3       , 0.1       , 0.5       , 0.1       ],\n",
       "       [0.33333333, 0.26666667, 0.06666667, 0.33333333],\n",
       "       [0.35294118, 0.23529412, 0.23529412, 0.17647059],\n",
       "       [0.06666667, 0.26666667, 0.26666667, 0.4       ],\n",
       "       [0.17647059, 0.29411765, 0.17647059, 0.35294118],\n",
       "       [0.1875    , 0.0625    , 0.375     , 0.375     ],\n",
       "       [0.5       , 0.08333333, 0.25      , 0.16666667],\n",
       "       [0.41666667, 0.25      , 0.25      , 0.08333333],\n",
       "       [0.16666667, 0.27777778, 0.22222222, 0.33333333],\n",
       "       [0.22222222, 0.22222222, 0.33333333, 0.22222222],\n",
       "       [0.33333333, 0.22222222, 0.33333333, 0.11111111],\n",
       "       [0.38461538, 0.23076923, 0.30769231, 0.07692308],\n",
       "       [0.5       , 0.25      , 0.16666667, 0.08333333],\n",
       "       [0.14285714, 0.35714286, 0.07142857, 0.42857143],\n",
       "       [0.09090909, 0.18181818, 0.45454545, 0.27272727],\n",
       "       [0.13333333, 0.13333333, 0.4       , 0.33333333],\n",
       "       [0.28571429, 0.35714286, 0.21428571, 0.14285714]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_weights(peptide, actual_aptamer):\n",
    "    # Initialization with exponential random nums\n",
    "    aptamer_0 = exponential_random_initialization()\n",
    "\n",
    "    curr_aptamer = aptamer_0\n",
    "    scores = []\n",
    "    for k in range(200):\n",
    "        a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "        train_score = update(a, p, 1, single_alphabet=False)\n",
    "        scores.append(train_score.item())\n",
    "        train_score.backward()\n",
    "        new_aptamer = np.zeros((40, 4, 1))\n",
    "        # Learning rate\n",
    "        lr = 1e-4\n",
    "        # Looping over the indices\n",
    "        for i in range(40):\n",
    "            # Gradient for this index, this is loss?\n",
    "            l_t = a.grad[:, :, i].cpu().numpy()        \n",
    "            w = a[:, :, i][0].cpu().detach().numpy()\n",
    "            # New probability distribution\n",
    "            denom = 0\n",
    "            new_w = [0, 0, 0, 0]\n",
    "            for j in range(4):\n",
    "                num = w[j]*math.exp(lr*-1*l_t[0][j])\n",
    "                new_w[j] = num\n",
    "                denom += num\n",
    "            for j in range(4):\n",
    "                new_aptamer[i][j] = new_w[j]/denom\n",
    "\n",
    "\n",
    "        curr_aptamer = new_aptamer\n",
    "    \n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    unrounded_score = update(a, p, 1, single_alphabet=False)\n",
    "    \n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    aptamer_string = stringify(rounded_aptamer)\n",
    "\n",
    "    a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "    a = a.permute((2, 1, 0))\n",
    "    final_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    # Test the actual aptamer\n",
    "    actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "    a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "    actual_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    return unrounded_score, final_score, actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5453]], device='cuda:0', grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.5044]], device='cuda:0', grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.5369]], device='cuda:0', grad_fn=<SigmoidBackward>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "exponential_weights(peptide, actual_aptamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:10<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Found:  64\n",
      "Unoptimal Found:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/fw_pairs.txt') as f:\n",
    "    optimal_count = 0\n",
    "    unoptimal_count = 0\n",
    "    found_scores = []\n",
    "    actual_scores = []\n",
    "    for i, pair in enumerate(tqdm.tqdm(f.readlines()[:100])):\n",
    "        apt, pep = pair.split()\n",
    "        unrounded_score, found_score, actual_score = exponential_weights(pep, apt)\n",
    "        if unrounded_score >= actual_score:\n",
    "            optimal_count += 1\n",
    "        else:\n",
    "            unoptimal_count += 1\n",
    "        found_scores.append(found_score)\n",
    "        actual_scores.append(actual_score)\n",
    "print(\"Optimal Found: \", optimal_count)\n",
    "print(\"Unoptimal Found: \", unoptimal_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Results of the Exponential Weights Algorithm on 100 items from the higher quality dataset')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.stripplot(found_scores, color='blue', label='FW Found Scores')\n",
    "sns.stripplot(actual_scores, color='red', label='Actual Aptamer Scores')\n",
    "plt.legend()\n",
    "plt.title(\"Results of the Exponential Weights Algorithm on 100 items from the higher quality dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
