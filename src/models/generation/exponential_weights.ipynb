{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://blog.wouterkoolen.info/GDasEW/post.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.reshape((-1, 1)).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     5,
     36,
     74,
     86,
     168,
     170
    ]
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlosumNet(\n",
       "  (cnn_apt_1): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_2): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_3): Conv1d(50, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_4): Conv1d(25, 10, kernel_size=(3,), stride=(1,))\n",
       "  (cnn_pep_1): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_2): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_3): Conv1d(80, 150, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_4): Conv1d(150, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_5): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_apt): Sequential(\n",
       "    (0): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (cnn_pep): Sequential(\n",
       "    (0): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc1): Linear(in_features=790, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = BlosumNet()\n",
    "model_name = model.name\n",
    "model_id = \"08042020_3\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "#optimizer = SGD(model.parameters(), lr=7e-4)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#init_epoch = checkpoint['epoch'] +1\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(apt[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def perturbed_uniform():\n",
    "    aptamer_0 = np.full((40, 4), 0.25)\n",
    "    for i in range(40):\n",
    "        # Perturb this column 25% of the time\n",
    "        if random.randint(0, 3) == 1:\n",
    "            # Find an index to perturb\n",
    "            inds = random.sample(range(0, 4), 2)\n",
    "            ind_0 = inds[0]\n",
    "            ind_1 = inds[1]\n",
    "            \n",
    "            # Find an amount to perturb\n",
    "            amt = random.randint(0, 25)\n",
    "            amt /= 100\n",
    "            \n",
    "            # Transaction\n",
    "            aptamer_0[i][ind_0] = 0.25 + amt\n",
    "            aptamer_0[i][ind_1] = 0.25 - amt\n",
    "    \n",
    "    return aptamer_0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def exponential_random_initialization():\n",
    "    aptamer_0 = np.zeros((40, 4))\n",
    "    for i in range(40):\n",
    "        d = 4\n",
    "        e = np.exp(np.random.rand(d))\n",
    "        e /= np.sum(e)\n",
    "        for j in range(4):\n",
    "            aptamer_0[i][j] = e[j]\n",
    "    return aptamer_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5405]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0a782b04f0>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#def exponential_weights(peptide, actual_aptamer):\n",
    "# Initialization with exponential random nums\n",
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "\n",
    "#aptamer_0 = one_hot(actual_aptamer, seq_type='aptamer')#exponential_random_initialization()\n",
    "aptamer_0 = exponential_random_initialization()\n",
    "curr_aptamer = aptamer_0\n",
    "lr = 1e-3\n",
    "scores = []\n",
    "for k in range(1000):\n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    train_score = update(a, p, 1, single_alphabet=False)\n",
    "    train_score.backward()\n",
    "    scores.append(train_score.item())\n",
    "    \n",
    "    new_aptamer = np.zeros((40, 4, 1))\n",
    "    # Looping over the indices\n",
    "    for i in range(40):\n",
    "        # Gradient for this position\n",
    "        l_t = a.grad.cpu().numpy()\n",
    "        l_t = l_t.reshape((40, 1, 4))[i]\n",
    "        l_t = l_t[0]\n",
    "        \n",
    "        # Old probability distribution\n",
    "        w = a.reshape((40, 1, 4))[i].cpu().detach().numpy()\n",
    "        w = w[0]\n",
    "        \n",
    "        # New probability distribution\n",
    "        denom = 0\n",
    "        for j in range(4):            \n",
    "            new_aptamer[i][j] = w[j]*np.exp(-1*lr*l_t[j])\n",
    "            denom += new_aptamer[i][j]\n",
    "        for j in range(4):\n",
    "            new_aptamer[i][j] = new_aptamer[i][j]/denom\n",
    "\n",
    "    curr_aptamer = new_aptamer\n",
    "    \n",
    "\n",
    "a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "unrounded_score = update(a, p, 1, single_alphabet=False)\n",
    "print(str(unrounded_score))\n",
    "plt.plot(scores)\n",
    "\n",
    "# Rounding Strategy # 1: Just max\n",
    "# rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "# aptamer_string = stringify(rounded_aptamer)\n",
    "\n",
    "# # Rounding Strategy #2: \n",
    "\n",
    "# a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "# a = a.permute((2, 1, 0))\n",
    "# final_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "# # Test the actual aptamer\n",
    "# actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "# a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "# actual_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "#return unrounded_score, final_score, actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.24153235]\n",
      "  [0.25729344]\n",
      "  [0.32193973]\n",
      "  [0.17923449]]\n",
      "\n",
      " [[0.31524466]\n",
      "  [0.22116797]\n",
      "  [0.16394551]\n",
      "  [0.29964186]]\n",
      "\n",
      " [[0.21206789]\n",
      "  [0.32092371]\n",
      "  [0.29109249]\n",
      "  [0.1759159 ]]\n",
      "\n",
      " [[0.27209484]\n",
      "  [0.31863523]\n",
      "  [0.20469172]\n",
      "  [0.20457821]]\n",
      "\n",
      " [[0.3345507 ]\n",
      "  [0.1924173 ]\n",
      "  [0.31887727]\n",
      "  [0.15415473]]\n",
      "\n",
      " [[0.29513255]\n",
      "  [0.23233107]\n",
      "  [0.26141029]\n",
      "  [0.21112609]]\n",
      "\n",
      " [[0.22771686]\n",
      "  [0.3569091 ]\n",
      "  [0.18344719]\n",
      "  [0.23192685]]\n",
      "\n",
      " [[0.16519694]\n",
      "  [0.21066955]\n",
      "  [0.38155635]\n",
      "  [0.24257716]]\n",
      "\n",
      " [[0.34902981]\n",
      "  [0.2858337 ]\n",
      "  [0.21862994]\n",
      "  [0.14650655]]\n",
      "\n",
      " [[0.24183712]\n",
      "  [0.22615693]\n",
      "  [0.27609082]\n",
      "  [0.25591513]]\n",
      "\n",
      " [[0.28330416]\n",
      "  [0.14843465]\n",
      "  [0.26972725]\n",
      "  [0.29853394]]\n",
      "\n",
      " [[0.35738205]\n",
      "  [0.24094221]\n",
      "  [0.14751543]\n",
      "  [0.2541603 ]]\n",
      "\n",
      " [[0.18121667]\n",
      "  [0.19196084]\n",
      "  [0.18916584]\n",
      "  [0.43765664]]\n",
      "\n",
      " [[0.32197225]\n",
      "  [0.28793722]\n",
      "  [0.24666407]\n",
      "  [0.14342646]]\n",
      "\n",
      " [[0.34112947]\n",
      "  [0.17729586]\n",
      "  [0.26259393]\n",
      "  [0.21898074]]\n",
      "\n",
      " [[0.22775634]\n",
      "  [0.2601161 ]\n",
      "  [0.27870952]\n",
      "  [0.23341804]]\n",
      "\n",
      " [[0.39307739]\n",
      "  [0.22843755]\n",
      "  [0.18268828]\n",
      "  [0.19579678]]\n",
      "\n",
      " [[0.33634397]\n",
      "  [0.29564959]\n",
      "  [0.16902594]\n",
      "  [0.1989805 ]]\n",
      "\n",
      " [[0.17359824]\n",
      "  [0.30293666]\n",
      "  [0.24152219]\n",
      "  [0.28194291]]\n",
      "\n",
      " [[0.41171452]\n",
      "  [0.18414382]\n",
      "  [0.20698384]\n",
      "  [0.19715782]]\n",
      "\n",
      " [[0.28817982]\n",
      "  [0.24931448]\n",
      "  [0.20411839]\n",
      "  [0.25838731]]\n",
      "\n",
      " [[0.33701969]\n",
      "  [0.14498984]\n",
      "  [0.21798647]\n",
      "  [0.300004  ]]\n",
      "\n",
      " [[0.14702705]\n",
      "  [0.16176745]\n",
      "  [0.35421054]\n",
      "  [0.33699496]]\n",
      "\n",
      " [[0.19350544]\n",
      "  [0.31460249]\n",
      "  [0.19352654]\n",
      "  [0.29836553]]\n",
      "\n",
      " [[0.39176569]\n",
      "  [0.16660228]\n",
      "  [0.17875824]\n",
      "  [0.2628738 ]]\n",
      "\n",
      " [[0.31947566]\n",
      "  [0.21547951]\n",
      "  [0.25718266]\n",
      "  [0.20786217]]\n",
      "\n",
      " [[0.40026683]\n",
      "  [0.18845337]\n",
      "  [0.172176  ]\n",
      "  [0.2391038 ]]\n",
      "\n",
      " [[0.18037958]\n",
      "  [0.24539109]\n",
      "  [0.2341207 ]\n",
      "  [0.34010864]]\n",
      "\n",
      " [[0.38732647]\n",
      "  [0.19502693]\n",
      "  [0.18738637]\n",
      "  [0.23026023]]\n",
      "\n",
      " [[0.17682752]\n",
      "  [0.22511931]\n",
      "  [0.38512514]\n",
      "  [0.21292802]]\n",
      "\n",
      " [[0.36077512]\n",
      "  [0.19510584]\n",
      "  [0.25788651]\n",
      "  [0.18623252]]\n",
      "\n",
      " [[0.30331402]\n",
      "  [0.26891961]\n",
      "  [0.24170179]\n",
      "  [0.18606459]]\n",
      "\n",
      " [[0.26534478]\n",
      "  [0.20405118]\n",
      "  [0.17312469]\n",
      "  [0.35747935]]\n",
      "\n",
      " [[0.24750224]\n",
      "  [0.25757511]\n",
      "  [0.2468268 ]\n",
      "  [0.24809585]]\n",
      "\n",
      " [[0.31515241]\n",
      "  [0.26554967]\n",
      "  [0.28821308]\n",
      "  [0.13108484]]\n",
      "\n",
      " [[0.17144585]\n",
      "  [0.2482902 ]\n",
      "  [0.23680283]\n",
      "  [0.34346112]]\n",
      "\n",
      " [[0.19915827]\n",
      "  [0.33605026]\n",
      "  [0.20160172]\n",
      "  [0.26318974]]\n",
      "\n",
      " [[0.35751729]\n",
      "  [0.20624197]\n",
      "  [0.22975381]\n",
      "  [0.20648694]]\n",
      "\n",
      " [[0.21097737]\n",
      "  [0.2366289 ]\n",
      "  [0.32981908]\n",
      "  [0.22257465]]\n",
      "\n",
      " [[0.28660077]\n",
      "  [0.2489517 ]\n",
      "  [0.17323255]\n",
      "  [0.29121498]]]\n"
     ]
    }
   ],
   "source": [
    "print(str(curr_aptamer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "unrounded, _, _ = exponential_weights(peptide, actual_aptamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/fw_pairs.txt') as f:\n",
    "    optimal_count = 0\n",
    "    unoptimal_count = 0\n",
    "    found_scores = []\n",
    "    actual_scores = []\n",
    "    for i, pair in enumerate(tqdm.tqdm(f.readlines()[:200])):\n",
    "        apt, pep = pair.split()\n",
    "        unrounded_score, found_score, actual_score = exponential_weights(pep, apt)\n",
    "        if unrounded_score >= actual_score:\n",
    "            optimal_count += 1\n",
    "        else:\n",
    "            unoptimal_count += 1\n",
    "            print(\"Actual Score: \" + str(actual_score) + \"Found score: \" + str(unrounded_score))\n",
    "        found_scores.append(unrounded_score.item())\n",
    "        actual_scores.append(actual_score.item())\n",
    "print(\"Optimal Found: \", optimal_count)\n",
    "print(\"Unoptimal Found: \", unoptimal_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(found_scores, color='blue', label='Exponential Weights Unrounded Aptamer Scores')\n",
    "sns.stripplot(actual_scores, color='red', label='Actual Aptamer Scores')\n",
    "plt.legend()\n",
    "plt.title(\"Results of the Exponential Weights Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
