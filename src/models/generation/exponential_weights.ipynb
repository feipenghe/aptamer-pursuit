{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://blog.wouterkoolen.info/GDasEW/post.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.reshape((-1, 1)).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     36,
     74,
     86,
     168,
     170
    ]
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = BlosumNet()\n",
    "model_name = model.name\n",
    "model_id = \"08042020_3\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "#optimizer = SGD(model.parameters(), lr=7e-4)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#init_epoch = checkpoint['epoch'] +1\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(apt[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def perturbed_uniform():\n",
    "    aptamer_0 = np.full((40, 4), 0.25)\n",
    "    for i in range(40):\n",
    "        # Perturb this column 25% of the time\n",
    "        if random.randint(0, 3) == 1:\n",
    "            # Find an index to perturb\n",
    "            inds = random.sample(range(0, 4), 2)\n",
    "            ind_0 = inds[0]\n",
    "            ind_1 = inds[1]\n",
    "            \n",
    "            # Find an amount to perturb\n",
    "            amt = random.randint(0, 25)\n",
    "            amt /= 100\n",
    "            \n",
    "            # Transaction\n",
    "            aptamer_0[i][ind_0] = 0.25 + amt\n",
    "            aptamer_0[i][ind_1] = 0.25 - amt\n",
    "    \n",
    "    return aptamer_0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_random_initialization():\n",
    "    aptamer_0 = np.zeros((40, 4))\n",
    "    for i in range(40):\n",
    "        d = 4\n",
    "        e = np.exp(np.random.rand(d))\n",
    "        e /= np.sum(e)\n",
    "        for j in range(4):\n",
    "            aptamer_0[i][j] = e[j]\n",
    "    return aptamer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climb(peptide, initial_aptamer_str, iterations=100):\n",
    "    initial_aptamer = one_hot(initial_aptamer_str, seq_type='aptamer', single_alphabet=False)\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    for it in range(iterations):\n",
    "        # Get the original score\n",
    "        a, p, l = convert(initial_aptamer, peptide, 1, single_alphabet=False)\n",
    "        best_score = update(a, p, 1, single_alphabet=False).item()\n",
    "        best_aptamer = initial_aptamer_str\n",
    "        curr_apt = list(initial_aptamer_str)\n",
    "        # For each position\n",
    "        for i in range(40):\n",
    "            for n in range(4):\n",
    "                # Change one character\n",
    "                test_apt = curr_apt.copy()\n",
    "                test_apt[i] = nucleotides[n]\n",
    "                test_apt_str = ''.join(test_apt)\n",
    "                test_apt = one_hot(test_apt_str, seq_type='aptamer', single_alphabet=False)\n",
    "                a, p, l = convert(test_apt, peptide, 1, single_alphabet=False)\n",
    "                score = update(a, p, 1, single_alphabet=False)\n",
    "                if score.item() > best_score:\n",
    "                    best_score = score.item()\n",
    "                    best_aptamer = test_apt_str\n",
    "    return best_score, best_aptamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_weights(peptide, actual_aptamer):\n",
    "    # Initialization with exponential random nums\n",
    "    peptide = \"MMFKYRAP\"\n",
    "    actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "\n",
    "    #aptamer_0 = one_hot(actual_aptamer, seq_type='aptamer')#exponential_random_initialization()\n",
    "    aptamer_0 = exponential_random_initialization()\n",
    "    curr_aptamer = aptamer_0\n",
    "\n",
    "    lr = 5e-1\n",
    "    scores = []\n",
    "    for k in range(10000):\n",
    "        a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "        train_score = update(a, p, 1, single_alphabet=False)\n",
    "        train_score.backward()\n",
    "        scores.append(train_score.item())\n",
    "\n",
    "        new_aptamer = np.zeros((40, 4, 1))\n",
    "        # Looping over the indices\n",
    "        for i in range(40):\n",
    "            # Gradient for this position\n",
    "            l_t = a.grad.cpu().numpy()\n",
    "            l_t = l_t.reshape((40, 1, 4))[i]\n",
    "            l_t = l_t[0]\n",
    "\n",
    "            # Old probability distribution\n",
    "            w = a.reshape((40, 1, 4))[i].cpu().detach().numpy()\n",
    "            w = w[0]\n",
    "\n",
    "            # New probability distribution\n",
    "            denom = 0\n",
    "            for j in range(4):            \n",
    "                new_aptamer[i][j] = w[j]*np.exp(lr*l_t[j])\n",
    "                denom += new_aptamer[i][j]\n",
    "            for j in range(4):\n",
    "                new_aptamer[i][j] = new_aptamer[i][j]/denom\n",
    "\n",
    "        curr_aptamer = new_aptamer\n",
    "\n",
    "\n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    unrounded_score = update(a, p, 1, single_alphabet=False)\n",
    "    print(\"Unrounded aptamer: \", curr_aptamer)\n",
    "\n",
    "    # Rounding Strategy # 1: Just max\n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    rounded_aptamer_str = stringify(rounded_aptamer)\n",
    "    a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "    a = a.permute((2, 1, 0))\n",
    "    max_rounded_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    # Rounding Strategy #2: Hill climbing\n",
    "    hill_climb_score, hill_climbed_aptamer = hill_climb(peptide, rounded_aptamer_str)\n",
    "    print(\"Hill climb score: \", hill_climb_score)\n",
    "\n",
    "\n",
    "    # # Test the actual aptamer\n",
    "    actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "    a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "    actual_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    print(\"Rounded Score: \", final_score)\n",
    "    print(\"Unrounded Score: \", unrounded_score)\n",
    "    return unrounded_score, max_rounded_score, hill_climb_score, actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "unrounded_score, max_rounded_score, hill_climb_score, actual_score = exponential_weights(peptide, actual_aptamer)\n",
    "print(\"Unrounded: \", unrounded_score)\n",
    "print(\"Max Rounded: \", max_rounded_score)\n",
    "print(\"Hill Climb: \", hill_climb_score)\n",
    "print(\"Actual Score: \", actual_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out if we get optimal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/fw_pairs.txt') as f:\n",
    "    optimal_count = 0\n",
    "    unoptimal_count = 0\n",
    "    found_scores = []\n",
    "    actual_scores = []\n",
    "    for i, pair in enumerate(tqdm.tqdm(f.readlines()[:30])):\n",
    "        apt, pep = pair.split()\n",
    "        unrounded_score, found_score, actual_score = exponential_weights(pep, apt)\n",
    "        if unrounded_score >= actual_score:\n",
    "            optimal_count += 1\n",
    "        else:\n",
    "            unoptimal_count += 1\n",
    "            print(\"Actual Score: \" + str(actual_score) + \"Found score: \" + str(unrounded_score))\n",
    "        found_scores.append(unrounded_score.item())\n",
    "        actual_scores.append(actual_score.item())\n",
    "print(\"Optimal Found: \", optimal_count)\n",
    "print(\"Unoptimal Found: \", unoptimal_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(found_scores, color='blue', label='Exponential Weights Unrounded Aptamer Scores')\n",
    "sns.stripplot(actual_scores, color='red', label='Actual Aptamer Scores')\n",
    "plt.legend()\n",
    "plt.title(\"Results of the Exponential Weights Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
