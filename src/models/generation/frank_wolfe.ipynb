{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.reshape((-1, 1)).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model:  BlosumNet  at epoch:  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlosumNet(\n",
       "  (cnn_apt_1): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_2): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_3): Conv1d(50, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_4): Conv1d(25, 10, kernel_size=(3,), stride=(1,))\n",
       "  (cnn_pep_1): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_2): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_3): Conv1d(80, 150, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_4): Conv1d(150, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_5): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_apt): Sequential(\n",
       "    (0): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (cnn_pep): Sequential(\n",
       "    (0): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc1): Linear(in_features=790, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = BlosumNet()\n",
    "model_name = model.name\n",
    "model_id = \"08042020_3\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "optimizer = SGD(model.parameters(), lr=7e-4)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "init_epoch = checkpoint['epoch'] +1\n",
    "print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(apt[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbed_uniform():\n",
    "    aptamer_0 = np.full((40, 4), 0.25)\n",
    "    for i in range(40):\n",
    "        # Perturb this column 25% of the time\n",
    "        if random.randint(0, 3) == 1:\n",
    "            # Find an index to perturb\n",
    "            inds = random.sample(range(0, 4), 2)\n",
    "            ind_0 = inds[0]\n",
    "            ind_1 = inds[1]\n",
    "            \n",
    "            # Find an amount to perturb\n",
    "            amt = random.randint(0, 25)\n",
    "            amt /= 100\n",
    "            \n",
    "            # Transaction\n",
    "            aptamer_0[i][ind_0] = 0.25 + amt\n",
    "            aptamer_0[i][ind_1] = 0.25 - amt\n",
    "    \n",
    "    return aptamer_0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frank_wolfe(peptide, actual_aptamer):\n",
    "# Initialization with random characters\n",
    "#     aptamer_0 = \"\"\n",
    "#     for i in range(40):\n",
    "#         aptamer_0 += random.choice(na_list)\n",
    "#     aptamer_0 = one_hot(aptamer_0, seq_type='aptamer')\n",
    "    # Initialization with all 0.25\n",
    "    #aptamer_0 = np.full((40, 4), 0.25)\n",
    "    \n",
    "    # Initialization with perturbed all 0s\n",
    "    aptamer_0 = perturbed_uniform()\n",
    "    curr_aptamer = aptamer_0\n",
    "    scores = []\n",
    "    for k in range(200):\n",
    "        a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "        train_score = update(a, p, 1, single_alphabet=False)\n",
    "        scores.append(train_score.item())\n",
    "        train_score.backward()\n",
    "        new_aptamer = np.zeros((40, 4, 1))\n",
    "        alpha_k = 4/(k + 2)\n",
    "        #print(str(np.sum(a.grad.cpu().numpy())))\n",
    "        for i in range(40):\n",
    "            # Gradient wrt aptamer\n",
    "            ind = np.argmax(a.grad[:, :, i].cpu().numpy())\n",
    "            for j in range(4):\n",
    "                # new_aptamer.shape = 40, 4, 1\n",
    "                # curr_aptamer.shape = 1, 4, 40\n",
    "                new_aptamer[i, j, 0] = (1 - alpha_k)*a[0, j, i] + alpha_k*(j == ind)\n",
    "\n",
    "        curr_aptamer = new_aptamer\n",
    "        # Round the aptamer and find the resulting string\n",
    "    \n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    aptamer_string = stringify(rounded_aptamer)\n",
    "    \n",
    "    a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "    a = a.permute((2, 1, 0))\n",
    "    final_score = update(a, p, None, single_alphabet=False)\n",
    "    #sns.lineplot([i for i in range(len(scores))], scores)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Test the actual aptamer\n",
    "    actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "    a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "    actual_score = update(a, p, None, single_alphabet=False)\n",
    "    \n",
    "    return final_score, actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4225]], device='cuda:0', grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "frank_wolfe(peptide, actual_aptamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:23<00:00,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Found:  77\n",
      "Unoptimal Found:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/fw_pairs.txt') as f:\n",
    "    optimal_count = 0\n",
    "    unoptimal_count = 0\n",
    "    found_scores = []\n",
    "    actual_scores = []\n",
    "    for i, pair in enumerate(tqdm.tqdm(f.readlines()[:100])):\n",
    "        apt, pep = pair.split()\n",
    "        found_score, actual_score = frank_wolfe(pep, apt)\n",
    "        if found_score >= actual_score:\n",
    "            optimal_count += 1\n",
    "        else:\n",
    "            unoptimal_count += 1\n",
    "        found_scores.append(found_score)\n",
    "        actual_scores.append(actual_score)\n",
    "print(\"Optimal Found: \", optimal_count)\n",
    "print(\"Unoptimal Found: \", unoptimal_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Results of the Frank Wolfe Algorithm on 50 items from the higher quality dataset')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.stripplot(found_scores, color='blue', label='FW Found Scores')\n",
    "sns.stripplot(actual_scores, color='red', label='Actual Aptamer Scores')\n",
    "plt.legend()\n",
    "plt.title(\"Results of the Frank Wolfe Algorithm on 50 items from the higher quality dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
