{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "N_GRAM = 3\n",
    "ngram = True #whether to use ngram encoding or matrix encoding\n",
    "k = 10000 # number of samples used to calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids for aptamer\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids for peptide\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of {token: index} for all possible ngrams with size <= N_GRAM\n",
    "def get_vocab():\n",
    "    def generator(letters):\n",
    "        letters = \"\".join(letters)\n",
    "        for n in range(1, N_GRAM+1):\n",
    "            for item in itertools.product(letters, repeat=n):\n",
    "                yield \"\".join(item)\n",
    "    a = [i for i in generator(na_list)]\n",
    "    p = [i for i in generator(aa_list)]\n",
    "    vocab_apt = {a[i]: i for i in range(len(a))}\n",
    "    vocab_pep = {p[i]: i for i in range(len(p))}\n",
    "    return vocab_apt, vocab_pep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_apt, vocab_pep = get_vocab()\n",
    "VOCAB_SIZE_APT = len(vocab_apt) #84\n",
    "VOCAB_SIZE_PEP = len(vocab_pep) #8420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, Encoding, Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates the broken-down tokens of the given sequence with N_GRAM\n",
    "def ngrams_iterator(seq):\n",
    "    for char in seq:\n",
    "        yield char\n",
    "    for n in range(2, N_GRAM + 1):\n",
    "        for char in zip(*[seq[i:] for i in range(n)]):\n",
    "            yield ''.join(char)\n",
    "\n",
    "            \n",
    "# Encodes aptamer/peptide to a binary vector, 1 being the correspoinding ngram is present\n",
    "def binary_encoding(apt, pep):\n",
    "    x = torch.zeros(VOCAB_SIZE_APT)\n",
    "    for i in ngrams_iterator(apt):\n",
    "        x[vocab_apt[i]] = 1\n",
    "    y = torch.zeros(VOCAB_SIZE_PEP)\n",
    "    for i in ngrams_iterator(pep):\n",
    "        y[vocab_pep[i]] = 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Construct dataset with specified encoding\n",
    "def construct_dataset(nGram=ngram):\n",
    "    with open(aptamer_dataset_file, 'r') as f:\n",
    "        aptamer_data = json.load(f)\n",
    "    ds = []\n",
    "    for aptamer in aptamer_data:\n",
    "        peptides = aptamer_data[aptamer]\n",
    "        if aptamer == \"CTTTGTAATTGGTTCTGAGTTCCGTTGTGGGAGGAACATG\": #took out aptamer control\n",
    "            continue\n",
    "        for peptide, _ in peptides:\n",
    "            peptide = peptide.replace(\"_\", \"\") #removed stop codons\n",
    "            if \"RRRRRR\" in peptide: #took out peptide control\n",
    "                continue\n",
    "            if len(aptamer) == 40 and len(peptide) == 8: #making sure right length\n",
    "                pep_pmf = get_y_pmf(peptide)\n",
    "                if nGram:\n",
    "                    apt, pep = binary_encoding(aptamer, peptide)\n",
    "                else:\n",
    "                    apt, pep = aptamer, peptide #matrix encoding\n",
    "                ds.append((apt, pep, pep_pmf))\n",
    "    ds = list(set(ds)) #removed duplicates\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Sample x from P_X (assume apatamers follow uniform) with specified encoding\n",
    "def get_x(nGram=ngram):\n",
    "    x_idx = np.random.randint(0, 4, 40)\n",
    "    x = \"\"\n",
    "    for i in x_idx:\n",
    "        x += na_list[i]\n",
    "    if nGram:\n",
    "        apt = torch.zeros(VOCAB_SIZE_APT)\n",
    "        for i in ngrams_iterator(x):\n",
    "            apt[vocab_apt[i]] = 1\n",
    "    else:\n",
    "        apt = x #matrix encoding\n",
    "    return apt\n",
    "\n",
    "\n",
    "# Sample y from P_y (assume peptides follow NNK) with specified encoding and its pmf \n",
    "def get_y(nGram=ngram):\n",
    "    y_idx = np.random.choice(20, 7, p=pvals)\n",
    "    y = \"M\"\n",
    "    for i in y_idx:\n",
    "        y += aa_list[i]\n",
    "    y_pmf = get_y_pmf(y)\n",
    "    if nGram:\n",
    "        pep = torch.zeros(VOCAB_SIZE_PEP)\n",
    "        for i in ngrams_iterator(y):\n",
    "            pep[vocab_pep[i]] = 1\n",
    "    else:\n",
    "        pep = y #matrix encoding\n",
    "    return pep, y_pmf\n",
    "\n",
    "\n",
    "# S'(train/test) contains S_train/S_test with double the size of S_train/S_test\n",
    "def get_S_prime(kind=\"train\", nGram=ngram):\n",
    "    if kind == \"train\":\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    k = len(dset)\n",
    "    S_prime_dict = dict.fromkeys(dset, 0) #indicator 0 means in S\n",
    "    for _ in range(k):\n",
    "        apt = get_x(nGram)\n",
    "        pep, pmf = get_y(nGram)\n",
    "        pair = (apt, pep, pmf)\n",
    "        S_prime_dict[pair] = 1 #indicator 1 means not in S\n",
    "    S_prime = [[k,int(v)] for k,v in S_prime_dict.items()] \n",
    "    np.random.shuffle(S_prime)\n",
    "    return S_prime\n",
    "\n",
    "\n",
    "# S new contains unseen new examples\n",
    "def get_S_new(k, nGram=ngram):\n",
    "    S_new = []\n",
    "    for i in range(k):\n",
    "        pair = (get_x(nGram), get_y(nGram))\n",
    "        S_new.append(pair)\n",
    "    np.random.shuffle(S_new)\n",
    "    return S_new\n",
    "    \n",
    "    \n",
    "# Returns pmf of an aptamer\n",
    "def get_x_pmf():\n",
    "    return 0.25**40\n",
    "\n",
    "\n",
    "# Returns pmf of a peptide\n",
    "def get_y_pmf(y):\n",
    "    pmf = 1\n",
    "    for char in y[1:]: #skips first char \"M\"\n",
    "        pmf *= aa_dict[char]\n",
    "    return pmf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aptamer_dataset_file = \"../data/aptamer_dataset.json\"\n",
    "S = construct_dataset(nGram=ngram)\n",
    "n = len(S)\n",
    "m = int(0.8*n) #length of S_train\n",
    "S_train = S[:m]\n",
    "S_test = S[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"S_train\", S_prime_train[0])\n",
    "print(\"S_test\", S_prime_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_prime_train = get_S_prime(\"train\") #use for sgd \n",
    "S_prime_test = get_S_prime(\"test\") #use for sgd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"S_prime_train\", S_prime_train[:2])\n",
    "print(\"S_prime_test\", S_prime_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_new = get_S_new(10*n) #use for eval\n",
    "train_ds = np.hstack((S_train, S_prime_train[:len(S_prime_train)//2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"S_new\", S_new[:2])\n",
    "print(\"train_ds\", train_ds[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(nn.Module):\n",
    "    def __init__(self, vocab_size_apt, vocab_size_pep):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_apt = nn.Linear(vocab_size_apt, 1000)\n",
    "        self.fc_pep = nn.Linear(vocab_size_pep, 1000)\n",
    "        self.fc = nn.Linear(2000, 1)\n",
    "\n",
    "    def forward(self, apt, pep):\n",
    "        fc_apt = self.fc_apt(apt)\n",
    "        apt = self.relu(fc_apt)\n",
    "        fc_pep = self.fc_pep(pep)\n",
    "        pep = self.relu(fc_pep)\n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the output of the ngram model for a pair (aptamer, peptide)\n",
    "def get_ngram_out(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "# Generates the samples used to calculate loss\n",
    "def loss_samples(k, ds='train'): # S_train/S_test\n",
    "    if ds == 'train':\n",
    "        dset = S_train\n",
    "    else:\n",
    "        dset = S_test\n",
    "    pairs = []\n",
    "    for (apt, pep, _) in dset[:k]:\n",
    "        pairs.append((apt, pep))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Generates the samples used to calculate loss from S_prime_train/S_prime_test\n",
    "def prime_loss_samples(k, ds='train'):\n",
    "    if ds == \"train\":\n",
    "        dset = S_prime_train[len(S_prime_train)//2:]    \n",
    "    else:\n",
    "        dset = S_prime_test[len(S_prime_test)//2:]\n",
    "    pairs = []\n",
    "    for (apt, pep, pmf), ind in dset[:k]:\n",
    "        pairs.append((apt, pep, ind, pmf))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# First term of the loss\n",
    "def get_log_out(dataset='train'):\n",
    "    outs = []\n",
    "    if dataset == 'train':\n",
    "        dset = train_loss_samples\n",
    "    else:\n",
    "        dset = test_loss_samples\n",
    "    for (apt, pep) in dset:\n",
    "        out = get_ngram_out(apt, pep)\n",
    "        outs.append(torch.log(out).cpu().detach().numpy().flatten()[0])\n",
    "    return np.average(outs)\n",
    "\n",
    "\n",
    "# Second term of loss\n",
    "def get_out_prime(ds=\"train\"):\n",
    "    outs = []\n",
    "    if ds == \"train\":\n",
    "        dset = prime_train_loss_samples\n",
    "        leng = m\n",
    "    else:\n",
    "        dset = prime_test_loss_samples\n",
    "        leng = n-m\n",
    "    for (apt, pep, ind, pmf) in dset:\n",
    "        x = apt.cuda()\n",
    "        y = pep.cuda()\n",
    "        out = model(x, y)\n",
    "        if ind == 0:\n",
    "            factor = (2*leng*get_x_pmf()*pmf)/(1+leng*get_x_pmf()*pmf)\n",
    "        else:\n",
    "            factor = 2\n",
    "        out_is = out.cpu().detach().numpy().flatten()[0] * factor\n",
    "        outs.append(out_is)\n",
    "    return np.average(outs)\n",
    "\n",
    "\n",
    "## Plotting functions\n",
    "def plot_loss(train_loss, test_loss, i, j, lamb, gamma):\n",
    "    plt.plot(train_loss, 'b', label='Train loss')\n",
    "    plt.plot(test_loss, 'y', label='Test loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Loss after ' +  str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_recall(train_recall, test_recall, new_specificity, i, j, lamb, gamma):\n",
    "    plt.plot(train_recall, 'b', label='Train recall')\n",
    "    plt.plot(test_recall, 'y', label='Test recall')\n",
    "    plt.plot(new_specificity, 'r', label='New specificity')\n",
    "    plt.ylabel(\"Recall (%)\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.title('Recall/specificity after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cdf(train_cdf, test_cdf, i, j, lamb, gamma):\n",
    "    plt.plot(train_cdf, 'b', label='Train CDF')\n",
    "    plt.plot(test_cdf, 'y', label='Test CDF')\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.xlabel(\"Most recent 10,000 samples\")\n",
    "    plt.title('CDF after ' + str(i) + \" iterations, \" + str(j) + \" epochs, \" + 'lambda =%.5f' % lamb  + ' gamma =%.5f' % gamma)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def histogram(eval_scores, train_scores, test_scores):\n",
    "    f, axes = plt.subplots(2, 2, figsize=(7, 7), sharex=True)\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0,)\n",
    "    sns.distplot(eval_scores , color=\"skyblue\", label='New: not in dataset', ax=axes[0, 0])\n",
    "    sns.distplot(train_scores , color=\"gold\", label='Train: in dataset', ax=axes[1, 0])\n",
    "    sns.distplot(test_scores, color='red', label='Test: in the dataset', ax=axes[0, 1])\n",
    "    axes[0,0].set_title(\"New: not in dataset\")\n",
    "    axes[1,0].set_title(\"Train: in dataset\")\n",
    "    axes[0,1].set_title(\"Test: in dataset\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_samples = loss_samples(k, 'train')\n",
    "test_loss_samples = loss_samples(k, 'test')\n",
    "prime_train_loss_samples = prime_loss_samples(k, 'train')\n",
    "prime_test_loss_samples = prime_loss_samples(k, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(model_name,\n",
    "        lamb=10, #hyperparam\n",
    "        gamma=1e-3, #step size\n",
    "        save_checkpoints=False): #save checkpoints\n",
    "    \n",
    "    optim = SGD(model.parameters(), lr=gamma)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_losses = []\n",
    "        train_recalls = []\n",
    "        train_recall_outputs = [] \n",
    "\n",
    "        test_losses = []\n",
    "        test_recalls = []\n",
    "        test_recall_outputs = []\n",
    "\n",
    "        new_outputs = []\n",
    "        new_specificities = []\n",
    "\n",
    "        train_correct = 0\n",
    "        test_correct = 0\n",
    "        new_correct = 0\n",
    "        \n",
    "        for i, (aptamer, peptide, (aptamer_prime, peptide_prime, pep_prime_pmf), indicator) in enumerate(tqdm.tqdm(train_ds)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            model.train()\n",
    "            optim.zero_grad() \n",
    "            out = get_ngram_out(aptamer, peptide) #get S_train output/score\n",
    "            log_out = torch.log(out) \n",
    "            \n",
    "            train_score = out.cpu().detach().numpy().flatten()[0] \n",
    "            if train_score > 0.6:\n",
    "                train_correct += 1 \n",
    "            train_recall_outputs.append(train_score) \n",
    "\n",
    "            optim.zero_grad() \n",
    "            out_prime = get_ngram_out(aptamer_prime, peptide_prime) #get score from S_prime_train\n",
    "            if indicator == 0:\n",
    "                factor = (2*m*get_x_pmf()*pep_prime_pmf)/(1+m*get_x_pmf()*pep_prime_pmf)\n",
    "            else:\n",
    "                factor = 2\n",
    "            out_prime = out_prime*factor #adjust for IS\n",
    "            print(\"Obj first part: \", out_prime.cpu().detach().numpy().flatten()[0]*lamb*indicator)\n",
    "            print(\"Obj second part: \", log_out.cpu().detach().numpy().flatten()[0])\n",
    "            (lamb*indicator*out_prime - log_out).backward(retain_graph=True) \n",
    "            optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            apt_test, pep_test = S_test[i%(n-m)][0], S_test[i%(n-m)][1]\n",
    "            apt_test = apt_test.cuda()\n",
    "            pep_test = pep_test.cuda()\n",
    "            test_score = model(apt_test, pep_test).cpu().detach().numpy().flatten()[0]\n",
    "            test_recall_outputs.append(test_score) \n",
    "            if test_score > 0.6:\n",
    "                test_correct += 1 \n",
    "\n",
    "            #generate 10 unseen examples from S_new as compared 1 example from S_train/S_test for cdfs\n",
    "            for apt_new, pep_new in S_new[10*i:10*(i+1)]:\n",
    "                apt_new = apt_new.cuda()\n",
    "                pep_new = pep_new.cuda()\n",
    "                new_score = model(apt_new, pep_new).cpu().detach().numpy().flatten()[0] #get unknown score\n",
    "                new_outputs.append(new_score)\n",
    "                if new_score < 0.3:\n",
    "                    new_correct += 1\n",
    "\n",
    "            if i % 25 == 0:\n",
    "                train_loss = lamb*get_out_prime(\"train\") - get_log_out('train') #training loss\n",
    "                print(\"Train loss first part: \", lamb*get_out_prime(\"train\"))\n",
    "                print(\"Train loss second part: \", get_log_out('train'))\n",
    "                test_loss = (m/(n-m))*lamb*get_out_prime(\"test\") - get_log_out('test') #test loss\n",
    "                print(\"Test loss first part: \", lamb*get_out_prime(\"test\"))\n",
    "                print(\"Test loss second part: \", get_log_out('test'))\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                train_recall = 100*train_correct/i #training recall\n",
    "                train_recalls.append(train_recall) \n",
    "                test_recall = 100*test_correct/i #test recall\n",
    "                test_recalls.append(test_recall)\n",
    "                new_specificity = 100*new_correct/(i*10) #generated dataset specificity\n",
    "                new_specificities.append(new_specificity)\n",
    "                if i > 1000:\n",
    "                    train_score = np.asarray(new_outputs[-10000:] + train_recall_outputs[-1000:]) \n",
    "                    test_score = np.asarray(new_outputs[-10000:] + test_recall_outputs[-1000:])\n",
    "                else:\n",
    "                    train_score = np.asarray(new_outputs + train_recall_outputs) #combine train and unknown scores\n",
    "                    test_score = np.asarray(new_outputs + test_recall_outputs) #combibne test and unknown scores\n",
    "                train_cdf = np.cumsum(train_score)/np.sum(train_score) #train cdf\n",
    "                test_cdf = np.cumsum(test_score)/np.sum(test_score) #test cdf\n",
    "\n",
    "\n",
    "            if i % 250 == 0:\n",
    "                plot_recall(train_recalls, test_recalls, new_specificities, i, epoch, lamb, gamma)\n",
    "                plot_loss(train_losses, test_losses, i, epoch, lamb, gamma)\n",
    "                plot_cdf(train_cdf, test_cdf, i, epoch, lamb, gamma)\n",
    "                histogram(new_outputs[-1000:], train_recall_outputs[-1000:], test_recall_outputs[-1000:])\n",
    "                print(\"New score: \", np.average(new_outputs[-1000:]))\n",
    "                print(\"Train score: \", np.average(train_score[-1000:]))\n",
    "                print(\"Test score: \", np.average(test_score[-1000:]))\n",
    "        # Save after every epoch\n",
    "        if save_checkpoints:\n",
    "            checkpoint_name = '../models/model_checkpoints/' + str(model_name) + '_lambda=' + str(lamb) + '_gamma=' + str(gamma) + '.pth'\n",
    "            torch.save({'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer_state_dict': optim.state_dict()}, checkpoint_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "gammas = [1e-3]\n",
    "lambdas = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for g in gammas:\n",
    "    for l in lambdas:\n",
    "        model = NGram(VOCAB_SIZE_APT, VOCAB_SIZE_PEP)\n",
    "        model.apply(weights_init)\n",
    "        model.cuda()\n",
    "        sgd(model_name=\"simple_ngram\", gamma=g, lamb=l, save_checkpoints=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
