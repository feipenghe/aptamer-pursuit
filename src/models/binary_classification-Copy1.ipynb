{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "hydrophobicity = {'G': 0, 'A': 41, 'L':97, 'M': 74, 'F':100, 'W':97, 'K':-23, 'Q':-10, 'E':-31, 'S':-5, 'P':-46, 'V':76, 'I':99, 'C':49, 'Y':63, 'H':8, 'R':-14, 'N':-28, 'D':-55, 'T':13}\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        def construct_binary_dataset(filepath):\n",
    "            with open(filepath, 'r') as f:\n",
    "                aptamer_data = json.load(f)\n",
    "            ds = []\n",
    "            for aptamer in aptamer_data:\n",
    "                peptides = aptamer_data[aptamer]\n",
    "                for peptide in peptides:\n",
    "                    ds.append((aptamer, peptide, 1))\n",
    "                    ds.append((get_x(), get_y(), 0))\n",
    "            ds = list(set(ds)) #removed duplicates, random order\n",
    "            return ds\n",
    "\n",
    "        # Sample x from P_X (assume apatamers follow uniform)\n",
    "        def get_x():\n",
    "            x_idx = np.random.randint(0, 4, 40)\n",
    "            x = \"\"\n",
    "            for i in x_idx:\n",
    "                x += na_list[i]\n",
    "            return x\n",
    "\n",
    "        # Sample y from P_y (assume peptides follow NNK)\n",
    "        def get_y():\n",
    "            y_idx = np.random.choice(20, 7, p=pvals)\n",
    "            y = \"M\"\n",
    "            for i in y_idx:\n",
    "                y += aa_list[i]\n",
    "            return y\n",
    "\n",
    "        self.binary_ds=construct_binary_dataset(filepath)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.binary_ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = [apt, pep, label]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return(self.binary_ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-fdd26705bfd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# binary_train = binary_ds[:m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# binary_val = binary_ds[m:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbinary_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbinary_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-be9c35602f65>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "binary_ds=BinaryDataset(filepath=\"../data/aptamer_dataset.json\")\n",
    "# n = len(binary_ds)\n",
    "# m = int(0.8*n) #length of train\n",
    "# binary_train = binary_ds[:m]\n",
    "# binary_val = binary_ds[m:]\n",
    "binary_train = binary_ds[:1000]\n",
    "binary_val = binary_ds[-200:]\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(binary_train, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(binary_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.CenterCrop(size=128),\n",
    "    transforms.Lambda(lambda x: myimresize(x, (128, 128))),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0., 0., 0.), (6, 6, 6))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 100) \n",
    "        self.fc_apt_2 = nn.Linear(100, 50)\n",
    "        self.fc_apt_3 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 50)\n",
    "        self.fc_pep_2 = nn.Linear(50, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearConv1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearConv1d, self).__init__()\n",
    "        self.name = \"LinearConv1d\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 100, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(100, 50, 3) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 5, 1) \n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 50, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(50, 25, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_3, self.maxpool, self.relu,\n",
    "                                     self.cnn_apt_4, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(30, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        #print(apt.size())\n",
    "        #print(pep.size())\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSimple, self).__init__()\n",
    "        self.name = \"ConvNetSimple\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 500, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(500, 250, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(250, 125, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(125, 50, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 500, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(500, 250, 3)\n",
    "        self.cnn_pep_3 = nn.Conv1d(250, 50, 1)\n",
    "       \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [BATCH_SIZE, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_1(apt)))\n",
    "        #print(\"apt1: \", apt.size())\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_2(apt)))\n",
    "        #print(\"apt2: \", apt.size())\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        #print(\"apt3: \", apt.size())\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        #print(\"apt4: \", apt.size()) (1, 50, 1)\n",
    "        \n",
    "        # pep input size [BATCH_SIZE, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "\n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        #print(\"pep1: \", pep.size())\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_2(pep)))\n",
    "        #print(\"pep2: \", pep.size())\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_3(pep)))\n",
    "        #print(\"pep3: \", pep.size()) (1, 50, 1)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        #print(\"cat: \", x.size())\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetComplex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetComplex, self).__init__()\n",
    "        self.name = \"ConvNetComplex\"\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 1000, 3) \n",
    "        self.cnn_apt_2 = nn.Conv1d(1000, 800, 3)\n",
    "        self.cnn_apt_3 = nn.Conv1d(800, 600, 3)\n",
    "        self.cnn_apt_4 = nn.Conv1d(600, 400, 3)\n",
    "        self.cnn_apt_5 = nn.Conv1d(400, 200, 1)\n",
    "        self.cnn_apt_6 = nn.Conv1d(200, 100, 1)\n",
    "        \n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 1000, 3)\n",
    "        self.cnn_pep_2 = nn.Conv1d(1000, 800, 1)\n",
    "        self.cnn_pep_3 = nn.Conv1d(800, 500, 1)\n",
    "        self.cnn_pep_4 = nn.Conv1d(500, 250, 1)\n",
    "        self.cnn_pep_5 = nn.Conv1d(250, 100, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)         \n",
    "        self.fc1 = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        # apt input size [BATCH_SIZE, 40, 4]\n",
    "        apt = apt.permute(0, 2, 1)\n",
    "        \n",
    "        apt = self.relu(self.cnn_apt_1(apt))\n",
    "        apt = self.relu(self.cnn_apt_2(apt))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_3(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_4(apt)))\n",
    "        apt = self.pool1(self.relu(self.cnn_apt_5(apt)))        \n",
    "        apt = self.pool1(self.relu(self.cnn_apt_6(apt)))\n",
    "        \n",
    "\n",
    "        # pep input size [BATCH_SIZE, 8, 20]\n",
    "        pep = pep.permute(0, 2, 1)\n",
    "        \n",
    "        pep = self.relu(self.cnn_pep_1(pep))\n",
    "        pep = self.relu(self.cnn_pep_2(pep))\n",
    "        pep = self.relu(self.cnn_pep_3(pep))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_4(pep)))\n",
    "        pep = self.pool1(self.relu(self.cnn_pep_5(pep)))\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='sigmoid')\n",
    "        nn.init.zeros_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide'):\n",
    "    if seq_type == 'peptide':\n",
    "        letters = aa_list\n",
    "    else:\n",
    "        letters = na_list\n",
    "    one_hot = np.zeros((len(sequence), len(letters)))\n",
    "    for i in range(len(sequence)):\n",
    "        char = sequence[i]\n",
    "        for _ in range(len(letters)):\n",
    "            idx = letters.index(char)\n",
    "            one_hot[i][idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label): \n",
    "    apt = one_hot(apt, seq_type='aptamer') #(40, 4)\n",
    "    pep = one_hot(pep, seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[0], apt.shape[1]))).to(device) #(1, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[0], pep.shape[1]))).to(device) #(1, 8, 20)\n",
    "    label = torch.FloatTensor([label]).to(device)\n",
    "    return apt, pep, label\n",
    "\n",
    "# Convert a pair to one-hot tensor\n",
    "def batch_convert(apt, pep, label): \n",
    "    for i in range(len(apt)):\n",
    "        apt[i] = one_hot(apt[i], seq_type='aptamer') #(40, 4)\n",
    "        pep[i] = one_hot(pep[i], seq_type='peptide') #(8, 20)\n",
    "    apt = torch.FloatTensor(np.reshape(apt, (BATCH_SIZE, apt.shape[0], apt.shape[1]))).to(device) #(BATCH_SIZE, 40, 4)\n",
    "    pep = torch.FloatTensor(np.reshape(pep, (BATCH_SIZE, pep.shape[0], pep[i].shape[1]))).to(device) #(1, 8, 20)\n",
    "    \n",
    "    label = torch.FloatTensor(label).to(device)\n",
    "    return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y):\n",
    "    x.requires_grad=True\n",
    "    y.requires_grad=True\n",
    "    out = model(x, y)\n",
    "    return out\n",
    "\n",
    "## Plotting functions\n",
    "def plot_loss(iters, train_losses, val_losses, model_name):\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/loss.png' %model_name, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(iters, train_acc, val_acc, model_name):\n",
    "    plt.title(\"Training Accuracy Curve\")\n",
    "    plt.plot(train_acc, label=\"Train\")\n",
    "    plt.plot(val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"%d Iterations\" %iters)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('plots/binary/%s/accuracy.png' %model_name, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model=LinearNet(), \n",
    "               train=train_loader, \n",
    "               val=val_loader,\n",
    "               num_epochs=10,\n",
    "               run_from_checkpoint=None, \n",
    "               save_checkpoints=None):\n",
    "    \n",
    "    if run_from_checkpoint is not None:\n",
    "        checkpointed_model = run_from_checkpoint\n",
    "        checkpoint = torch.load(checkpointed_model)\n",
    "        optimizer = SGD(model.parameters(), lr=5e-3)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        init_epoch = checkpoint['epoch']\n",
    "        print(\"Reloading model: \", model.name, \" at epoch: \", init_epoch)\n",
    "    else:\n",
    "        model.apply(weights_init)\n",
    "    \n",
    "    train_losses, val_losses, train_losses_avg, val_losses_avg, train_acc, val_acc = [], [], [], [], [], []\n",
    "    iters, train_correct, val_correct = 0, 0, 0\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=5e-3)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5) #Decays lr by gamma factor every step_size epochs. \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting epoch: %d\" %epoch)\n",
    "        for (apt, pep, label) in train:\n",
    "            model_name = model.name\n",
    "            model.train()\n",
    "            print(apt, pep, label)\n",
    "            a, p, l = batch_convert(apt, pep, label)\n",
    "            train_score = update(a, p)\n",
    "            if (train_score.item() >= 0.5 and label == 1.0) or (train_score.item() <= 0.5 and label == 0.0):\n",
    "                train_correct += 1\n",
    "            iters += 1\n",
    "            train_loss = criterion(train_score, l) \n",
    "            train_losses.append(train_loss.item())\n",
    "            train_loss.backward()               \n",
    "            scheduler.step()              \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if iters % 10 == 0:\n",
    "                train_acc.append(100*train_correct/iters)\n",
    "                train_losses_avg.append(np.average(train_losses[-10:]))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            a_val, p_val, l_val = convert(val[iters%(n-m)][0], val[iters%(n-m)][1], val[iters%(n-m)][2])\n",
    "            val_score = model(a_val, p_val)\n",
    "            if (val_score.item() >= 0.5 and val[iters%(n-m)][2] == 1.0) or (val_score.item() <= 0.5 and val[iters%(n-m)][2] == 0.0):\n",
    "                val_correct += 1\n",
    "            val_loss = criterion(val_score, l_val) \n",
    "            val_losses.append(val_loss.item())\n",
    "            if iters % 10 == 0:\n",
    "                val_acc.append(100*val_correct/iters)\n",
    "                val_losses_avg.append(np.average(val_losses[-10:]))\n",
    "\n",
    "            if iters % 100 == 0:\n",
    "                plot_loss(iters, train_losses_avg, val_losses_avg, model_name)\n",
    "                plot_accuracy(iters, train_acc, val_acc, model_name)\n",
    "                print(\"Training Accuracy at epoch %d: {}\".format(train_acc[-1]) %epoch)\n",
    "                print(\"Validation Accuracy epoch %d: {}\".format(val_acc[-1]) %epoch)\n",
    "        if save_checkpoints is not None:\n",
    "            print(\"Saving to: \", save_checkpoints)\n",
    "            checkpoint_name = save_checkpoints\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(), \n",
    "                        'optimizer_state_dict': optimizer.state_dict()}, checkpoint_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0\n",
      "('TATCTCTTAAAAATGGCGAGAATACAAAGATAGTATATCA', 'GTAGAACTCATGATATTAGTATCTCGTGTTACAGTCATAT', 'CGATTTAATAGAACGTGTCATTAGGAGTCTTCCGCAACCA', 'CCCTTGATAGGATACATTGCCAGACTAACCATGATAGGTG', 'ATTCAATCACTTTAATTCGGAATCATCAATTGTGTCTCTC', 'TGACTAACCGGGCCCGTACGGGTTTGGTGGGCTACAGTGA', 'GTTAATATGCTTTTCGAAGAATATAGATTAACACTAAAAA', 'ACGTGATTGCCCCAACCTTTGGCTACTACCACTTTAAGTC', 'GCTTTTGTTTCAAATACGCGGAAGGGTTCTTATTACTCTC', 'TGTTTGAGGCGTGATAGGTGGTTCGCCCACTGGGTCAAGA', 'GAGTTCGACGGATCACTTCTAAAAGTCATCACATACTTAG', 'CTGACATTGTGCCCAGGCTTGGTTAAACGACTGGACTCTG', 'CGAATTCATACATTTAGTGATAAGTAGGTCAGGTGTCCTA', 'AAAGTGTCATACATCTACGAAATCGAGGCAGGTGATGGCT', 'CACTGGCTTTCTTATTTGTGTTGCTAATGCCTGTGCCCCA', 'TACGAGCGAGATCCACCTGTCTAGTCAGAACAGTATGAGA', 'GCTGTGCGTTCTATCACGAGAGCTGCGTCCCCTATTTATG', 'ATGGCCAGACCAGGCTGAAAAAACGGATACCAGCAAGTAC', 'AATCGTCTAACATCCCTATCACATTGGTAGTTGGGTGGGG', 'GACTATTGTTACTTTAGTCGCGTGCCTATGCGAGGGTGTC', 'ACCGTGTTATACGCGCGTTTACCGTGGGTCGAGACGCTCC', 'AGCTGTTTTGTGCGCCAACACCGAATTCTCAGTTCGCCAA', 'CTACGACGCCGGGCTGGTCGCAGTCCTGACAGACACGCCC', 'AATGTAAGGTACTCGCTACGGCCCGTTAGATGCTGTAACT', 'TGTACGTCAGGTTCCTCCTAAGACCTCTCATACTATCGGA', 'TACAGAGGCTCGTGCGATACCGTGCAACAAGCACCTTCGA', 'TATTTGCCGGCGACTATACAAGAGAGCGTTAGGGCTTCGG', 'CACCCGCTAGCTTACTAATTCACCAGTTCCTATTCTCTCT', 'CATTAATACCAATTTAGTCGTCACTATGGGACCCGCTTAA', 'TGCTACCAATTGCGCATCCATGAGGAAATCTCAAATGCTT', 'TGTGTCTTCCGGTTTATATCGGTCAGGACCCGCGTTATGT', 'AATCGGAGAGACGGCCTCAGGCAGAACGATGAACATCGAG') ('MGTHSLWA', 'MHMLDMFR', 'MRMMYLRC', 'MLMTLVSS', 'MYPVSWRC', 'MRVPGLQA', 'MRLGPGPD', 'MRGSCAMF', 'MHAPRLGS', 'MVSYMPSF', 'MRRPSLGT', 'MMRTGQSM', 'METGRFQR', 'MRFAPLPQ', 'MSPLPRVT', 'MPVGAMSE', 'MRHLYQRS', 'MTPSVIKQ', 'MKGPWLSG', 'MVLHAGMV', 'MTEQRCAL', 'MRSSWTWV', 'MAGYSQGP', 'MPIVPCGC', 'MSAGCLWG', 'MMDPNGQL', 'MKPGCRPY', 'MLKEGSKR', 'MGMCVLIV', 'MQPPDWAG', 'MEHWGTRA', 'MSPNPAQS') tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'TATCTCTTAAAAATGGCGAGAATACAAAGATAGTATATCA' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-380310d3bf3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_checkpoints/binary/%s/05042020.pth'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-99a89cc8b82b>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(model, train, val, num_epochs, run_from_checkpoint, save_checkpoints)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-825bde1f680a>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(apt, pep, label)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Convert a pair to one-hot tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'aptamer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(40, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'peptide'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(8, 20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mapt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(1, 40, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-825bde1f680a>\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(sequence, seq_type)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mone_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'TATCTCTTAAAAATGGCGAGAATACAAAGATAGTATATCA' is not in list"
     ]
    }
   ],
   "source": [
    "model = LinearNet()\n",
    "model_name = model.name\n",
    "model.to(device)\n",
    "checkpoint = None\n",
    "save_path = 'model_checkpoints/binary/%s/05042020.pth' %model_name\n",
    "classifier(model, train_loader, val_loader, 10, checkpoint, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
