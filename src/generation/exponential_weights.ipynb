{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://blog.wouterkoolen.info/GDasEW/post.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "k = 10000\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "na_list = ['A', 'C', 'G', 'T'] #nucleic acids\n",
    "aa_list = ['R', 'L', 'S', 'A', 'G', 'P', 'T', 'V', 'N', 'D', 'C', 'Q', 'E', 'H', 'I', 'K', 'M', 'F', 'W', 'Y'] #amino acids\n",
    "NNK_freq = [0.09375]*3 + [0.0625]*5 + [0.03125]*13 #freq of 21 NNK codons including the stop codon\n",
    "sum_20 = 0.0625*5 + 0.09375*3 + 0.03125*12 #sum of freq without the stop codon\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*12 #normalize freq for 20 codons\n",
    "pvals = [0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11 + \\\n",
    "        [1- sum([0.09375/sum_20]*3 + [0.0625/sum_20]*5 + [0.03125/sum_20]*11)] \n",
    "        #adjust sum to 1 due to numerical issue\n",
    "aa_dict = dict(zip(aa_list, pvals))\n",
    "encoding_style = 'clipped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Original BLOSUM62 matrix\n",
    "original_blosum62 = {}\n",
    "with open('../blosum62.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split()\n",
    "        aa = split_line[0]\n",
    "        encoding = [int(x) for x in split_line[1:-3]]\n",
    "        original_blosum62[aa] = encoding\n",
    "blosum_matrix = np.zeros((20, 20))\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    sims = original_blosum62[aa]\n",
    "    for j, s in enumerate(sims):\n",
    "        blosum_matrix[i][j] = s   \n",
    "u, V = LA.eig(blosum_matrix)\n",
    "clipped_u = u\n",
    "clipped_u[clipped_u < 0] = 0\n",
    "lamb = np.diag(clipped_u)\n",
    "T = V\n",
    "clip_blosum62 = {}\n",
    "for i, aa in enumerate(original_blosum62.keys()):\n",
    "    clip_blosum62[aa] = np.dot(np.sqrt(lamb), V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumNet, self).__init__()\n",
    "        self.name = \"BlosumNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.cnn_apt_1 = nn.Conv1d(4, 25, 3, padding=2) \n",
    "        self.cnn_apt_2 = nn.Conv1d(25, 50, 3, padding=2) \n",
    "        self.cnn_apt_3 = nn.Conv1d(50, 25, 3, padding=2) \n",
    "        self.cnn_apt_4 = nn.Conv1d(25, 10, 3) \n",
    "        \n",
    "        # There are 20 channels\n",
    "        self.cnn_pep_1 = nn.Conv1d(20, 40, 3, padding=2)\n",
    "        self.cnn_pep_2 = nn.Conv1d(40, 80, 3, padding=2)\n",
    "        self.cnn_pep_3 = nn.Conv1d(80, 150, 3, padding=2)\n",
    "        self.cnn_pep_4 = nn.Conv1d(150, 50, 3, padding=2)\n",
    "        self.cnn_pep_5 = nn.Conv1d(50, 10, 3, padding=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(2) \n",
    "        \n",
    "        self.cnn_apt = nn.Sequential(self.cnn_apt_1, self.maxpool, self.relu, \n",
    "                                     self.cnn_apt_2, self.maxpool, self.relu)\n",
    "        self.cnn_pep = nn.Sequential(self.cnn_pep_1, self.maxpool, self.relu,\n",
    "                                     self.cnn_pep_2, self.maxpool, self.relu)\n",
    "        \n",
    "        self.fc1 = nn.Linear(790, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, 1)\n",
    "    \n",
    "    def forward(self, apt, pep):\n",
    "        apt = self.cnn_apt(apt)\n",
    "        pep = self.cnn_pep(pep)\n",
    "        \n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Expects peptides to be encoding according to BLOSUM62 matrix\n",
    "# Expects aptamers to be one hot encoded\n",
    "class BlosumLinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlosumLinearNet, self).__init__()\n",
    "        self.name = \"BlosumLinearNet\"\n",
    "        self.single_alphabet = False\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.reshape((-1, 1)).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        \n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.name = \"LinearNet\"\n",
    "        \n",
    "        self.fc_apt_1 = nn.Linear(160, 200) \n",
    "        self.fc_apt_2 = nn.Linear(200, 250)\n",
    "        self.fc_apt_3 = nn.Linear(250, 300)\n",
    "        \n",
    "        self.fc_pep_1 = nn.Linear(160, 200)\n",
    "        self.fc_pep_2 = nn.Linear(200, 250)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc_apt = nn.Sequential(self.fc_apt_1, self.fc_apt_2, self.fc_apt_3)\n",
    "        self.fc_pep = nn.Sequential(self.fc_pep_1, self.fc_pep_2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(550, 600)\n",
    "        self.fc2 = nn.Linear(600, 1)\n",
    "        \n",
    "    def forward(self, apt, pep):\n",
    "        apt = apt.view(-1, 1).T\n",
    "        pep = pep.view(-1, 1).T\n",
    "        apt = self.fc_apt(apt)\n",
    "        pep = self.fc_pep(pep)\n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        x = self.fc2(self.fc1(x))\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     5,
     36,
     74,
     86,
     168,
     170
    ]
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, kernel_size=3, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
    "                     padding=kernel_size//2, bias=True)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, kernel_size=kernel_size, stride=stride)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, kernel_size=3, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=kernel_size, stride=1,\n",
    "                               padding=kernel_size//2, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=1, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class VariableLengthPooling(nn.Module):\n",
    "    def forward(self, x, **kwargs):\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        # print(\"--------x--------\", x.size(), x)\n",
    "        # print(\"--------bounds--------\", bounds.size(), bounds)\n",
    "        cnt = torch.sum(bounds, dim=1)\n",
    "        # print(\"--------cnt--------\", cnt.size(), cnt)\n",
    "        # print(\"--------bmm--------\", torch.bmm(x, bounds).size(), torch.bmm(x, bounds))\n",
    "        out = torch.bmm(x, bounds) / cnt\n",
    "        # print(\"--------out--------\", out.size(), out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"Resnet\"\n",
    "        self.single_alphabet=True\n",
    "        self.inplanes = 192\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(48, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(48, 1)\n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, bounds=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        x = self.conv_merge(x)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        x = x.view(1, -1)\n",
    "        \n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class ResNetSeparated(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=46):\n",
    "        self.name = \"ResnetSeparated\"\n",
    "        self.single_alphabet=False\n",
    "        self.inplanes = 192\n",
    "        super(ResNetSeparated, self).__init__()\n",
    "        self.conv1_apt = nn.Conv1d(4, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_pep = nn.Conv1d(20, 192, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(192)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer0 = self._make_layer(block, 256, layers[0])\n",
    "        self.layer1 = self._make_layer(block, 256, layers[0], kernel_size=1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], kernel_size=5, stride=1)\n",
    "        #self.layer3 = self._make_layer(block, 256, layers[2], kernel_size=5, stride=1)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], kernel_size=1, stride=1)\n",
    "        #self.layer5 = self._make_layer(block, 512, layers[3], stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(96, 1)\n",
    "        \n",
    "        self.apt_initial = nn.Sequential(self.conv1_apt, self.bn1, self.relu)\n",
    "        self.pep_initial = nn.Sequential(self.conv1_pep, self.bn1, self.relu)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.conv_merge = nn.Conv1d(256 * block.expansion, num_classes,\n",
    "                                    kernel_size=3, stride=1, padding=1,\n",
    "                                    bias=True)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(self.layer0, self.layer1, self.layer2,\n",
    "                                         self.conv_merge)\n",
    "        self.vlp = VariableLengthPooling()\n",
    "        # self.avgpool = nn.AvgPool2d((5, 1), stride=1)\n",
    "        # self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.xavier_normal(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, kernel_size=3, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, kernel_size=kernel_size,\n",
    "                            stride=stride, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, kernel_size=kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, apt, pep, bounds=None):\n",
    "        apt = self.apt_initial(apt)\n",
    "        pep = self.pep_initial(pep)\n",
    "        \n",
    "        apt = self.conv_layers(apt)\n",
    "        pep = self.conv_layers(pep)\n",
    "        \n",
    "        apt = torch.squeeze(apt, dim=2)\n",
    "        pep = torch.squeeze(pep, dim=2)\n",
    "        \n",
    "        apt = apt.view(1, -1)\n",
    "        pep = pep.view(1, -1)\n",
    "        \n",
    "        x = torch.cat((apt, pep), 1)\n",
    "        \n",
    "        # I don't think I want variable length pooling\n",
    "        #x = self.vlp(x, bounds=bounds)\n",
    "        x = self.fc1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlosumNet(\n",
       "  (cnn_apt_1): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_2): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_3): Conv1d(50, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_apt_4): Conv1d(25, 10, kernel_size=(3,), stride=(1,))\n",
       "  (cnn_pep_1): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_2): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_3): Conv1d(80, 150, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_4): Conv1d(150, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (cnn_pep_5): Conv1d(50, 10, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_apt): Sequential(\n",
       "    (0): Conv1d(4, 25, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(25, 50, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (cnn_pep): Sequential(\n",
       "    (0): Conv1d(20, 40, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(40, 80, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc1): Linear(in_features=790, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinstantiate the model with the proper weights\n",
    "model = BlosumNet()\n",
    "model_name = model.name\n",
    "model_id = \"08042020_3\"\n",
    "model.to(device)\n",
    "checkpointed_model = '../model_checkpoints/binary/%s/%s.pth' % (model_name, model_id)\n",
    "checkpoint = torch.load(checkpointed_model)\n",
    "#optimizer = SGD(model.parameters(), lr=7e-4)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#init_epoch = checkpoint['epoch'] +1\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Encode the peptide appropriately\n",
    "def blosum62_encoding(sequence, seq_type='peptide', single_alphabet=False, style=encoding_style):\n",
    "    if single_alphabet:\n",
    "        pass\n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            encoding = []\n",
    "            for i in range(len(sequence)):\n",
    "                if style == \"clipped\":\n",
    "                    encoding.append(clip_blosum62[sequence[i]])\n",
    "                else:\n",
    "                    encoding.append(original_blosum62[sequence[i]])\n",
    "            encoding = np.asarray(encoding)\n",
    "        else:\n",
    "            #Translation\n",
    "            letters = na_list\n",
    "            encoding = np.zeros(len(sequence))\n",
    "            for i in range(len(sequence)):\n",
    "                char = sequence[i]\n",
    "                idx = letters.index(char)\n",
    "                encoding[i] = idx\n",
    "        return encoding \n",
    "\n",
    "## Takes a peptide and aptamer sequence and converts to one-hot matrix\n",
    "def one_hot(sequence, seq_type='peptide', single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        apt = sequence[0]\n",
    "        pep = sequence[1]\n",
    "        one_hot = np.zeros((len(apt) + len(pep), 24))\n",
    "        # Encode the aptamer first\n",
    "        for i in range(len(apt)):\n",
    "            char = apt[i]\n",
    "            for _ in range(len(na_list)):\n",
    "                idx = na_list.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "            \n",
    "        # Encode the peptide second\n",
    "        for i in range(len(pep)):\n",
    "            char = pep[i]\n",
    "            for _ in range(len(aa_list)):\n",
    "                idx = aa_list.index(char) + len(na_list)\n",
    "                one_hot[i+len(apt)][idx] = 1\n",
    "        \n",
    "        return one_hot       \n",
    "    else:\n",
    "        if seq_type == 'peptide':\n",
    "            letters = aa_list\n",
    "        else:\n",
    "            letters = na_list\n",
    "        one_hot = np.zeros((len(sequence), len(letters)))\n",
    "        for i in range(len(sequence)):\n",
    "            char = sequence[i]\n",
    "            for _ in range(len(letters)):\n",
    "                idx = letters.index(char)\n",
    "                one_hot[i][idx] = 1\n",
    "        return one_hot\n",
    "# Convert a pair to one-hot tensor\n",
    "def convert(apt, pep, label, single_alphabet=False): \n",
    "    if single_alphabet:\n",
    "        pair = translate([apt, pep], single_alphabet=True) #(2, 40)\n",
    "        pair = torch.FloatTensor(np.reshape(pair, (-1, pair.shape[0], pair.shape[1]))).to(device)\n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return pair, label\n",
    "    else:\n",
    "        #pep = blosum62_encoding(pep, seq_type='peptide') Blosum encoding\n",
    "        pep = one_hot(pep, seq_type='peptide')\n",
    "        apt = torch.FloatTensor(np.reshape(apt, (-1, apt.shape[1], apt.shape[0]))).to(device) #(1, 4, 40)\n",
    "        pep = torch.FloatTensor(np.reshape(pep, (-1, pep.shape[1], pep.shape[0]))).to(device) #(1, 20, 8)\n",
    "        \n",
    "        label = torch.FloatTensor([[label]]).to(device)\n",
    "        return apt, pep, label\n",
    "\n",
    "# Getting the output of the model for a pair (aptamer, peptide)\n",
    "def update(x, y, p, single_alphabet=False):\n",
    "    if single_alphabet:\n",
    "        p.requires_grad=True\n",
    "        p = p.to(device)\n",
    "        out = model(p)\n",
    "        return out\n",
    "    else:\n",
    "        x.requires_grad=True\n",
    "        y.requires_grad=False\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(x, y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Un one-hot the aptamer\n",
    "def stringify(oh):\n",
    "    # oh.shape = (1, 4, 40)\n",
    "    aptamer_string = \"\"\n",
    "    na_list = ['A', 'C', 'G', 'T']\n",
    "    for i in range(40):\n",
    "        column = oh[0, :, i]\n",
    "        ind = np.argmax(column)\n",
    "        aptamer_string += na_list[ind]\n",
    "    return aptamer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Round the resulting aptamer\n",
    "def round_aptamer(apt):\n",
    "    rounded_aptamer = np.zeros((1, 4, 40))\n",
    "    for i in range(40):\n",
    "        ind = np.argmax(apt[i, :, :])\n",
    "        rounded_aptamer[0, ind, i] = 1\n",
    "    return rounded_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def perturbed_uniform():\n",
    "    aptamer_0 = np.full((40, 4), 0.25)\n",
    "    for i in range(40):\n",
    "        # Perturb this column 25% of the time\n",
    "        if random.randint(0, 3) == 1:\n",
    "            # Find an index to perturb\n",
    "            inds = random.sample(range(0, 4), 2)\n",
    "            ind_0 = inds[0]\n",
    "            ind_1 = inds[1]\n",
    "            \n",
    "            # Find an amount to perturb\n",
    "            amt = random.randint(0, 25)\n",
    "            amt /= 100\n",
    "            \n",
    "            # Transaction\n",
    "            aptamer_0[i][ind_0] = 0.25 + amt\n",
    "            aptamer_0[i][ind_1] = 0.25 - amt\n",
    "    \n",
    "    return aptamer_0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_random_initialization():\n",
    "    aptamer_0 = np.zeros((40, 4))\n",
    "    for i in range(40):\n",
    "        d = 4\n",
    "        e = np.exp(np.random.rand(d))\n",
    "        e /= np.sum(e)\n",
    "        for j in range(4):\n",
    "            aptamer_0[i][j] = e[j]\n",
    "    return aptamer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climb(peptide, initial_aptamer_str, iterations=10):\n",
    "    initial_aptamer = one_hot(initial_aptamer_str, seq_type='aptamer', single_alphabet=False)\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    hill_climb_scores = []\n",
    "    curr_aptamer = initial_aptamer\n",
    "    curr_aptamer_str = initial_aptamer_str\n",
    "    best_score = 0\n",
    "    num_iter = 0\n",
    "    \n",
    "    # Go through the number of iterations\n",
    "    for it in range(iterations):\n",
    "        num_iter += 1\n",
    "        updated = False\n",
    "        \n",
    "        # Get the initial score for this iteration\n",
    "        a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "        curr_best_score = update(a, p, 1, single_alphabet=False).item()\n",
    "        curr_best_aptamer = curr_aptamer_str\n",
    "        curr_apt = list(curr_aptamer_str)\n",
    "        \n",
    "        # For each position\n",
    "        for i in range(40):\n",
    "            for n in range(4):\n",
    "                # Change one character\n",
    "                test_apt = curr_apt.copy()\n",
    "                test_apt[i] = nucleotides[n]\n",
    "                test_apt_str = ''.join(test_apt)\n",
    "                test_apt = one_hot(test_apt_str, seq_type='aptamer', single_alphabet=False)\n",
    "                a, p, l = convert(test_apt, peptide, 1, single_alphabet=False)\n",
    "                new_score = update(a, p, 1, single_alphabet=False)\n",
    "                if new_score.item() > curr_best_score:\n",
    "                    updated = True\n",
    "                    curr_best_score = new_score.item()\n",
    "                    curr_best_aptamer = test_apt_str\n",
    "                    hill_climb_scores.append(new_score)\n",
    "        \n",
    "        # If we find that none of the changes increase the score, just stop\n",
    "        if not updated:\n",
    "            break\n",
    "        \n",
    "        # Updated the global best aptamer\n",
    "        curr_aptamer_str = curr_best_aptamer\n",
    "        best_score = curr_best_score\n",
    "        curr_aptamer = one_hot(curr_aptamer_str, seq_type='aptamer', single_alphabet=False)\n",
    "    print(\"Num Iterations: \", num_iter)\n",
    "    \n",
    "    return best_score, curr_aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  25\n",
      "Hill climb score:  0.9094732999801636\n"
     ]
    }
   ],
   "source": [
    "hill_climb_score, hill_climbed_aptamer = hill_climb(\"MMFKYRAP\", \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\", iterations=10000)\n",
    "print(\"Hill climb score: \", hill_climb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SGD to find an aptamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def exponential_weights(peptide, actual_aptamer):\n",
    "    # Initialization with exponential random nums\n",
    "    peptide = \"MMFKYRAP\"\n",
    "    actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "\n",
    "    #aptamer_0 = one_hot(actual_aptamer, seq_type='aptamer')#exponential_random_initialization()\n",
    "    aptamer_0 = exponential_random_initialization()\n",
    "    curr_aptamer = aptamer_0\n",
    "\n",
    "    lr = 5e-1\n",
    "    scores = []\n",
    "    for k in range(1000):\n",
    "        a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "        train_score = update(a, p, 1, single_alphabet=False)\n",
    "        train_score.backward()\n",
    "        scores.append(train_score.item())\n",
    "\n",
    "        new_aptamer = np.zeros((40, 4, 1))\n",
    "        # Looping over the indices\n",
    "        for i in range(40):\n",
    "            # Gradient for this position\n",
    "            l_t = a.grad.cpu().numpy()\n",
    "            l_t = l_t.reshape((40, 1, 4))[i]\n",
    "            l_t = l_t[0]\n",
    "\n",
    "            # Old probability distribution\n",
    "            w = a.reshape((40, 1, 4))[i].cpu().detach().numpy()\n",
    "            w = w[0]\n",
    "\n",
    "            # New probability distribution\n",
    "            denom = 0\n",
    "            for j in range(4):            \n",
    "                new_aptamer[i][j] = w[j]*np.exp(lr*l_t[j])\n",
    "                denom += new_aptamer[i][j]\n",
    "            for j in range(4):\n",
    "                new_aptamer[i][j] = new_aptamer[i][j]/denom\n",
    "\n",
    "        curr_aptamer = new_aptamer\n",
    "\n",
    "    a, p, l = convert(curr_aptamer, peptide, 1, single_alphabet=False)\n",
    "    unrounded_score = update(a, p, 1, single_alphabet=False)\n",
    "\n",
    "    # Rounding Strategy # 1: Just max\n",
    "    rounded_aptamer = round_aptamer(curr_aptamer)\n",
    "    rounded_aptamer_str = stringify(rounded_aptamer)\n",
    "    a, p, l = convert(rounded_aptamer, peptide, 1, single_alphabet=False)\n",
    "    a = a.permute((2, 1, 0))\n",
    "    max_rounded_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    # Rounding Strategy #2: Hill climbing\n",
    "    hill_climb_score, hill_climbed_aptamer = hill_climb(peptide, rounded_aptamer_str, iterations=100000)\n",
    "\n",
    "    # Test the actual aptamer\n",
    "    actual_aptamer_oh = one_hot(actual_aptamer, seq_type='aptamer')\n",
    "    a, p, l = convert(actual_aptamer_oh, peptide, 1, single_alphabet=False)\n",
    "    actual_score = update(a, p, None, single_alphabet=False)\n",
    "\n",
    "    return unrounded_score, max_rounded_score, hill_climb_score, actual_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrounded:  tensor([[0.9052]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "Max Rounded:  tensor([[0.5955]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "Hill Climb:  0.930354654788971\n",
      "Actual Score:  tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "peptide = \"MMFKYRAP\"\n",
    "actual_aptamer = \"GCAAAAAGTCTACTTCTCCGTAACGGTAGGATACAGATCG\"\n",
    "unrounded_score, max_rounded_score, hill_climb_score, actual_score = exponential_weights(peptide, actual_aptamer)\n",
    "print(\"Unrounded: \", unrounded_score)\n",
    "print(\"Max Rounded: \", max_rounded_score)\n",
    "print(\"Hill Climb: \", hill_climb_score)\n",
    "print(\"Actual Score: \", actual_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out if we get optimal scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:14<11:42, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:30<11:56, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [00:46<12:00, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [01:04<12:20, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [01:20<11:53, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  12\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9165347218513489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [01:33<11:09, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [01:49<10:55, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [02:02<10:14, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  4\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.8825384378433228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [02:19<10:24, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [02:35<10:23, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [02:49<09:55, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [03:08<10:13, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [03:23<09:51, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [03:37<09:11, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [03:50<08:28, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9034560918807983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [04:04<08:13, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [04:17<07:39, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [04:30<07:23, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  7\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9064624309539795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [04:45<07:13, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 20/50 [05:00<07:14, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 21/50 [05:16<07:13, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 22/50 [05:32<07:06, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 23/50 [05:46<06:42, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 24/50 [06:02<06:34, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 25/50 [06:18<06:24, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 26/50 [06:32<06:00, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 27/50 [06:47<05:42, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  11\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9012414813041687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 28/50 [07:02<05:27, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 29/50 [07:16<05:09, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  12\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.898536741733551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 30/50 [07:33<05:09, 15.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 31/50 [07:50<04:58, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 32/50 [08:03<04:31, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  7\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9092848896980286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 33/50 [08:19<04:20, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 34/50 [08:35<04:09, 15.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 35/50 [08:53<04:03, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 36/50 [09:11<03:55, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 37/50 [09:25<03:27, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  8\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9096685647964478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 38/50 [09:43<03:18, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 39/50 [09:59<03:00, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 40/50 [10:16<02:44, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 41/50 [10:30<02:22, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9036784172058105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 42/50 [10:46<02:07, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 43/50 [11:03<01:52, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 44/50 [11:16<01:32, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  10\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9048740863800049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 45/50 [11:30<01:13, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 46/50 [11:45<00:59, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [11:58<00:43, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  9\n",
      "Actual Score: tensor([[0.3760]], device='cuda:0', grad_fn=<SigmoidBackward>)Found score: 0.9159601926803589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 48/50 [12:13<00:28, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 49/50 [12:28<00:14, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:44<00:00, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Iterations:  19\n",
      "Optimal Found:  39\n",
      "Unoptimal Found:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/fw_pairs.txt') as f:\n",
    "    optimal_count = 0\n",
    "    unoptimal_count = 0\n",
    "    found_scores = []\n",
    "    actual_scores = []\n",
    "    for i, pair in enumerate(tqdm.tqdm(f.readlines()[:50])):\n",
    "        apt, pep = pair.split()\n",
    "        unrounded_score, max_rounded_score, hill_climb_score, actual_score = exponential_weights(pep, apt)\n",
    "        if hill_climb_score >= unrounded_score:\n",
    "            optimal_count += 1\n",
    "        else:\n",
    "            unoptimal_count += 1\n",
    "            print(\"Actual Score: \" + str(actual_score) + \"Found score: \" + str(hill_climb_score))\n",
    "        found_scores.append(hill_climb_score)\n",
    "        actual_scores.append(unrounded_score.item())\n",
    "print(\"Optimal Found: \", optimal_count)\n",
    "print(\"Unoptimal Found: \", unoptimal_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Results of the Exponential Weights Algorithm')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.stripplot(found_scores, color='blue', label='Hill climb scores')\n",
    "sns.stripplot(actual_scores, color='red', label='Unrounded scores')\n",
    "plt.legend()\n",
    "plt.title(\"Results of the Exponential Weights Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
